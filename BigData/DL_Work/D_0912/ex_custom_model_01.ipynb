{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    " - 부모클래스 : nn.Module\n",
    " - 필수오버라이딩\n",
    "    * __init__() : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch                                # 텐서 관련 모듈\n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 ( 손실함수, 활성화함수 등등 )\n",
    "import torch.optim as optim                 # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈\n",
    "import torchmetrics as metrics              \n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐 고정\n",
    "    * 출력층 - 출력 : 타겟 수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입 력 층 : 입력 004개  출력 020개    AF ReLU  (s/ 시그모이드 써도 괜찮다. ReLU를 쓰든 시그모이드를 쓰던 그건 자율적. 뭔지 모르겟음 공부 필요)\n",
    "# 은 닉 층 : 입력 020개  출력 100개    AF ReLU\n",
    "# 출 력 층 : 입력 100개  출력 001개    AF Sigmoid & Softmax\n",
    "\n",
    "# 2진 분류에는 시그모이드, 다중분류에는 소프트맥스\n",
    "# 회귀는 필요없다. X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스 생성\n",
    "        self.input_layer = nn.Linear(1,20)      # w 4 + b 1  => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20,100)   # w 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)    # w 100 + b 1 => 1P, 101\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 CallBack Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        F.relu(y)                   # 0 <= y    ---> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y)    # y = x1w1 + x2w2 + ... + x20w20 + b\n",
    "        F.relu(y) \n",
    "\n",
    "        # \n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수 동적인 모델 \n",
    "class MyModel2(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 : 시스템에서 호출되는 함수)\n",
    "\n",
    "    def __init__(self, in_features,  in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스 생성\n",
    "        self.input_layer = nn.Linear(in_features,in_out)      # w 4 + b 1  => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out,h_out)   # w 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out,1)    # w 100 + b 1 => 1P, 101\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 CallBack Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        F.relu(y)                   # 0 <= y    ---> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y)    # y = x1w1 + x2w2 + ... + x20w20 + b\n",
    "        F.relu(y) \n",
    "\n",
    "        # \n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "\n",
    "class MyModel4(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 : 시스템에서 호출되는 함수)\n",
    "\n",
    "    def __init__(self, in_features, Linear ,in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스 생성\n",
    "        self.input_layer = nn.Linear(in_features,in_out)      # w 4 + b 1  => 1P, 5 * 20 = 100개 변수\n",
    "        for i in range(Linear):\n",
    "            self.hidden_layer = nn.Linear(in_out//Linear,h_out)\n",
    "            self.hidden_layer = nn.Linear(in_out,h_out)\n",
    "        self.output_layer = nn.Linear(h_out,1)    # w 100 + b 1 => 1P, 101\n",
    "\n",
    "        \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 CallBack Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        F.relu(y)                   # 0 <= y    ---> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y)    # y = x1w1 + x2w2 + ... + x20w20 + b\n",
    "        F.relu(y) \n",
    "\n",
    "        # \n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel4(\n",
       "  (input_layer): Linear(in_features=2, out_features=100, bias=True)\n",
       "  (hidden_layer): Linear(in_features=25, out_features=70, bias=True)\n",
       "  (output_layer): Linear(in_features=70, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel()\n",
    "\n",
    "m2 = MyModel2(3,100,70)\n",
    "m2\n",
    "\n",
    "m4 = MyModel4(2,4,100,70)\n",
    "m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5153],\n",
      "        [-0.4414],\n",
      "        [-0.1939],\n",
      "        [ 0.4694],\n",
      "        [-0.9414],\n",
      "        [ 0.5997],\n",
      "        [-0.2057],\n",
      "        [ 0.5087],\n",
      "        [ 0.1390],\n",
      "        [-0.1224],\n",
      "        [ 0.2774],\n",
      "        [ 0.0493],\n",
      "        [ 0.3652],\n",
      "        [-0.3897],\n",
      "        [-0.0729],\n",
      "        [-0.0900],\n",
      "        [ 0.1449],\n",
      "        [-0.0040],\n",
      "        [ 0.8742],\n",
      "        [ 0.3112]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3724, -0.6040, -0.1676, -0.4313, -0.3204,  0.0479,  0.5961,  0.5435,\n",
      "        -0.9776,  0.6199,  0.2794,  0.9486,  0.6601, -0.9111, -0.9508, -0.4823,\n",
      "         0.8781, -0.1666,  0.4280, -0.4647], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2194, -0.0946,  0.1677,  ..., -0.1493,  0.1362,  0.0694],\n",
      "        [-0.1445,  0.1452,  0.1358,  ...,  0.1108, -0.2071,  0.1126],\n",
      "        [-0.1572, -0.1687,  0.0136,  ...,  0.0401, -0.0951, -0.0677],\n",
      "        ...,\n",
      "        [-0.2228,  0.0137,  0.0685,  ...,  0.0825, -0.1046, -0.1153],\n",
      "        [-0.0414,  0.0074,  0.1323,  ..., -0.1552,  0.0568, -0.2110],\n",
      "        [ 0.0490, -0.0862, -0.1338,  ...,  0.1569,  0.1833, -0.1882]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1844, -0.1471, -0.1923, -0.0671,  0.0819, -0.2040, -0.2185, -0.1925,\n",
      "        -0.1167,  0.1402,  0.1983,  0.2221,  0.0024, -0.1894,  0.2181, -0.1503,\n",
      "         0.0844,  0.1118,  0.0691,  0.0738, -0.2086, -0.0753, -0.1438,  0.1544,\n",
      "        -0.1270, -0.0053,  0.1988,  0.1695, -0.1609, -0.1865,  0.1770,  0.0644,\n",
      "        -0.1714,  0.1933, -0.1614,  0.1936,  0.1604,  0.1593, -0.0429,  0.0194,\n",
      "        -0.1867,  0.1260,  0.0129,  0.1656, -0.0943,  0.2160,  0.1491, -0.0626,\n",
      "         0.0610, -0.1631, -0.1489, -0.1532, -0.1285,  0.0622, -0.0964, -0.0772,\n",
      "         0.1564,  0.0369, -0.1090, -0.0607, -0.0588,  0.1044,  0.1574,  0.1782,\n",
      "        -0.1521, -0.0239,  0.0525, -0.0874,  0.1470, -0.0249,  0.0239, -0.1912,\n",
      "         0.1616, -0.1482, -0.0054, -0.1824,  0.1701,  0.0129,  0.1007, -0.1758,\n",
      "         0.0190,  0.0916,  0.1579,  0.1643, -0.0399, -0.0227,  0.0743, -0.0804,\n",
      "        -0.0910, -0.2059, -0.2193, -0.1177,  0.0279,  0.0180,  0.0179,  0.0335,\n",
      "        -0.0413, -0.2209,  0.1997, -0.1636], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0012,  0.0397,  0.0256, -0.0434, -0.0869,  0.0801,  0.0028,  0.0912,\n",
      "          0.0467,  0.0427,  0.0216, -0.0226, -0.0586,  0.0121,  0.0961,  0.0600,\n",
      "          0.0909,  0.0938,  0.0781, -0.0561, -0.0545, -0.0034, -0.0517, -0.0271,\n",
      "         -0.0343,  0.0875, -0.0067,  0.0968, -0.0775, -0.0986,  0.0143,  0.0199,\n",
      "         -0.0404, -0.0506,  0.0285,  0.0050, -0.0134,  0.0508, -0.0425,  0.0787,\n",
      "         -0.0682, -0.0520,  0.0346,  0.0615, -0.0012, -0.0785, -0.0040, -0.0338,\n",
      "         -0.0320, -0.0185,  0.0241, -0.0233, -0.0701, -0.0499, -0.0838, -0.0999,\n",
      "          0.0483,  0.0559,  0.0756,  0.0079, -0.0411, -0.0601,  0.0498, -0.0604,\n",
      "          0.0747, -0.0542, -0.0933, -0.0720,  0.0814,  0.0755,  0.0244,  0.0023,\n",
      "          0.0857,  0.0603,  0.0592, -0.0681,  0.0160,  0.0906, -0.0704,  0.0996,\n",
      "         -0.0655, -0.0294, -0.0047,  0.0260,  0.0307, -0.0572,  0.0458,  0.0265,\n",
      "         -0.0111, -0.0995,  0.0990, -0.0370, -0.0533,  0.0990, -0.0507,  0.0668,\n",
      "          0.0610, -0.0651, -0.0806, -0.0544]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0313], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for m in m1.parameters(): print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x4 and 1x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m targetTS \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m4\u001b[39m],[\u001b[38;5;241m5\u001b[39m]])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pre_y \u001b[38;5;241m=\u001b[39m \u001b[43mm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(pre_y)\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalling forward()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# y = x1w1 + x2w2 + x3w3 + x4w4 + b\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     F\u001b[38;5;241m.\u001b[39mrelu(y)                   \u001b[38;5;66;03m# 0 <= y    ---> 죽은 relu ==> leakyReLU\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(y)    \u001b[38;5;66;03m# y = x1w1 + x2w2 + ... + x20w20 + b\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x4 and 1x20)"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ===> 모델 인스턴스명()\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1,3,5,7],[2,4,6,8]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "\n",
    "print(pre_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

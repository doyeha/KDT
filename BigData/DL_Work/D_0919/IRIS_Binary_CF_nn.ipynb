{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN 기반 회귀 모델 구현 및 학습 모니터링과 저장\n",
    "- 데이터 : iris.csv\n",
    "- 피처/속성 : 4개 Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n",
    "- 타겟/라벨 : 1개 Setosa와 나머지\n",
    "- 학습-방법 : 지도학습 > 분류> 이진분류\n",
    "- 학습 알고리즘 : 인공신경망(ANN) -> 심층 신경망 (MLP, DNN) : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "# 모델 관련 모듈\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score, BinaryF1Score, BinaryConfusionMatrix\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 및 시각화 관련 모듈\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch v.2.4.1\n",
      "Pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 ==> 사용자 정의 함수로 구현하기~~~\n",
    "print(f'Pytorch v.{torch.__version__}')\n",
    "print(f'Pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 데이터 로딩\n",
    "DATA_FILE = 'C:\\Git\\KDT\\BigData\\data\\iris.csv'\n",
    "\n",
    "### CSV => DataFrame\n",
    "irisDF = pd.read_csv(DATA_FILE)\n",
    "\n",
    "### 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 타겟 변경 => 정수화\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 3개 => 2개 (replace로 변경도 가능함 / Setosa를 False로 주는 것도 가능)\n",
    "irisDF['variety'] = (irisDF['variety'] == 'Setosa')\n",
    "\n",
    "# 방법 2 \n",
    "# labels = dict(zip(irisDF['variety'].unique().tolist(),range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "145    0\n",
       "146    0\n",
       "147    0\n",
       "148    0\n",
       "149    0\n",
       "Name: variety, Length: 150, dtype: int32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDF['variety'] = irisDF['variety'].astype('int')\n",
    "irisDF['variety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유값 : [1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유값 확인\n",
    "print(f\"고유값 : {irisDF['variety'].unique()}\")\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의 <hr>\n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적\n",
    "- 클래스 이름 : IrisBCFModel\n",
    "- 부모 클래스 : nn.Module\n",
    "- 매 개 변 수 : 층별 입출력 개수 고정하므로 필요 없음\n",
    "- 속성 / 필드 :\n",
    "- 기능 / 역할 : __init__() : 모델 구조 설정, forward() : 순방향 학습 <=오버라이딩(상속관계에서만 가능)\n",
    "- 클래스 구조\n",
    "    * 입력층 : 입력 4개(피처 개수) / 출력 10개(퍼셉트론/뉴런 개수 10개)\n",
    "    * 은닉층 : 입력 10개          / 출력 5개\n",
    "    * 출력층 : 입력 5개          / 출력 1개(이진분류)\n",
    "- - -\n",
    "- 손실함수 / 활성화 함수\n",
    "    * 클래스 형태 ==> nn.MESLoss, nn.ReLU ==> __init__() 메서드\n",
    "    * 함수 형태 ==> torch.nn.fuctional 아래에 ==> forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisBCFModel(nn.Module):\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,1)\n",
    "\n",
    "    #순방향 학습 진행 메서드\n",
    "    def forward(self,x) : \n",
    "        # 입력층\n",
    "        y = self.in_layer(x)    # \n",
    "        y=F.relu(y)             # relu 값의 범위 : 0<=y / 시그모이드 : 0~1\n",
    "        # 은닉층 : 10개 숫자의 값(>=0)\n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "        # 출력층 : 5개 숫자값 / 분류이므로 시그모이드 함수 적용해서 반환\n",
    "        return F.sigmoid(self.out_layer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisBCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 모델 인스턴스 생성\n",
    "model = IrisBCFModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisBCFModel                             [17, 1]                   --\n",
       "├─Linear: 1-1                            [17, 10]                  50\n",
       "├─Linear: 1-2                            [17, 5]                   55\n",
       "├─Linear: 1-3                            [17, 1]                   6\n",
       "==========================================================================================\n",
       "Total params: 111\n",
       "Trainable params: 111\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [테스트] 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(17,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의 <hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피처 개수 : 4개\n",
    "- 타겟 개수 : 1개\n",
    "- 클래스 이름 : IrisDataset\n",
    "- 부모 클래스 : utils.data.Dataset\n",
    "- 속성 / 필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수 메서드\n",
    "    * _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * _ _len_ _(self) : 데이터의 개수 반환\n",
    "    * _ _getItem_ _(self, index) : 특정 인덱스의 피처와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF = featureDF\n",
    "        self.targetDF = targetDF\n",
    "        self.n_rows = featureDF.shape[0]\n",
    "        self.n_features = featureDF.shape[1]\n",
    "\n",
    "    def __len__(self) : \n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        feaureTS = torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "        \n",
    "        # 피처와 타겟 반환\n",
    "        return feaureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "## [테스트] 데이터셋 인스턴스 생성\n",
    "# 피처와 타겟 데이터 추출\n",
    "featureDF = irisDF[irisDF.columns[:-1]]     # 2D (150,4)\n",
    "targetDF = irisDF[irisDF.columns[-1:]]      # 2D (150,1)\n",
    "\n",
    "# 커스텀데이터셋 인스턴스 생성\n",
    "irisDS = IrisDataset(featureDF, targetDF)\n",
    "\n",
    "# 데이터로더 인스턴스 생성\n",
    "irisDL = DataLoader(irisDS)\n",
    "for feature, label in irisDL:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비 <hr>\n",
    "- 학습 횟수 : EPOCH   <- 처음부터 끝까지 공부하는 단위\n",
    "- 배치 크기 : BATCH_SIZE   <- 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE  <- 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학 습 율  : LR 가중치와 절편 업데이트 시 경사 하강법으로 업데이트 간격 설정 0.001~0.1 (하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 진행 관련 설정\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10\n",
    "BATCH_CNT = irisDF.shape[0]//BATCH_SIZE # 선택사항 - 코드에 넣을 수도 있음\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (, 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스\n",
    "model = IrisBCFModel().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n",
      "variety\n",
      "0          0.702381\n",
      "1          0.297619\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.657895\n",
      "1          0.342105\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.571429\n",
      "1          0.428571\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "### DS과 DL 인스턴스\n",
    "\n",
    "# 학습/검증/테스트용 데이터 분리\n",
    "X_train,X_test, y_train, y_test = train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "print(f'{y_train.value_counts()/y_train.shape[0]} {y_test.value_counts()/y_test.shape[0]} {y_val.value_counts()/y_val.shape[0]}')\n",
    "print(f'{type(X_train)} {type(X_test)} {type(X_val)}')\n",
    "\n",
    "## 학습/검증/테스트용 데이터셋\n",
    "# irisDS = IrisDataset(X_train, y_train)\n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "valDS = IrisDataset(X_val, y_val)\n",
    "testDS = IrisDataset(X_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 (검증용은 필요 없음, 테스트는 양이 많을 때 개발자가 선택하여 인스턴스 생성)\n",
    "# irisDL = DataLoader(irisDS, batch_size = BATCH_SIZE)\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W,b 텐서, 즉 model.parameters() 전달 - 최적화하는 이유 : 오차를 줄이기 위해서!\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 이진분류 : BinaryCrossEntropyLoss => BCELoss\n",
    "#                            예측값은 확률값으로 전달 ==> sigmoid() AF 처리 후 전달\n",
    "reqLoss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - > > > 모델 저장 관련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import \n",
    "\n",
    "# 저장경로\n",
    "SAVE_PATH = '../models/iris/'\n",
    "\n",
    "#저장 파일 명\n",
    "SAVE_FILE = SAVE_PATH + 'model_train_wbs.pth'\n",
    "\n",
    "# 모델구조 및 파라미터 모두 저장 파일명\n",
    "SAVE_MODEL = SAVE_PATH + 'model_all.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDL), trainDL.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.4, 9, 84)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "trainDS.n_rows/BATCH_SIZE, m.ceil(trainDS.n_rows/BATCH_SIZE), trainDS.featureDF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT =>9\n",
      "[0/1000]\n",
      "- Train Loss : 0.8280731108453538 Score : 0.4377178516652849\n",
      "- Val Loss : 0.7662215828895569 Score : 0.6000000238418579\n",
      "[1/1000]\n",
      "- Train Loss : 0.761603421635098 Score : 0.425507836871677\n",
      "- Val Loss : 0.730405330657959 Score : 0.3529411852359772\n",
      "[2/1000]\n",
      "- Train Loss : 0.710745222038693 Score : 0.04242424335744646\n",
      "- Val Loss : 0.7056595683097839 Score : 0.0\n",
      "[3/1000]\n",
      "- Train Loss : 0.6732535031106737 Score : 0.0\n",
      "- Val Loss : 0.6910340189933777 Score : 0.0\n",
      "[4/1000]\n",
      "- Train Loss : 0.6535295910305448 Score : 0.0\n",
      "- Val Loss : 0.6884689331054688 Score : 0.0\n",
      "[5/1000]\n",
      "- Train Loss : 0.643258081542121 Score : 0.0\n",
      "- Val Loss : 0.683221697807312 Score : 0.0\n",
      "[6/1000]\n",
      "- Train Loss : 0.6331265502505832 Score : 0.0\n",
      "- Val Loss : 0.6767665147781372 Score : 0.0\n",
      "[7/1000]\n",
      "- Train Loss : 0.6247332361009386 Score : 0.0\n",
      "- Val Loss : 0.6715885400772095 Score : 0.0\n",
      "[8/1000]\n",
      "- Train Loss : 0.6173473066753812 Score : 0.0\n",
      "- Val Loss : 0.6678016781806946 Score : 0.0\n",
      "[9/1000]\n",
      "- Train Loss : 0.6118056178092957 Score : 0.0\n",
      "- Val Loss : 0.6668930649757385 Score : 0.0\n",
      "[10/1000]\n",
      "- Train Loss : 0.6079566280047098 Score : 0.0\n",
      "- Val Loss : 0.6635555624961853 Score : 0.0\n",
      "[11/1000]\n",
      "- Train Loss : 0.6026950544781156 Score : 0.0\n",
      "- Val Loss : 0.658671498298645 Score : 0.0\n",
      "[12/1000]\n",
      "- Train Loss : 0.5973110993703207 Score : 0.0\n",
      "- Val Loss : 0.6537072062492371 Score : 0.0\n",
      "[13/1000]\n",
      "- Train Loss : 0.5910190410084195 Score : 0.0\n",
      "- Val Loss : 0.6490879654884338 Score : 0.0\n",
      "[14/1000]\n",
      "- Train Loss : 0.5838009450170729 Score : 0.0\n",
      "- Val Loss : 0.6439856290817261 Score : 0.0\n",
      "[15/1000]\n",
      "- Train Loss : 0.5756334894233279 Score : 0.0\n",
      "- Val Loss : 0.6380388140678406 Score : 0.0\n",
      "[16/1000]\n",
      "- Train Loss : 0.5667250586880578 Score : 0.0\n",
      "- Val Loss : 0.6316961646080017 Score : 0.0\n",
      "[17/1000]\n",
      "- Train Loss : 0.5572434299521976 Score : 0.0\n",
      "- Val Loss : 0.6251066327095032 Score : 0.0\n",
      "[18/1000]\n",
      "- Train Loss : 0.5472538669904073 Score : 0.0\n",
      "- Val Loss : 0.6180402636528015 Score : 0.0\n",
      "[19/1000]\n",
      "- Train Loss : 0.536794000201755 Score : 0.0\n",
      "- Val Loss : 0.6099509596824646 Score : 0.0\n",
      "[20/1000]\n",
      "- Train Loss : 0.5260414746072557 Score : 0.0\n",
      "- Val Loss : 0.6010875701904297 Score : 0.0\n",
      "[21/1000]\n",
      "- Train Loss : 0.5149667693508996 Score : 0.0\n",
      "- Val Loss : 0.5912973284721375 Score : 0.0\n",
      "[22/1000]\n",
      "- Train Loss : 0.503685153192944 Score : 0.0\n",
      "- Val Loss : 0.5811092257499695 Score : 0.0\n",
      "[23/1000]\n",
      "- Train Loss : 0.49216606550746494 Score : 0.0\n",
      "- Val Loss : 0.5702704191207886 Score : 0.0\n",
      "[24/1000]\n",
      "- Train Loss : 0.48025362690289813 Score : 0.0\n",
      "- Val Loss : 0.558038055896759 Score : 0.0\n",
      "[25/1000]\n",
      "- Train Loss : 0.46790984604093766 Score : 0.0\n",
      "- Val Loss : 0.5442594289779663 Score : 0.0\n",
      "[26/1000]\n",
      "- Train Loss : 0.4550553262233734 Score : 0.0\n",
      "- Val Loss : 0.528950035572052 Score : 0.0\n",
      "[27/1000]\n",
      "- Train Loss : 0.4416537582874298 Score : 0.0\n",
      "- Val Loss : 0.5123199224472046 Score : 0.0\n",
      "[28/1000]\n",
      "- Train Loss : 0.42764992515246075 Score : 0.0\n",
      "- Val Loss : 0.4943625032901764 Score : 0.0\n",
      "[29/1000]\n",
      "- Train Loss : 0.4130292236804962 Score : 0.0\n",
      "- Val Loss : 0.47525355219841003 Score : 0.0\n",
      "[30/1000]\n",
      "- Train Loss : 0.3978484769662221 Score : 0.0\n",
      "- Val Loss : 0.4553375840187073 Score : 0.0\n",
      "[31/1000]\n",
      "- Train Loss : 0.38208356830808854 Score : 0.0\n",
      "- Val Loss : 0.4346192181110382 Score : 0.1538461595773697\n",
      "[32/1000]\n",
      "- Train Loss : 0.3658043146133423 Score : 0.0\n",
      "- Val Loss : 0.41330385208129883 Score : 0.2857142984867096\n",
      "[33/1000]\n",
      "- Train Loss : 0.3490639080603917 Score : 0.3000000019868215\n",
      "- Val Loss : 0.3913312256336212 Score : 0.6666666865348816\n",
      "[34/1000]\n",
      "- Train Loss : 0.33198706309000653 Score : 0.6555555595291985\n",
      "- Val Loss : 0.36885973811149597 Score : 0.9090909361839294\n",
      "[35/1000]\n",
      "- Train Loss : 0.31471409069167244 Score : 0.9365079402923584\n",
      "- Val Loss : 0.34622257947921753 Score : 1.0\n",
      "[36/1000]\n",
      "- Train Loss : 0.297381223903762 Score : 0.9876543217235141\n",
      "- Val Loss : 0.32379934191703796 Score : 1.0\n",
      "[37/1000]\n",
      "- Train Loss : 0.2801394909620285 Score : 1.0\n",
      "- Val Loss : 0.3018760085105896 Score : 1.0\n",
      "[38/1000]\n",
      "- Train Loss : 0.26307474242316353 Score : 1.0\n",
      "- Val Loss : 0.28041133284568787 Score : 1.0\n",
      "[39/1000]\n",
      "- Train Loss : 0.24637886550691393 Score : 1.0\n",
      "- Val Loss : 0.25975874066352844 Score : 1.0\n",
      "[40/1000]\n",
      "- Train Loss : 0.23016734255684745 Score : 1.0\n",
      "- Val Loss : 0.24018120765686035 Score : 1.0\n",
      "[41/1000]\n",
      "- Train Loss : 0.21451405518584782 Score : 1.0\n",
      "- Val Loss : 0.22159238159656525 Score : 1.0\n",
      "[42/1000]\n",
      "- Train Loss : 0.19951822691493565 Score : 1.0\n",
      "- Val Loss : 0.2040570080280304 Score : 1.0\n",
      "[43/1000]\n",
      "- Train Loss : 0.1852836807568868 Score : 1.0\n",
      "- Val Loss : 0.18750333786010742 Score : 1.0\n",
      "[44/1000]\n",
      "- Train Loss : 0.17189412315686545 Score : 1.0\n",
      "- Val Loss : 0.1721491813659668 Score : 1.0\n",
      "[45/1000]\n",
      "- Train Loss : 0.15936159756448534 Score : 1.0\n",
      "- Val Loss : 0.15811406075954437 Score : 1.0\n",
      "[46/1000]\n",
      "- Train Loss : 0.14766611241632038 Score : 1.0\n",
      "- Val Loss : 0.14532078802585602 Score : 1.0\n",
      "[47/1000]\n",
      "- Train Loss : 0.13678197976615694 Score : 1.0\n",
      "- Val Loss : 0.13363026082515717 Score : 1.0\n",
      "[48/1000]\n",
      "- Train Loss : 0.12670097831222746 Score : 1.0\n",
      "- Val Loss : 0.12292022258043289 Score : 1.0\n",
      "[49/1000]\n",
      "- Train Loss : 0.11741152736875746 Score : 1.0\n",
      "- Val Loss : 0.11314152181148529 Score : 1.0\n",
      "[50/1000]\n",
      "- Train Loss : 0.1088808493481742 Score : 1.0\n",
      "- Val Loss : 0.10422949492931366 Score : 1.0\n",
      "[51/1000]\n",
      "- Train Loss : 0.10105505088965099 Score : 1.0\n",
      "- Val Loss : 0.09616819769144058 Score : 1.0\n",
      "[52/1000]\n",
      "- Train Loss : 0.09389135738213857 Score : 1.0\n",
      "- Val Loss : 0.08889029175043106 Score : 1.0\n",
      "[53/1000]\n",
      "- Train Loss : 0.0873245857656002 Score : 1.0\n",
      "- Val Loss : 0.08229851722717285 Score : 1.0\n",
      "[54/1000]\n",
      "- Train Loss : 0.08131806511018011 Score : 1.0\n",
      "- Val Loss : 0.0762798860669136 Score : 1.0\n",
      "[55/1000]\n",
      "- Train Loss : 0.07580847541491191 Score : 1.0\n",
      "- Val Loss : 0.07074826210737228 Score : 1.0\n",
      "[56/1000]\n",
      "- Train Loss : 0.07077647414472368 Score : 1.0\n",
      "- Val Loss : 0.06572275608778 Score : 1.0\n",
      "[57/1000]\n",
      "- Train Loss : 0.06617458826965755 Score : 1.0\n",
      "- Val Loss : 0.06119389086961746 Score : 1.0\n",
      "[58/1000]\n",
      "- Train Loss : 0.061953720119264394 Score : 1.0\n",
      "- Val Loss : 0.05709725245833397 Score : 1.0\n",
      "[59/1000]\n",
      "- Train Loss : 0.05807273214062055 Score : 1.0\n",
      "- Val Loss : 0.053373247385025024 Score : 1.0\n",
      "[60/1000]\n",
      "- Train Loss : 0.0545040691892306 Score : 1.0\n",
      "- Val Loss : 0.049977365881204605 Score : 1.0\n",
      "[61/1000]\n",
      "- Train Loss : 0.05121983256604937 Score : 1.0\n",
      "- Val Loss : 0.04686123877763748 Score : 1.0\n",
      "[62/1000]\n",
      "- Train Loss : 0.04819549827112092 Score : 1.0\n",
      "- Val Loss : 0.04399724677205086 Score : 1.0\n",
      "[63/1000]\n",
      "- Train Loss : 0.04541046876046392 Score : 1.0\n",
      "- Val Loss : 0.04135865345597267 Score : 1.0\n",
      "[64/1000]\n",
      "- Train Loss : 0.04284403742187553 Score : 1.0\n",
      "- Val Loss : 0.038932569324970245 Score : 1.0\n",
      "[65/1000]\n",
      "- Train Loss : 0.04047509779532751 Score : 1.0\n",
      "- Val Loss : 0.03670339658856392 Score : 1.0\n",
      "[66/1000]\n",
      "- Train Loss : 0.03828443959355354 Score : 1.0\n",
      "- Val Loss : 0.03465498611330986 Score : 1.0\n",
      "[67/1000]\n",
      "- Train Loss : 0.03625596273276541 Score : 1.0\n",
      "- Val Loss : 0.03276317939162254 Score : 1.0\n",
      "[68/1000]\n",
      "- Train Loss : 0.03437457378539774 Score : 1.0\n",
      "- Val Loss : 0.031009722501039505 Score : 1.0\n",
      "[69/1000]\n",
      "- Train Loss : 0.03262957868476709 Score : 1.0\n",
      "- Val Loss : 0.02938520535826683 Score : 1.0\n",
      "[70/1000]\n",
      "- Train Loss : 0.031008017145925097 Score : 1.0\n",
      "- Val Loss : 0.027880754321813583 Score : 1.0\n",
      "[71/1000]\n",
      "- Train Loss : 0.02949956866602103 Score : 1.0\n",
      "- Val Loss : 0.026486173272132874 Score : 1.0\n",
      "[72/1000]\n",
      "- Train Loss : 0.028093044542604022 Score : 1.0\n",
      "- Val Loss : 0.025190060958266258 Score : 1.0\n",
      "[73/1000]\n",
      "- Train Loss : 0.026779863776432142 Score : 1.0\n",
      "- Val Loss : 0.02398311160504818 Score : 1.0\n",
      "[74/1000]\n",
      "- Train Loss : 0.025552522184120283 Score : 1.0\n",
      "- Val Loss : 0.022860709577798843 Score : 1.0\n",
      "[75/1000]\n",
      "- Train Loss : 0.024404829367995262 Score : 1.0\n",
      "- Val Loss : 0.02181440033018589 Score : 1.0\n",
      "[76/1000]\n",
      "- Train Loss : 0.02332969516929653 Score : 1.0\n",
      "- Val Loss : 0.02082734741270542 Score : 1.0\n",
      "[77/1000]\n",
      "- Train Loss : 0.022324213551150426 Score : 1.0\n",
      "- Val Loss : 0.019900541752576828 Score : 1.0\n",
      "[78/1000]\n",
      "- Train Loss : 0.02138317024542226 Score : 1.0\n",
      "- Val Loss : 0.019035547971725464 Score : 1.0\n",
      "[79/1000]\n",
      "- Train Loss : 0.020498935236699052 Score : 1.0\n",
      "- Val Loss : 0.01822553016245365 Score : 1.0\n",
      "[80/1000]\n",
      "- Train Loss : 0.019666945768727198 Score : 1.0\n",
      "- Val Loss : 0.017461102455854416 Score : 1.0\n",
      "[81/1000]\n",
      "- Train Loss : 0.01888590647528569 Score : 1.0\n",
      "- Val Loss : 0.01674319989979267 Score : 1.0\n",
      "[82/1000]\n",
      "- Train Loss : 0.018151508540742926 Score : 1.0\n",
      "- Val Loss : 0.016071736812591553 Score : 1.0\n",
      "[83/1000]\n",
      "- Train Loss : 0.017456979387336306 Score : 1.0\n",
      "- Val Loss : 0.015437555499374866 Score : 1.0\n",
      "[84/1000]\n",
      "- Train Loss : 0.016802089081870183 Score : 1.0\n",
      "- Val Loss : 0.014839346520602703 Score : 1.0\n",
      "[85/1000]\n",
      "- Train Loss : 0.016183397836155362 Score : 1.0\n",
      "- Val Loss : 0.014275764115154743 Score : 1.0\n",
      "[86/1000]\n",
      "- Train Loss : 0.015597638260159228 Score : 1.0\n",
      "- Val Loss : 0.013744463212788105 Score : 1.0\n",
      "[87/1000]\n",
      "- Train Loss : 0.015042273741629388 Score : 1.0\n",
      "- Val Loss : 0.013242709450423717 Score : 1.0\n",
      "[88/1000]\n",
      "- Train Loss : 0.014515158927275075 Score : 1.0\n",
      "- Val Loss : 0.012767987325787544 Score : 1.0\n",
      "[89/1000]\n",
      "- Train Loss : 0.01401446159515116 Score : 1.0\n",
      "- Val Loss : 0.012317953631281853 Score : 1.0\n",
      "[90/1000]\n",
      "- Train Loss : 0.013538688576469818 Score : 1.0\n",
      "- Val Loss : 0.011890009976923466 Score : 1.0\n",
      "[91/1000]\n",
      "- Train Loss : 0.013086028655783998 Score : 1.0\n",
      "- Val Loss : 0.011482499539852142 Score : 1.0\n",
      "[92/1000]\n",
      "- Train Loss : 0.012655580054140754 Score : 1.0\n",
      "- Val Loss : 0.01109406165778637 Score : 1.0\n",
      "[93/1000]\n",
      "- Train Loss : 0.012245801639639668 Score : 1.0\n",
      "- Val Loss : 0.01072266511619091 Score : 1.0\n",
      "[94/1000]\n",
      "- Train Loss : 0.011856250671876801 Score : 1.0\n",
      "- Val Loss : 0.010367684066295624 Score : 1.0\n",
      "[95/1000]\n",
      "- Train Loss : 0.011485919459826417 Score : 1.0\n",
      "- Val Loss : 0.010030130855739117 Score : 1.0\n",
      "[96/1000]\n",
      "- Train Loss : 0.011133040353241894 Score : 1.0\n",
      "- Val Loss : 0.009709819220006466 Score : 1.0\n",
      "[97/1000]\n",
      "- Train Loss : 0.010796057784722911 Score : 1.0\n",
      "- Val Loss : 0.009404726326465607 Score : 1.0\n",
      "[98/1000]\n",
      "- Train Loss : 0.01047388691869047 Score : 1.0\n",
      "- Val Loss : 0.009113246574997902 Score : 1.0\n",
      "[99/1000]\n",
      "- Train Loss : 0.010166345681581233 Score : 1.0\n",
      "- Val Loss : 0.008835548534989357 Score : 1.0\n",
      "[100/1000]\n",
      "- Train Loss : 0.009872122046848139 Score : 1.0\n",
      "- Val Loss : 0.008570942096412182 Score : 1.0\n",
      "[101/1000]\n",
      "- Train Loss : 0.009590244748526149 Score : 1.0\n",
      "- Val Loss : 0.008318503387272358 Score : 1.0\n",
      "[102/1000]\n",
      "- Train Loss : 0.009319962261037694 Score : 1.0\n",
      "- Val Loss : 0.008077341131865978 Score : 1.0\n",
      "[103/1000]\n",
      "- Train Loss : 0.009060634765774012 Score : 1.0\n",
      "- Val Loss : 0.007846595719456673 Score : 1.0\n",
      "[104/1000]\n",
      "- Train Loss : 0.008811692034618722 Score : 1.0\n",
      "- Val Loss : 0.007625549100339413 Score : 1.0\n",
      "[105/1000]\n",
      "- Train Loss : 0.00857260450720787 Score : 1.0\n",
      "- Val Loss : 0.00741328950971365 Score : 1.0\n",
      "[106/1000]\n",
      "- Train Loss : 0.008342702303909592 Score : 1.0\n",
      "- Val Loss : 0.007208724040538073 Score : 1.0\n",
      "[107/1000]\n",
      "- Train Loss : 0.008122318424284458 Score : 1.0\n",
      "- Val Loss : 0.007012177258729935 Score : 1.0\n",
      "[108/1000]\n",
      "- Train Loss : 0.007910605416529708 Score : 1.0\n",
      "- Val Loss : 0.0068235513754189014 Score : 1.0\n",
      "[109/1000]\n",
      "- Train Loss : 0.0077069539369808305 Score : 1.0\n",
      "- Val Loss : 0.006642543710768223 Score : 1.0\n",
      "[110/1000]\n",
      "- Train Loss : 0.00751089822087023 Score : 1.0\n",
      "- Val Loss : 0.006468716077506542 Score : 1.0\n",
      "[111/1000]\n",
      "- Train Loss : 0.007322039791486329 Score : 1.0\n",
      "- Val Loss : 0.00630162563174963 Score : 1.0\n",
      "[112/1000]\n",
      "- Train Loss : 0.007139862256331576 Score : 1.0\n",
      "- Val Loss : 0.006140571553260088 Score : 1.0\n",
      "[113/1000]\n",
      "- Train Loss : 0.006964441725156373 Score : 1.0\n",
      "- Val Loss : 0.005985704716295004 Score : 1.0\n",
      "[114/1000]\n",
      "- Train Loss : 0.006795448251068592 Score : 1.0\n",
      "- Val Loss : 0.005836766678839922 Score : 1.0\n",
      "[115/1000]\n",
      "- Train Loss : 0.0066324626095592976 Score : 1.0\n",
      "- Val Loss : 0.005693226121366024 Score : 1.0\n",
      "[116/1000]\n",
      "- Train Loss : 0.006475185768471824 Score : 1.0\n",
      "- Val Loss : 0.0055548870004713535 Score : 1.0\n",
      "[117/1000]\n",
      "- Train Loss : 0.006323035754677322 Score : 1.0\n",
      "- Val Loss : 0.005421019159257412 Score : 1.0\n",
      "[118/1000]\n",
      "- Train Loss : 0.006176538041068448 Score : 1.0\n",
      "- Val Loss : 0.0052918787114322186 Score : 1.0\n",
      "[119/1000]\n",
      "- Train Loss : 0.006035076764722665 Score : 1.0\n",
      "- Val Loss : 0.005167312454432249 Score : 1.0\n",
      "[120/1000]\n",
      "- Train Loss : 0.0058983464518355 Score : 1.0\n",
      "- Val Loss : 0.005047124810516834 Score : 1.0\n",
      "[121/1000]\n",
      "- Train Loss : 0.005765912097154392 Score : 1.0\n",
      "- Val Loss : 0.004930783528834581 Score : 1.0\n",
      "[122/1000]\n",
      "- Train Loss : 0.0056380269428094225 Score : 1.0\n",
      "- Val Loss : 0.004818510729819536 Score : 1.0\n",
      "[123/1000]\n",
      "- Train Loss : 0.005514322469631831 Score : 1.0\n",
      "- Val Loss : 0.004710108507424593 Score : 1.0\n",
      "[124/1000]\n",
      "- Train Loss : 0.005394379867033826 Score : 1.0\n",
      "- Val Loss : 0.0046049621887505054 Score : 1.0\n",
      "[125/1000]\n",
      "- Train Loss : 0.005278472167750199 Score : 1.0\n",
      "- Val Loss : 0.004503156058490276 Score : 1.0\n",
      "[126/1000]\n",
      "- Train Loss : 0.005166288206560744 Score : 1.0\n",
      "- Val Loss : 0.004404682200402021 Score : 1.0\n",
      "[127/1000]\n",
      "- Train Loss : 0.0050575873918003505 Score : 1.0\n",
      "- Val Loss : 0.004309412557631731 Score : 1.0\n",
      "[128/1000]\n",
      "- Train Loss : 0.004952183847005169 Score : 1.0\n",
      "- Val Loss : 0.004217219538986683 Score : 1.0\n",
      "[129/1000]\n",
      "- Train Loss : 0.004849942918452952 Score : 1.0\n",
      "- Val Loss : 0.00412794528529048 Score : 1.0\n",
      "[130/1000]\n",
      "- Train Loss : 0.004750758745811052 Score : 1.0\n",
      "- Val Loss : 0.004041374661028385 Score : 1.0\n",
      "[131/1000]\n",
      "- Train Loss : 0.004654430525584353 Score : 1.0\n",
      "- Val Loss : 0.003957313019782305 Score : 1.0\n",
      "[132/1000]\n",
      "- Train Loss : 0.004561038351514273 Score : 1.0\n",
      "- Val Loss : 0.0038759985473006964 Score : 1.0\n",
      "[133/1000]\n",
      "- Train Loss : 0.0044703988047937555 Score : 1.0\n",
      "- Val Loss : 0.0037971388082951307 Score : 1.0\n",
      "[134/1000]\n",
      "- Train Loss : 0.004382351801420252 Score : 1.0\n",
      "- Val Loss : 0.0037206318229436874 Score : 1.0\n",
      "[135/1000]\n",
      "- Train Loss : 0.004296806873753667 Score : 1.0\n",
      "- Val Loss : 0.003646379103884101 Score : 1.0\n",
      "[136/1000]\n",
      "- Train Loss : 0.004213662274802725 Score : 1.0\n",
      "- Val Loss : 0.0035743042826652527 Score : 1.0\n",
      "[137/1000]\n",
      "- Train Loss : 0.0041328125322858495 Score : 1.0\n",
      "- Val Loss : 0.0035042953677475452 Score : 1.0\n",
      "[138/1000]\n",
      "- Train Loss : 0.004054191625780529 Score : 1.0\n",
      "- Val Loss : 0.003436286235228181 Score : 1.0\n",
      "[139/1000]\n",
      "- Train Loss : 0.003977758421873053 Score : 1.0\n",
      "- Val Loss : 0.003370099002495408 Score : 1.0\n",
      "[140/1000]\n",
      "- Train Loss : 0.0039031216227966878 Score : 1.0\n",
      "- Val Loss : 0.0033053371589630842 Score : 1.0\n",
      "[141/1000]\n",
      "- Train Loss : 0.003830733126960695 Score : 1.0\n",
      "- Val Loss : 0.003242274047806859 Score : 1.0\n",
      "[142/1000]\n",
      "- Train Loss : 0.003760379299314486 Score : 1.0\n",
      "- Val Loss : 0.0031809695065021515 Score : 1.0\n",
      "[143/1000]\n",
      "- Train Loss : 0.003691903147329059 Score : 1.0\n",
      "- Val Loss : 0.003121405141428113 Score : 1.0\n",
      "[144/1000]\n",
      "- Train Loss : 0.003625225831961466 Score : 1.0\n",
      "- Val Loss : 0.003063526703044772 Score : 1.0\n",
      "[145/1000]\n",
      "- Train Loss : 0.0035602678544819355 Score : 1.0\n",
      "- Val Loss : 0.0030072887893766165 Score : 1.0\n",
      "[146/1000]\n",
      "- Train Loss : 0.0034964822180983094 Score : 1.0\n",
      "- Val Loss : 0.002952044829726219 Score : 1.0\n",
      "[147/1000]\n",
      "- Train Loss : 0.003435000001142422 Score : 1.0\n",
      "- Val Loss : 0.0028981801588088274 Score : 1.0\n",
      "[148/1000]\n",
      "- Train Loss : 0.0033752950192946526 Score : 1.0\n",
      "- Val Loss : 0.0028457941953092813 Score : 1.0\n",
      "[149/1000]\n",
      "- Train Loss : 0.0033171810856502918 Score : 1.0\n",
      "- Val Loss : 0.002794925356283784 Score : 1.0\n",
      "[150/1000]\n",
      "- Train Loss : 0.003260532756232553 Score : 1.0\n",
      "- Val Loss : 0.002745510544627905 Score : 1.0\n",
      "[151/1000]\n",
      "- Train Loss : 0.0032052750466391444 Score : 1.0\n",
      "- Val Loss : 0.002697453135624528 Score : 1.0\n",
      "[152/1000]\n",
      "- Train Loss : 0.003151343074730701 Score : 1.0\n",
      "- Val Loss : 0.002650719601660967 Score : 1.0\n",
      "[153/1000]\n",
      "- Train Loss : 0.0030986894724062746 Score : 1.0\n",
      "- Val Loss : 0.00260519958101213 Score : 1.0\n",
      "[154/1000]\n",
      "- Train Loss : 0.0030472833896055818 Score : 1.0\n",
      "- Val Loss : 0.0025608583819121122 Score : 1.0\n",
      "[155/1000]\n",
      "- Train Loss : 0.002997076009503669 Score : 1.0\n",
      "- Val Loss : 0.0025176256895065308 Score : 1.0\n",
      "[156/1000]\n",
      "- Train Loss : 0.0029480349557060334 Score : 1.0\n",
      "- Val Loss : 0.002475467976182699 Score : 1.0\n",
      "[157/1000]\n",
      "- Train Loss : 0.0029001337113893693 Score : 1.0\n",
      "- Val Loss : 0.002434316324070096 Score : 1.0\n",
      "[158/1000]\n",
      "- Train Loss : 0.002853314754449659 Score : 1.0\n",
      "- Val Loss : 0.0023941565304994583 Score : 1.0\n",
      "[159/1000]\n",
      "- Train Loss : 0.0028075780460817944 Score : 1.0\n",
      "- Val Loss : 0.0023549573961645365 Score : 1.0\n",
      "[160/1000]\n",
      "- Train Loss : 0.002762872661049995 Score : 1.0\n",
      "- Val Loss : 0.002316658152267337 Score : 1.0\n",
      "[161/1000]\n",
      "- Train Loss : 0.002719172690477636 Score : 1.0\n",
      "- Val Loss : 0.002279255073517561 Score : 1.0\n",
      "[162/1000]\n",
      "- Train Loss : 0.0026764440117403865 Score : 1.0\n",
      "- Val Loss : 0.0022426883224397898 Score : 1.0\n",
      "[163/1000]\n",
      "- Train Loss : 0.002634677065846821 Score : 1.0\n",
      "- Val Loss : 0.0022069618571549654 Score : 1.0\n",
      "[164/1000]\n",
      "- Train Loss : 0.002593816814219786 Score : 1.0\n",
      "- Val Loss : 0.0021720395889133215 Score : 1.0\n",
      "[165/1000]\n",
      "- Train Loss : 0.0025538529735058546 Score : 1.0\n",
      "- Val Loss : 0.002137900562956929 Score : 1.0\n",
      "[166/1000]\n",
      "- Train Loss : 0.0025147608895268706 Score : 1.0\n",
      "- Val Loss : 0.002104503568261862 Score : 1.0\n",
      "[167/1000]\n",
      "- Train Loss : 0.0024765101520137656 Score : 1.0\n",
      "- Val Loss : 0.0020718451123684645 Score : 1.0\n",
      "[168/1000]\n",
      "- Train Loss : 0.002439076882890529 Score : 1.0\n",
      "- Val Loss : 0.0020398921333253384 Score : 1.0\n",
      "[169/1000]\n",
      "- Train Loss : 0.002402448677457869 Score : 1.0\n",
      "- Val Loss : 0.0020086902659386396 Score : 1.0\n",
      "[170/1000]\n",
      "- Train Loss : 0.0023665768870462975 Score : 1.0\n",
      "- Val Loss : 0.00197815103456378 Score : 1.0\n",
      "[171/1000]\n",
      "- Train Loss : 0.002331477434684833 Score : 1.0\n",
      "- Val Loss : 0.0019482632633298635 Score : 1.0\n",
      "[172/1000]\n",
      "- Train Loss : 0.00229710026178509 Score : 1.0\n",
      "- Val Loss : 0.0019190183375030756 Score : 1.0\n",
      "[173/1000]\n",
      "- Train Loss : 0.0022634297816289794 Score : 1.0\n",
      "- Val Loss : 0.0018903677118942142 Score : 1.0\n",
      "[174/1000]\n",
      "- Train Loss : 0.0022304559436937175 Score : 1.0\n",
      "- Val Loss : 0.001862328383140266 Score : 1.0\n",
      "[175/1000]\n",
      "- Train Loss : 0.0021981588150891992 Score : 1.0\n",
      "- Val Loss : 0.0018348654266446829 Score : 1.0\n",
      "[176/1000]\n",
      "- Train Loss : 0.0021665148281802735 Score : 1.0\n",
      "- Val Loss : 0.0018079662695527077 Score : 1.0\n",
      "[177/1000]\n",
      "- Train Loss : 0.002135437997316735 Score : 1.0\n",
      "- Val Loss : 0.0017815607134252787 Score : 1.0\n",
      "[178/1000]\n",
      "- Train Loss : 0.0021050923322844836 Score : 1.0\n",
      "- Val Loss : 0.0017556730890646577 Score : 1.0\n",
      "[179/1000]\n",
      "- Train Loss : 0.002075394175739752 Score : 1.0\n",
      "- Val Loss : 0.0017303181812167168 Score : 1.0\n",
      "[180/1000]\n",
      "- Train Loss : 0.002046290882087002 Score : 1.0\n",
      "- Val Loss : 0.0017054921481758356 Score : 1.0\n",
      "[181/1000]\n",
      "- Train Loss : 0.0020177929675103063 Score : 1.0\n",
      "- Val Loss : 0.0016812158282846212 Score : 1.0\n",
      "[182/1000]\n",
      "- Train Loss : 0.0019899082124336725 Score : 1.0\n",
      "- Val Loss : 0.0016574548790231347 Score : 1.0\n",
      "[183/1000]\n",
      "- Train Loss : 0.001962547623811083 Score : 1.0\n",
      "- Val Loss : 0.0016341801965609193 Score : 1.0\n",
      "[184/1000]\n",
      "- Train Loss : 0.001935706564432217 Score : 1.0\n",
      "- Val Loss : 0.001611392362974584 Score : 1.0\n",
      "[185/1000]\n",
      "- Train Loss : 0.0019093647974336313 Score : 1.0\n",
      "- Val Loss : 0.0015890354989096522 Score : 1.0\n",
      "[186/1000]\n",
      "- Train Loss : 0.0018835259316903022 Score : 1.0\n",
      "- Val Loss : 0.0015671000583097339 Score : 1.0\n",
      "[187/1000]\n",
      "- Train Loss : 0.0018581201115416156 Score : 1.0\n",
      "- Val Loss : 0.0015455760294571519 Score : 1.0\n",
      "[188/1000]\n",
      "- Train Loss : 0.0018332503451448348 Score : 1.0\n",
      "- Val Loss : 0.001524459570646286 Score : 1.0\n",
      "[189/1000]\n",
      "- Train Loss : 0.0018088495384694801 Score : 1.0\n",
      "- Val Loss : 0.0015037214616313577 Score : 1.0\n",
      "[190/1000]\n",
      "- Train Loss : 0.0017849118836844962 Score : 1.0\n",
      "- Val Loss : 0.0014833867317065597 Score : 1.0\n",
      "[191/1000]\n",
      "- Train Loss : 0.0017614056123420596 Score : 1.0\n",
      "- Val Loss : 0.0014634403632953763 Score : 1.0\n",
      "[192/1000]\n",
      "- Train Loss : 0.0017383334343321621 Score : 1.0\n",
      "- Val Loss : 0.0014438709476962686 Score : 1.0\n",
      "[193/1000]\n",
      "- Train Loss : 0.0017156804032209846 Score : 1.0\n",
      "- Val Loss : 0.001424657297320664 Score : 1.0\n",
      "[194/1000]\n",
      "- Train Loss : 0.001693431527302083 Score : 1.0\n",
      "- Val Loss : 0.0014057978987693787 Score : 1.0\n",
      "[195/1000]\n",
      "- Train Loss : 0.001671581302717742 Score : 1.0\n",
      "- Val Loss : 0.0013872974086552858 Score : 1.0\n",
      "[196/1000]\n",
      "- Train Loss : 0.0016500663542602626 Score : 1.0\n",
      "- Val Loss : 0.001369092264212668 Score : 1.0\n",
      "[197/1000]\n",
      "- Train Loss : 0.001629000872425321 Score : 1.0\n",
      "- Val Loss : 0.0013512353179976344 Score : 1.0\n",
      "[198/1000]\n",
      "- Train Loss : 0.0016083269664603802 Score : 1.0\n",
      "- Val Loss : 0.001333693158812821 Score : 1.0\n",
      "[199/1000]\n",
      "- Train Loss : 0.0015880258029533757 Score : 1.0\n",
      "- Val Loss : 0.0013164746342226863 Score : 1.0\n",
      "[200/1000]\n",
      "- Train Loss : 0.0015680780246232946 Score : 1.0\n",
      "- Val Loss : 0.0012995656579732895 Score : 1.0\n",
      "[201/1000]\n",
      "- Train Loss : 0.001548478172885047 Score : 1.0\n",
      "- Val Loss : 0.0012829628540202975 Score : 1.0\n",
      "[202/1000]\n",
      "- Train Loss : 0.0015292107839033836 Score : 1.0\n",
      "- Val Loss : 0.00126667320728302 Score : 1.0\n",
      "[203/1000]\n",
      "- Train Loss : 0.0015102740015006727 Score : 1.0\n",
      "- Val Loss : 0.0012506588827818632 Score : 1.0\n",
      "[204/1000]\n",
      "- Train Loss : 0.0014916628392206298 Score : 1.0\n",
      "- Val Loss : 0.0012349250027909875 Score : 1.0\n",
      "[205/1000]\n",
      "- Train Loss : 0.0014733558895790742 Score : 1.0\n",
      "- Val Loss : 0.001219470053911209 Score : 1.0\n",
      "[206/1000]\n",
      "- Train Loss : 0.0014553637269677387 Score : 1.0\n",
      "- Val Loss : 0.001204284722916782 Score : 1.0\n",
      "[207/1000]\n",
      "- Train Loss : 0.0014376690960489213 Score : 1.0\n",
      "- Val Loss : 0.0011893569026142359 Score : 1.0\n",
      "[208/1000]\n",
      "- Train Loss : 0.001420267443690035 Score : 1.0\n",
      "- Val Loss : 0.0011746789095923305 Score : 1.0\n",
      "[209/1000]\n",
      "- Train Loss : 0.0014031590609293845 Score : 1.0\n",
      "- Val Loss : 0.0011602701852098107 Score : 1.0\n",
      "[210/1000]\n",
      "- Train Loss : 0.0013863203219241565 Score : 1.0\n",
      "- Val Loss : 0.0011461034882813692 Score : 1.0\n",
      "[211/1000]\n",
      "- Train Loss : 0.001369768229778856 Score : 1.0\n",
      "- Val Loss : 0.0011321604251861572 Score : 1.0\n",
      "[212/1000]\n",
      "- Train Loss : 0.0013534796113769214 Score : 1.0\n",
      "- Val Loss : 0.0011184485629200935 Score : 1.0\n",
      "[213/1000]\n",
      "- Train Loss : 0.0013374817531762852 Score : 1.0\n",
      "- Val Loss : 0.001104957889765501 Score : 1.0\n",
      "[214/1000]\n",
      "- Train Loss : 0.001321743830986735 Score : 1.0\n",
      "- Val Loss : 0.0010916977189481258 Score : 1.0\n",
      "[215/1000]\n",
      "- Train Loss : 0.0013062598882243037 Score : 1.0\n",
      "- Val Loss : 0.0010786444181576371 Score : 1.0\n",
      "[216/1000]\n",
      "- Train Loss : 0.0012910280363737708 Score : 1.0\n",
      "- Val Loss : 0.0010658164974302053 Score : 1.0\n",
      "[217/1000]\n",
      "- Train Loss : 0.0012760303151379856 Score : 1.0\n",
      "- Val Loss : 0.001053197425790131 Score : 1.0\n",
      "[218/1000]\n",
      "- Train Loss : 0.0012612784436593454 Score : 1.0\n",
      "- Val Loss : 0.0010407757945358753 Score : 1.0\n",
      "[219/1000]\n",
      "- Train Loss : 0.0012467518099583685 Score : 1.0\n",
      "- Val Loss : 0.0010285588214173913 Score : 1.0\n",
      "[220/1000]\n",
      "- Train Loss : 0.001232454233104363 Score : 1.0\n",
      "- Val Loss : 0.0010165366111323237 Score : 1.0\n",
      "[221/1000]\n",
      "- Train Loss : 0.001218382980571025 Score : 1.0\n",
      "- Val Loss : 0.0010047074174508452 Score : 1.0\n",
      "[222/1000]\n",
      "- Train Loss : 0.0012045258675546695 Score : 1.0\n",
      "- Val Loss : 0.0009930654196068645 Score : 1.0\n",
      "[223/1000]\n",
      "- Train Loss : 0.001190877807352485 Score : 1.0\n",
      "- Val Loss : 0.0009816031670197845 Score : 1.0\n",
      "[224/1000]\n",
      "- Train Loss : 0.0011774386544453187 Score : 1.0\n",
      "- Val Loss : 0.0009703185060061514 Score : 1.0\n",
      "[225/1000]\n",
      "- Train Loss : 0.0011642034094418502 Score : 1.0\n",
      "- Val Loss : 0.0009592355345375836 Score : 1.0\n",
      "[226/1000]\n",
      "- Train Loss : 0.0011511781776789576 Score : 1.0\n",
      "- Val Loss : 0.000948307744693011 Score : 1.0\n",
      "[227/1000]\n",
      "- Train Loss : 0.0011383402969739917 Score : 1.0\n",
      "- Val Loss : 0.0009375723893754184 Score : 1.0\n",
      "[228/1000]\n",
      "- Train Loss : 0.0011256987701118407 Score : 1.0\n",
      "- Val Loss : 0.0009270099690183997 Score : 1.0\n",
      "[229/1000]\n",
      "- Train Loss : 0.0011132352552117987 Score : 1.0\n",
      "- Val Loss : 0.0009166732779704034 Score : 1.0\n",
      "[230/1000]\n",
      "- Train Loss : 0.0011009626743745888 Score : 1.0\n",
      "- Val Loss : 0.000906475237570703 Score : 1.0\n",
      "[231/1000]\n",
      "- Train Loss : 0.001088884060866096 Score : 1.0\n",
      "- Val Loss : 0.0008964315056800842 Score : 1.0\n",
      "[232/1000]\n",
      "- Train Loss : 0.0010769913723278376 Score : 1.0\n",
      "- Val Loss : 0.0008865505224093795 Score : 1.0\n",
      "[233/1000]\n",
      "- Train Loss : 0.001065349688158474 Score : 1.0\n",
      "- Val Loss : 0.0008767714607529342 Score : 1.0\n",
      "[234/1000]\n",
      "- Train Loss : 0.001053701064342426 Score : 1.0\n",
      "- Val Loss : 0.0008670349488966167 Score : 1.0\n",
      "[235/1000]\n",
      "- Train Loss : 0.0010423562715813103 Score : 1.0\n",
      "- Val Loss : 0.0008574589737690985 Score : 1.0\n",
      "[236/1000]\n",
      "- Train Loss : 0.0010312606981541547 Score : 1.0\n",
      "- Val Loss : 0.0008481199038214982 Score : 1.0\n",
      "[237/1000]\n",
      "- Train Loss : 0.0010202638449199083 Score : 1.0\n",
      "- Val Loss : 0.0008389552822336555 Score : 1.0\n",
      "[238/1000]\n",
      "- Train Loss : 0.0010094273473239606 Score : 1.0\n",
      "- Val Loss : 0.0008299060282297432 Score : 1.0\n",
      "[239/1000]\n",
      "- Train Loss : 0.0009985980513091716 Score : 1.0\n",
      "- Val Loss : 0.000820918008685112 Score : 1.0\n",
      "[240/1000]\n",
      "- Train Loss : 0.0009880981880188403 Score : 1.0\n",
      "- Val Loss : 0.0008121049031615257 Score : 1.0\n",
      "[241/1000]\n",
      "- Train Loss : 0.0009776392156103004 Score : 1.0\n",
      "- Val Loss : 0.0008033648482523859 Score : 1.0\n",
      "[242/1000]\n",
      "- Train Loss : 0.0009674587345216423 Score : 1.0\n",
      "- Val Loss : 0.0007948405109345913 Score : 1.0\n",
      "[243/1000]\n",
      "- Train Loss : 0.0009573880298477081 Score : 1.0\n",
      "- Val Loss : 0.0007864183862693608 Score : 1.0\n",
      "[244/1000]\n",
      "- Train Loss : 0.0009473366565847149 Score : 1.0\n",
      "- Val Loss : 0.0007780881715007126 Score : 1.0\n",
      "[245/1000]\n",
      "- Train Loss : 0.0009375730975686262 Score : 1.0\n",
      "- Val Loss : 0.0007698772824369371 Score : 1.0\n",
      "[246/1000]\n",
      "- Train Loss : 0.0009278416012724241 Score : 1.0\n",
      "- Val Loss : 0.0007617879309691489 Score : 1.0\n",
      "[247/1000]\n",
      "- Train Loss : 0.0009183478218296336 Score : 1.0\n",
      "- Val Loss : 0.0007538480567745864 Score : 1.0\n",
      "[248/1000]\n",
      "- Train Loss : 0.0009089541692648911 Score : 1.0\n",
      "- Val Loss : 0.0007460236665792763 Score : 1.0\n",
      "[249/1000]\n",
      "- Train Loss : 0.0008995774518957154 Score : 1.0\n",
      "- Val Loss : 0.0007382614421658218 Score : 1.0\n",
      "[250/1000]\n",
      "- Train Loss : 0.0008904720824729237 Score : 1.0\n",
      "- Val Loss : 0.0007306328625418246 Score : 1.0\n",
      "[251/1000]\n",
      "- Train Loss : 0.0008813784354262882 Score : 1.0\n",
      "- Val Loss : 0.0007230924675241113 Score : 1.0\n",
      "[252/1000]\n",
      "- Train Loss : 0.000872502109915432 Score : 1.0\n",
      "- Val Loss : 0.000715685891918838 Score : 1.0\n",
      "[253/1000]\n",
      "- Train Loss : 0.0008637377145027535 Score : 1.0\n",
      "- Val Loss : 0.0007083705859258771 Score : 1.0\n",
      "[254/1000]\n",
      "- Train Loss : 0.0008549807438006004 Score : 1.0\n",
      "- Val Loss : 0.0007011236739344895 Score : 1.0\n",
      "[255/1000]\n",
      "- Train Loss : 0.0008465066138241026 Score : 1.0\n",
      "- Val Loss : 0.0006940016755834222 Score : 1.0\n",
      "[256/1000]\n",
      "- Train Loss : 0.0008379856169792927 Score : 1.0\n",
      "- Val Loss : 0.0006869480130262673 Score : 1.0\n",
      "[257/1000]\n",
      "- Train Loss : 0.0008297339017089042 Score : 1.0\n",
      "- Val Loss : 0.0006800092523917556 Score : 1.0\n",
      "[258/1000]\n",
      "- Train Loss : 0.0008215634637357047 Score : 1.0\n",
      "- Val Loss : 0.0006731796893291175 Score : 1.0\n",
      "[259/1000]\n",
      "- Train Loss : 0.0008134407980833203 Score : 1.0\n",
      "- Val Loss : 0.0006664125830866396 Score : 1.0\n",
      "[260/1000]\n",
      "- Train Loss : 0.0008055321620001147 Score : 1.0\n",
      "- Val Loss : 0.0006597788305953145 Score : 1.0\n",
      "[261/1000]\n",
      "- Train Loss : 0.0007976946039384024 Score : 1.0\n",
      "- Val Loss : 0.0006532298284582794 Score : 1.0\n",
      "[262/1000]\n",
      "- Train Loss : 0.0007898915662533707 Score : 1.0\n",
      "- Val Loss : 0.0006467443308793008 Score : 1.0\n",
      "[263/1000]\n",
      "- Train Loss : 0.00078228888903848 Score : 1.0\n",
      "- Val Loss : 0.0006403860752470791 Score : 1.0\n",
      "[264/1000]\n",
      "- Train Loss : 0.0007747766050872289 Score : 1.0\n",
      "- Val Loss : 0.0006340974359773099 Score : 1.0\n",
      "[265/1000]\n",
      "- Train Loss : 0.000767240048541377 Score : 1.0\n",
      "- Val Loss : 0.0006278701475821435 Score : 1.0\n",
      "[266/1000]\n",
      "- Train Loss : 0.0007599390422304472 Score : 1.0\n",
      "- Val Loss : 0.0006217434420250356 Score : 1.0\n",
      "[267/1000]\n",
      "- Train Loss : 0.0007526138991427919 Score : 1.0\n",
      "- Val Loss : 0.0006156713934615254 Score : 1.0\n",
      "[268/1000]\n",
      "- Train Loss : 0.000745488450370936 Score : 1.0\n",
      "- Val Loss : 0.0006097113364376128 Score : 1.0\n",
      "[269/1000]\n",
      "- Train Loss : 0.0007384379795338544 Score : 1.0\n",
      "- Val Loss : 0.0006038312567397952 Score : 1.0\n",
      "[270/1000]\n",
      "- Train Loss : 0.0007313570563888384 Score : 1.0\n",
      "- Val Loss : 0.0005979787674732506 Score : 1.0\n",
      "[271/1000]\n",
      "- Train Loss : 0.0007244461254837612 Score : 1.0\n",
      "- Val Loss : 0.0005922127165831625 Score : 1.0\n",
      "[272/1000]\n",
      "- Train Loss : 0.0007176772374401076 Score : 1.0\n",
      "- Val Loss : 0.0005865484708920121 Score : 1.0\n",
      "[273/1000]\n",
      "- Train Loss : 0.0007109438172645039 Score : 1.0\n",
      "- Val Loss : 0.0005809518625028431 Score : 1.0\n",
      "[274/1000]\n",
      "- Train Loss : 0.0007042327261943785 Score : 1.0\n",
      "- Val Loss : 0.0005754111334681511 Score : 1.0\n",
      "[275/1000]\n",
      "- Train Loss : 0.0006976955402125088 Score : 1.0\n",
      "- Val Loss : 0.0005699708708561957 Score : 1.0\n",
      "[276/1000]\n",
      "- Train Loss : 0.0006912368036056352 Score : 1.0\n",
      "- Val Loss : 0.0005645847995765507 Score : 1.0\n",
      "[277/1000]\n",
      "- Train Loss : 0.0006847359052497066 Score : 1.0\n",
      "- Val Loss : 0.000559221429284662 Score : 1.0\n",
      "[278/1000]\n",
      "- Train Loss : 0.0006783802269233598 Score : 1.0\n",
      "- Val Loss : 0.0005539393750950694 Score : 1.0\n",
      "[279/1000]\n",
      "- Train Loss : 0.0006721748504787683 Score : 1.0\n",
      "- Val Loss : 0.0005487456219270825 Score : 1.0\n",
      "[280/1000]\n",
      "- Train Loss : 0.0006660057260887697 Score : 1.0\n",
      "- Val Loss : 0.0005436193314380944 Score : 1.0\n",
      "[281/1000]\n",
      "- Train Loss : 0.0006598294162864073 Score : 1.0\n",
      "- Val Loss : 0.0005385515396483243 Score : 1.0\n",
      "[282/1000]\n",
      "- Train Loss : 0.0006538338428880605 Score : 1.0\n",
      "- Val Loss : 0.0005335246678441763 Score : 1.0\n",
      "[283/1000]\n",
      "- Train Loss : 0.0006478121552693968 Score : 1.0\n",
      "- Val Loss : 0.0005285757943056524 Score : 1.0\n",
      "[284/1000]\n",
      "- Train Loss : 0.0006419608463248652 Score : 1.0\n",
      "- Val Loss : 0.0005236808792687953 Score : 1.0\n",
      "[285/1000]\n",
      "- Train Loss : 0.0006360902981315223 Score : 1.0\n",
      "- Val Loss : 0.0005188334034755826 Score : 1.0\n",
      "[286/1000]\n",
      "- Train Loss : 0.0006303561434227353 Score : 1.0\n",
      "- Val Loss : 0.0005140716675668955 Score : 1.0\n",
      "[287/1000]\n",
      "- Train Loss : 0.0006246875838971593 Score : 1.0\n",
      "- Val Loss : 0.0005093624931760132 Score : 1.0\n",
      "[288/1000]\n",
      "- Train Loss : 0.000618993280416665 Score : 1.0\n",
      "- Val Loss : 0.0005046917358413339 Score : 1.0\n",
      "[289/1000]\n",
      "- Train Loss : 0.0006134345538965943 Score : 1.0\n",
      "- Val Loss : 0.0005000676610507071 Score : 1.0\n",
      "[290/1000]\n",
      "- Train Loss : 0.0006079997870579569 Score : 1.0\n",
      "- Val Loss : 0.0004955341573804617 Score : 1.0\n",
      "[291/1000]\n",
      "- Train Loss : 0.0006025818778046718 Score : 1.0\n",
      "- Val Loss : 0.0004910387797281146 Score : 1.0\n",
      "[292/1000]\n",
      "- Train Loss : 0.0005971783021878866 Score : 1.0\n",
      "- Val Loss : 0.00048659698222763836 Score : 1.0\n",
      "[293/1000]\n",
      "- Train Loss : 0.0005919133270961336 Score : 1.0\n",
      "- Val Loss : 0.0004822069895453751 Score : 1.0\n",
      "[294/1000]\n",
      "- Train Loss : 0.0005866415255392591 Score : 1.0\n",
      "- Val Loss : 0.00047786356299184263 Score : 1.0\n",
      "[295/1000]\n",
      "- Train Loss : 0.0005814910821047508 Score : 1.0\n",
      "- Val Loss : 0.0004735926631838083 Score : 1.0\n",
      "[296/1000]\n",
      "- Train Loss : 0.000576406369671329 Score : 1.0\n",
      "- Val Loss : 0.0004693560767918825 Score : 1.0\n",
      "[297/1000]\n",
      "- Train Loss : 0.0005712739269559582 Score : 1.0\n",
      "- Val Loss : 0.00046515552094206214 Score : 1.0\n",
      "[298/1000]\n",
      "- Train Loss : 0.0005662578429716328 Score : 1.0\n",
      "- Val Loss : 0.00046100173494778574 Score : 1.0\n",
      "[299/1000]\n",
      "- Train Loss : 0.0005613523292898511 Score : 1.0\n",
      "- Val Loss : 0.0004569146840367466 Score : 1.0\n",
      "[300/1000]\n",
      "- Train Loss : 0.0005564680355342312 Score : 1.0\n",
      "- Val Loss : 0.00045288613182492554 Score : 1.0\n",
      "[301/1000]\n",
      "- Train Loss : 0.0005515678931260481 Score : 1.0\n",
      "- Val Loss : 0.0004488711419980973 Score : 1.0\n",
      "[302/1000]\n",
      "- Train Loss : 0.0005468341874398498 Score : 1.0\n",
      "- Val Loss : 0.00044493324821814895 Score : 1.0\n",
      "[303/1000]\n",
      "- Train Loss : 0.0005420624349628472 Score : 1.0\n",
      "- Val Loss : 0.0004410256224218756 Score : 1.0\n",
      "[304/1000]\n",
      "- Train Loss : 0.0005374278876438944 Score : 1.0\n",
      "- Val Loss : 0.0004371695686131716 Score : 1.0\n",
      "[305/1000]\n",
      "- Train Loss : 0.0005327689286787063 Score : 1.0\n",
      "- Val Loss : 0.0004333457618486136 Score : 1.0\n",
      "[306/1000]\n",
      "- Train Loss : 0.0005282405424319828 Score : 1.0\n",
      "- Val Loss : 0.0004295777471270412 Score : 1.0\n",
      "[307/1000]\n",
      "- Train Loss : 0.0005236820789933619 Score : 1.0\n",
      "- Val Loss : 0.00042584672337397933 Score : 1.0\n",
      "[308/1000]\n",
      "- Train Loss : 0.0005192492244532332 Score : 1.0\n",
      "- Val Loss : 0.00042216540896333754 Score : 1.0\n",
      "[309/1000]\n",
      "- Train Loss : 0.000514855773265784 Score : 1.0\n",
      "- Val Loss : 0.0004185096768196672 Score : 1.0\n",
      "[310/1000]\n",
      "- Train Loss : 0.000510423981015467 Score : 1.0\n",
      "- Val Loss : 0.00041490100556984544 Score : 1.0\n",
      "[311/1000]\n",
      "- Train Loss : 0.0005060857931514167 Score : 1.0\n",
      "- Val Loss : 0.0004113198083359748 Score : 1.0\n",
      "[312/1000]\n",
      "- Train Loss : 0.0005018407860512121 Score : 1.0\n",
      "- Val Loss : 0.00040778439142741263 Score : 1.0\n",
      "[313/1000]\n",
      "- Train Loss : 0.0004976390272430661 Score : 1.0\n",
      "- Val Loss : 0.00040430252556689084 Score : 1.0\n",
      "[314/1000]\n",
      "- Train Loss : 0.0004933960428590783 Score : 1.0\n",
      "- Val Loss : 0.00040083780186250806 Score : 1.0\n",
      "[315/1000]\n",
      "- Train Loss : 0.0004892439788414373 Score : 1.0\n",
      "- Val Loss : 0.00039741111686453223 Score : 1.0\n",
      "[316/1000]\n",
      "- Train Loss : 0.0004851864958052627 Score : 1.0\n",
      "- Val Loss : 0.000394043221604079 Score : 1.0\n",
      "[317/1000]\n",
      "- Train Loss : 0.000481155005723445 Score : 1.0\n",
      "- Val Loss : 0.0003906961064785719 Score : 1.0\n",
      "[318/1000]\n",
      "- Train Loss : 0.00047708905185572803 Score : 1.0\n",
      "- Val Loss : 0.00038738144212402403 Score : 1.0\n",
      "[319/1000]\n",
      "- Train Loss : 0.0004731084724577765 Score : 1.0\n",
      "- Val Loss : 0.00038409745320677757 Score : 1.0\n",
      "[320/1000]\n",
      "- Train Loss : 0.0004692139983591106 Score : 1.0\n",
      "- Val Loss : 0.0003808639303315431 Score : 1.0\n",
      "[321/1000]\n",
      "- Train Loss : 0.0004653528610813535 Score : 1.0\n",
      "- Val Loss : 0.0003776556404773146 Score : 1.0\n",
      "[322/1000]\n",
      "- Train Loss : 0.00046145720342691574 Score : 1.0\n",
      "- Val Loss : 0.0003744890564121306 Score : 1.0\n",
      "[323/1000]\n",
      "- Train Loss : 0.0004576422361424193 Score : 1.0\n",
      "- Val Loss : 0.00037133877049200237 Score : 1.0\n",
      "[324/1000]\n",
      "- Train Loss : 0.0004539147630566731 Score : 1.0\n",
      "- Val Loss : 0.0003682500100694597 Score : 1.0\n",
      "[325/1000]\n",
      "- Train Loss : 0.00045020840156616434 Score : 1.0\n",
      "- Val Loss : 0.00036517702392302454 Score : 1.0\n",
      "[326/1000]\n",
      "- Train Loss : 0.00044647125630742975 Score : 1.0\n",
      "- Val Loss : 0.00036213346174918115 Score : 1.0\n",
      "[327/1000]\n",
      "- Train Loss : 0.0004428099701827806 Score : 1.0\n",
      "- Val Loss : 0.0003591129498090595 Score : 1.0\n",
      "[328/1000]\n",
      "- Train Loss : 0.0004392309945413015 Score : 1.0\n",
      "- Val Loss : 0.0003561482299119234 Score : 1.0\n",
      "[329/1000]\n",
      "- Train Loss : 0.00043568099176304205 Score : 1.0\n",
      "- Val Loss : 0.00035320789902471006 Score : 1.0\n",
      "[330/1000]\n",
      "- Train Loss : 0.0004320976650989097 Score : 1.0\n",
      "- Val Loss : 0.0003502973704598844 Score : 1.0\n",
      "[331/1000]\n",
      "- Train Loss : 0.00042858233791776 Score : 1.0\n",
      "- Val Loss : 0.0003473967663012445 Score : 1.0\n",
      "[332/1000]\n",
      "- Train Loss : 0.00042514796789166413 Score : 1.0\n",
      "- Val Loss : 0.0003445469483267516 Score : 1.0\n",
      "[333/1000]\n",
      "- Train Loss : 0.00042174690881640546 Score : 1.0\n",
      "- Val Loss : 0.0003417225962039083 Score : 1.0\n",
      "[334/1000]\n",
      "- Train Loss : 0.000418304507749983 Score : 1.0\n",
      "- Val Loss : 0.00033891198108904064 Score : 1.0\n",
      "[335/1000]\n",
      "- Train Loss : 0.00041491985191694565 Score : 1.0\n",
      "- Val Loss : 0.0003361409471835941 Score : 1.0\n",
      "[336/1000]\n",
      "- Train Loss : 0.00041162543089335994 Score : 1.0\n",
      "- Val Loss : 0.0003334155771881342 Score : 1.0\n",
      "[337/1000]\n",
      "- Train Loss : 0.00040830356495765346 Score : 1.0\n",
      "- Val Loss : 0.00033070988138206303 Score : 1.0\n",
      "[338/1000]\n",
      "- Train Loss : 0.0004050662213962318 Score : 1.0\n",
      "- Val Loss : 0.00032803555950522423 Score : 1.0\n",
      "[339/1000]\n",
      "- Train Loss : 0.00040182702327405825 Score : 1.0\n",
      "- Val Loss : 0.0003253801551181823 Score : 1.0\n",
      "[340/1000]\n",
      "- Train Loss : 0.00039866082079242915 Score : 1.0\n",
      "- Val Loss : 0.00032275239937007427 Score : 1.0\n",
      "[341/1000]\n",
      "- Train Loss : 0.00039553423509156954 Score : 1.0\n",
      "- Val Loss : 0.00032016480690799654 Score : 1.0\n",
      "[342/1000]\n",
      "- Train Loss : 0.00039236075584388647 Score : 1.0\n",
      "- Val Loss : 0.0003175874298904091 Score : 1.0\n",
      "[343/1000]\n",
      "- Train Loss : 0.0003892507981314945 Score : 1.0\n",
      "- Val Loss : 0.00031503665377385914 Score : 1.0\n",
      "[344/1000]\n",
      "- Train Loss : 0.00038621900279799267 Score : 1.0\n",
      "- Val Loss : 0.0003125153307337314 Score : 1.0\n",
      "[345/1000]\n",
      "- Train Loss : 0.00038316849936058535 Score : 1.0\n",
      "- Val Loss : 0.00031002453761175275 Score : 1.0\n",
      "[346/1000]\n",
      "- Train Loss : 0.0003801890068441733 Score : 1.0\n",
      "- Val Loss : 0.0003075502463616431 Score : 1.0\n",
      "[347/1000]\n",
      "- Train Loss : 0.00037719111585627415 Score : 1.0\n",
      "- Val Loss : 0.0003050982195418328 Score : 1.0\n",
      "[348/1000]\n",
      "- Train Loss : 0.00037426581224685325 Score : 1.0\n",
      "- Val Loss : 0.00030267369584180415 Score : 1.0\n",
      "[349/1000]\n",
      "- Train Loss : 0.0003713225637006366 Score : 1.0\n",
      "- Val Loss : 0.0003002821758855134 Score : 1.0\n",
      "[350/1000]\n",
      "- Train Loss : 0.00036845534325241007 Score : 1.0\n",
      "- Val Loss : 0.0002979114942718297 Score : 1.0\n",
      "[351/1000]\n",
      "- Train Loss : 0.00036557483771401975 Score : 1.0\n",
      "- Val Loss : 0.0002955501840915531 Score : 1.0\n",
      "[352/1000]\n",
      "- Train Loss : 0.00036276308269912586 Score : 1.0\n",
      "- Val Loss : 0.00029322988120839 Score : 1.0\n",
      "[353/1000]\n",
      "- Train Loss : 0.00035994164580996666 Score : 1.0\n",
      "- Val Loss : 0.00029093012562952936 Score : 1.0\n",
      "[354/1000]\n",
      "- Train Loss : 0.0003571867871667362 Score : 1.0\n",
      "- Val Loss : 0.0002886473957914859 Score : 1.0\n",
      "[355/1000]\n",
      "- Train Loss : 0.0003544205844971455 Score : 1.0\n",
      "- Val Loss : 0.0002863877161871642 Score : 1.0\n",
      "[356/1000]\n",
      "- Train Loss : 0.0003517174538703532 Score : 1.0\n",
      "- Val Loss : 0.0002841492823790759 Score : 1.0\n",
      "[357/1000]\n",
      "- Train Loss : 0.0003489970864999729 Score : 1.0\n",
      "- Val Loss : 0.0002819253131747246 Score : 1.0\n",
      "[358/1000]\n",
      "- Train Loss : 0.00034635239717317745 Score : 1.0\n",
      "- Val Loss : 0.0002797288470901549 Score : 1.0\n",
      "[359/1000]\n",
      "- Train Loss : 0.00034367665405928466 Score : 1.0\n",
      "- Val Loss : 0.0002775528409983963 Score : 1.0\n",
      "[360/1000]\n",
      "- Train Loss : 0.00034108332814260695 Score : 1.0\n",
      "- Val Loss : 0.00027539991424418986 Score : 1.0\n",
      "[361/1000]\n",
      "- Train Loss : 0.0003384542586799297 Score : 1.0\n",
      "- Val Loss : 0.0002732680586632341 Score : 1.0\n",
      "[362/1000]\n",
      "- Train Loss : 0.00033591826488393254 Score : 1.0\n",
      "- Val Loss : 0.0002711474662646651 Score : 1.0\n",
      "[363/1000]\n",
      "- Train Loss : 0.0003333371051768255 Score : 1.0\n",
      "- Val Loss : 0.00026904919650405645 Score : 1.0\n",
      "[364/1000]\n",
      "- Train Loss : 0.00033081207786583237 Score : 1.0\n",
      "- Val Loss : 0.0002669781388249248 Score : 1.0\n",
      "[365/1000]\n",
      "- Train Loss : 0.00032833941562178853 Score : 1.0\n",
      "- Val Loss : 0.0002649206144269556 Score : 1.0\n",
      "[366/1000]\n",
      "- Train Loss : 0.0003258738296507444 Score : 1.0\n",
      "- Val Loss : 0.0002628829679451883 Score : 1.0\n",
      "[367/1000]\n",
      "- Train Loss : 0.00032338213592690107 Score : 1.0\n",
      "- Val Loss : 0.0002608664508443326 Score : 1.0\n",
      "[368/1000]\n",
      "- Train Loss : 0.0003209501301171258 Score : 1.0\n",
      "- Val Loss : 0.0002588657080195844 Score : 1.0\n",
      "[369/1000]\n",
      "- Train Loss : 0.00031855990932348906 Score : 1.0\n",
      "- Val Loss : 0.00025688009918667376 Score : 1.0\n",
      "[370/1000]\n",
      "- Train Loss : 0.00031619510587511794 Score : 1.0\n",
      "- Val Loss : 0.00025492001441307366 Score : 1.0\n",
      "[371/1000]\n",
      "- Train Loss : 0.00031379712648534525 Score : 1.0\n",
      "- Val Loss : 0.00025297925458289683 Score : 1.0\n",
      "[372/1000]\n",
      "- Train Loss : 0.00031143283397088654 Score : 1.0\n",
      "- Val Loss : 0.0002510586637072265 Score : 1.0\n",
      "[373/1000]\n",
      "- Train Loss : 0.00030914210381322645 Score : 1.0\n",
      "- Val Loss : 0.0002491525374352932 Score : 1.0\n",
      "[374/1000]\n",
      "- Train Loss : 0.00030682413166181906 Score : 1.0\n",
      "- Val Loss : 0.0002472529886290431 Score : 1.0\n",
      "[375/1000]\n",
      "- Train Loss : 0.0003045712033377236 Score : 1.0\n",
      "- Val Loss : 0.00024537855642847717 Score : 1.0\n",
      "[376/1000]\n",
      "- Train Loss : 0.0003022785951745593 Score : 1.0\n",
      "- Val Loss : 0.00024351882166229188 Score : 1.0\n",
      "[377/1000]\n",
      "- Train Loss : 0.0003000716452435073 Score : 1.0\n",
      "- Val Loss : 0.00024169310927391052 Score : 1.0\n",
      "[378/1000]\n",
      "- Train Loss : 0.0002978219919087779 Score : 1.0\n",
      "- Val Loss : 0.0002398633660050109 Score : 1.0\n",
      "[379/1000]\n",
      "- Train Loss : 0.0002956164236012329 Score : 1.0\n",
      "- Val Loss : 0.00023804951342754066 Score : 1.0\n",
      "[380/1000]\n",
      "- Train Loss : 0.0002934671218907978 Score : 1.0\n",
      "- Val Loss : 0.00023626280017197132 Score : 1.0\n",
      "[381/1000]\n",
      "- Train Loss : 0.00029132617459658324 Score : 1.0\n",
      "- Val Loss : 0.00023449126456398517 Score : 1.0\n",
      "[382/1000]\n",
      "- Train Loss : 0.0002891507198607239 Score : 1.0\n",
      "- Val Loss : 0.00023273631813935935 Score : 1.0\n",
      "[383/1000]\n",
      "- Train Loss : 0.00028701643087616604 Score : 1.0\n",
      "- Val Loss : 0.00023099272220861167 Score : 1.0\n",
      "[384/1000]\n",
      "- Train Loss : 0.0002849471929948777 Score : 1.0\n",
      "- Val Loss : 0.00022925782832317054 Score : 1.0\n",
      "[385/1000]\n",
      "- Train Loss : 0.0002828460346790962 Score : 1.0\n",
      "- Val Loss : 0.00022754025121685117 Score : 1.0\n",
      "[386/1000]\n",
      "- Train Loss : 0.0002808110495809362 Score : 1.0\n",
      "- Val Loss : 0.00022584968246519566 Score : 1.0\n",
      "[387/1000]\n",
      "- Train Loss : 0.0002787337256854193 Score : 1.0\n",
      "- Val Loss : 0.00022417287982534617 Score : 1.0\n",
      "[388/1000]\n",
      "- Train Loss : 0.0002767075170090215 Score : 1.0\n",
      "- Val Loss : 0.0002225010102847591 Score : 1.0\n",
      "[389/1000]\n",
      "- Train Loss : 0.00027471958472031273 Score : 1.0\n",
      "- Val Loss : 0.00022084319789428264 Score : 1.0\n",
      "[390/1000]\n",
      "- Train Loss : 0.0002727461129931423 Score : 1.0\n",
      "- Val Loss : 0.00021920146537013352 Score : 1.0\n",
      "[391/1000]\n",
      "- Train Loss : 0.0002707376859487138 Score : 1.0\n",
      "- Val Loss : 0.0002175832778448239 Score : 1.0\n",
      "[392/1000]\n",
      "- Train Loss : 0.0002687680130798577 Score : 1.0\n",
      "- Val Loss : 0.0002159761788789183 Score : 1.0\n",
      "[393/1000]\n",
      "- Train Loss : 0.00026684917975217104 Score : 1.0\n",
      "- Val Loss : 0.0002143793972209096 Score : 1.0\n",
      "[394/1000]\n",
      "- Train Loss : 0.0002649063923551391 Score : 1.0\n",
      "- Val Loss : 0.00021280131477396935 Score : 1.0\n",
      "[395/1000]\n",
      "- Train Loss : 0.0002630289867132281 Score : 1.0\n",
      "- Val Loss : 0.00021122772886883467 Score : 1.0\n",
      "[396/1000]\n",
      "- Train Loss : 0.0002611167631888141 Score : 1.0\n",
      "- Val Loss : 0.00020967608725186437 Score : 1.0\n",
      "[397/1000]\n",
      "- Train Loss : 0.0002592464071413916 Score : 1.0\n",
      "- Val Loss : 0.00020814695744775236 Score : 1.0\n",
      "[398/1000]\n",
      "- Train Loss : 0.0002574119676460719 Score : 1.0\n",
      "- Val Loss : 0.00020661440794356167 Score : 1.0\n",
      "[399/1000]\n",
      "- Train Loss : 0.0002555897929899705 Score : 1.0\n",
      "- Val Loss : 0.00020510550530161709 Score : 1.0\n",
      "[400/1000]\n",
      "- Train Loss : 0.0002537324245268893 Score : 1.0\n",
      "- Val Loss : 0.00020360473718028516 Score : 1.0\n",
      "[401/1000]\n",
      "- Train Loss : 0.0002519091641362239 Score : 1.0\n",
      "- Val Loss : 0.0002021150867221877 Score : 1.0\n",
      "[402/1000]\n",
      "- Train Loss : 0.0002501463483592185 Score : 1.0\n",
      "- Val Loss : 0.00020064666750840843 Score : 1.0\n",
      "[403/1000]\n",
      "- Train Loss : 0.0002483495393082396 Score : 1.0\n",
      "- Val Loss : 0.00019918697944376618 Score : 1.0\n",
      "[404/1000]\n",
      "- Train Loss : 0.00024661661963263113 Score : 1.0\n",
      "- Val Loss : 0.0001977350766537711 Score : 1.0\n",
      "[405/1000]\n",
      "- Train Loss : 0.00024484399424788233 Score : 1.0\n",
      "- Val Loss : 0.00019630753376986831 Score : 1.0\n",
      "[406/1000]\n",
      "- Train Loss : 0.0002431040427634596 Score : 1.0\n",
      "- Val Loss : 0.00019488648104015738 Score : 1.0\n",
      "[407/1000]\n",
      "- Train Loss : 0.0002414088198242502 Score : 1.0\n",
      "- Val Loss : 0.0001934760803123936 Score : 1.0\n",
      "[408/1000]\n",
      "- Train Loss : 0.0002397237331024371 Score : 1.0\n",
      "- Val Loss : 0.0001920666400110349 Score : 1.0\n",
      "[409/1000]\n",
      "- Train Loss : 0.0002380035641383276 Score : 1.0\n",
      "- Val Loss : 0.00019068965048063546 Score : 1.0\n",
      "[410/1000]\n",
      "- Train Loss : 0.00023631869569524296 Score : 1.0\n",
      "- Val Loss : 0.00018931405793409795 Score : 1.0\n",
      "[411/1000]\n",
      "- Train Loss : 0.000234682726537964 Score : 1.0\n",
      "- Val Loss : 0.0001879628689493984 Score : 1.0\n",
      "[412/1000]\n",
      "- Train Loss : 0.0002330164538256617 Score : 1.0\n",
      "- Val Loss : 0.00018660661589819938 Score : 1.0\n",
      "[413/1000]\n",
      "- Train Loss : 0.00023138758034393605 Score : 1.0\n",
      "- Val Loss : 0.00018526581698097289 Score : 1.0\n",
      "[414/1000]\n",
      "- Train Loss : 0.00022978639349781183 Score : 1.0\n",
      "- Val Loss : 0.00018394799553789198 Score : 1.0\n",
      "[415/1000]\n",
      "- Train Loss : 0.0002281801345007908 Score : 1.0\n",
      "- Val Loss : 0.00018263599486090243 Score : 1.0\n",
      "[416/1000]\n",
      "- Train Loss : 0.00022655801714992977 Score : 1.0\n",
      "- Val Loss : 0.0001813481649151072 Score : 1.0\n",
      "[417/1000]\n",
      "- Train Loss : 0.00022497472915953645 Score : 1.0\n",
      "- Val Loss : 0.0001800750324036926 Score : 1.0\n",
      "[418/1000]\n",
      "- Train Loss : 0.00022342921794107597 Score : 1.0\n",
      "- Val Loss : 0.00017879506049212068 Score : 1.0\n",
      "[419/1000]\n",
      "- Train Loss : 0.00022187155102276139 Score : 1.0\n",
      "- Val Loss : 0.00017753751308191568 Score : 1.0\n",
      "[420/1000]\n",
      "- Train Loss : 0.00022034749054000713 Score : 1.0\n",
      "- Val Loss : 0.00017628842033445835 Score : 1.0\n",
      "[421/1000]\n",
      "- Train Loss : 0.0002188110403140955 Score : 1.0\n",
      "- Val Loss : 0.00017504833522252738 Score : 1.0\n",
      "[422/1000]\n",
      "- Train Loss : 0.00021731459572846588 Score : 1.0\n",
      "- Val Loss : 0.00017382310761604458 Score : 1.0\n",
      "[423/1000]\n",
      "- Train Loss : 0.000215794157055724 Score : 1.0\n",
      "- Val Loss : 0.00017260811000596732 Score : 1.0\n",
      "[424/1000]\n",
      "- Train Loss : 0.00021432572126893015 Score : 1.0\n",
      "- Val Loss : 0.00017140588897746056 Score : 1.0\n",
      "[425/1000]\n",
      "- Train Loss : 0.00021283422675979737 Score : 1.0\n",
      "- Val Loss : 0.0001702223380561918 Score : 1.0\n",
      "[426/1000]\n",
      "- Train Loss : 0.00021139486812494902 Score : 1.0\n",
      "- Val Loss : 0.0001690333738224581 Score : 1.0\n",
      "[427/1000]\n",
      "- Train Loss : 0.000209927506350343 Score : 1.0\n",
      "- Val Loss : 0.00016785373736638576 Score : 1.0\n",
      "[428/1000]\n",
      "- Train Loss : 0.00020849531776750356 Score : 1.0\n",
      "- Val Loss : 0.00016670110926497728 Score : 1.0\n",
      "[429/1000]\n",
      "- Train Loss : 0.0002070908982811185 Score : 1.0\n",
      "- Val Loss : 0.00016553382738493383 Score : 1.0\n",
      "[430/1000]\n",
      "- Train Loss : 0.00020568790366976627 Score : 1.0\n",
      "- Val Loss : 0.00016438828606624156 Score : 1.0\n",
      "[431/1000]\n",
      "- Train Loss : 0.00020426235965310803 Score : 1.0\n",
      "- Val Loss : 0.0001632624916965142 Score : 1.0\n",
      "[432/1000]\n",
      "- Train Loss : 0.00020286507383894382 Score : 1.0\n",
      "- Val Loss : 0.00016213567869272083 Score : 1.0\n",
      "[433/1000]\n",
      "- Train Loss : 0.00020150697714092935 Score : 1.0\n",
      "- Val Loss : 0.00016100730863399804 Score : 1.0\n",
      "[434/1000]\n",
      "- Train Loss : 0.00020015855382856293 Score : 1.0\n",
      "- Val Loss : 0.00015989378152880818 Score : 1.0\n",
      "[435/1000]\n",
      "- Train Loss : 0.0001987787929667522 Score : 1.0\n",
      "- Val Loss : 0.00015879623242653906 Score : 1.0\n",
      "[436/1000]\n",
      "- Train Loss : 0.00019742161991113485 Score : 1.0\n",
      "- Val Loss : 0.0001577027142047882 Score : 1.0\n",
      "[437/1000]\n",
      "- Train Loss : 0.00019610872792933756 Score : 1.0\n",
      "- Val Loss : 0.00015661613724660128 Score : 1.0\n",
      "[438/1000]\n",
      "- Train Loss : 0.00019476419078677686 Score : 1.0\n",
      "- Val Loss : 0.00015553925186395645 Score : 1.0\n",
      "[439/1000]\n",
      "- Train Loss : 0.00019347647927740277 Score : 1.0\n",
      "- Val Loss : 0.00015447479381691664 Score : 1.0\n",
      "[440/1000]\n",
      "- Train Loss : 0.00019215352626310455 Score : 1.0\n",
      "- Val Loss : 0.0001534133480163291 Score : 1.0\n",
      "[441/1000]\n",
      "- Train Loss : 0.00019086164401313808 Score : 1.0\n",
      "- Val Loss : 0.00015236937906593084 Score : 1.0\n",
      "[442/1000]\n",
      "- Train Loss : 0.00018959505339605838 Score : 1.0\n",
      "- Val Loss : 0.00015132178668864071 Score : 1.0\n",
      "[443/1000]\n",
      "- Train Loss : 0.00018833528964832012 Score : 1.0\n",
      "- Val Loss : 0.00015028119378257543 Score : 1.0\n",
      "[444/1000]\n",
      "- Train Loss : 0.0001870500503476554 Score : 1.0\n",
      "- Val Loss : 0.00014925414870958775 Score : 1.0\n",
      "[445/1000]\n",
      "- Train Loss : 0.00018578897692754658 Score : 1.0\n",
      "- Val Loss : 0.0001482378429500386 Score : 1.0\n",
      "[446/1000]\n",
      "- Train Loss : 0.00018454981020315446 Score : 1.0\n",
      "- Val Loss : 0.0001472269359510392 Score : 1.0\n",
      "[447/1000]\n",
      "- Train Loss : 0.00018333519943149036 Score : 1.0\n",
      "- Val Loss : 0.00014622100570704788 Score : 1.0\n",
      "[448/1000]\n",
      "- Train Loss : 0.0001821188606198929 Score : 1.0\n",
      "- Val Loss : 0.00014521213597618043 Score : 1.0\n",
      "[449/1000]\n",
      "- Train Loss : 0.00018088107026414946 Score : 1.0\n",
      "- Val Loss : 0.00014423161337617785 Score : 1.0\n",
      "[450/1000]\n",
      "- Train Loss : 0.00017967574806081958 Score : 1.0\n",
      "- Val Loss : 0.0001432545541319996 Score : 1.0\n",
      "[451/1000]\n",
      "- Train Loss : 0.00017849934091726836 Score : 1.0\n",
      "- Val Loss : 0.0001422819186700508 Score : 1.0\n",
      "[452/1000]\n",
      "- Train Loss : 0.00017729530211201764 Score : 1.0\n",
      "- Val Loss : 0.0001413146819686517 Score : 1.0\n",
      "[453/1000]\n",
      "- Train Loss : 0.00017613840827834792 Score : 1.0\n",
      "- Val Loss : 0.0001403499918524176 Score : 1.0\n",
      "[454/1000]\n",
      "- Train Loss : 0.00017495371098953506 Score : 1.0\n",
      "- Val Loss : 0.00013941110228188336 Score : 1.0\n",
      "[455/1000]\n",
      "- Train Loss : 0.00017379695620749972 Score : 1.0\n",
      "- Val Loss : 0.00013847211084794253 Score : 1.0\n",
      "[456/1000]\n",
      "- Train Loss : 0.00017265960357488238 Score : 1.0\n",
      "- Val Loss : 0.00013753437087871134 Score : 1.0\n",
      "[457/1000]\n",
      "- Train Loss : 0.00017151581171977645 Score : 1.0\n",
      "- Val Loss : 0.00013659900287166238 Score : 1.0\n",
      "[458/1000]\n",
      "- Train Loss : 0.00017039711181294278 Score : 1.0\n",
      "- Val Loss : 0.0001356795255560428 Score : 1.0\n",
      "[459/1000]\n",
      "- Train Loss : 0.0001692504402955011 Score : 1.0\n",
      "- Val Loss : 0.00013476212916430086 Score : 1.0\n",
      "[460/1000]\n",
      "- Train Loss : 0.00016816217814468674 Score : 1.0\n",
      "- Val Loss : 0.00013385382771957666 Score : 1.0\n",
      "[461/1000]\n",
      "- Train Loss : 0.00016703620308867862 Score : 1.0\n",
      "- Val Loss : 0.00013295758981257677 Score : 1.0\n",
      "[462/1000]\n",
      "- Train Loss : 0.00016593109628754773 Score : 1.0\n",
      "- Val Loss : 0.00013206138100940734 Score : 1.0\n",
      "[463/1000]\n",
      "- Train Loss : 0.0001648643046792131 Score : 1.0\n",
      "- Val Loss : 0.00013117605703882873 Score : 1.0\n",
      "[464/1000]\n",
      "- Train Loss : 0.0001637684504708482 Score : 1.0\n",
      "- Val Loss : 0.00013030284026172012 Score : 1.0\n",
      "[465/1000]\n",
      "- Train Loss : 0.00016270942958524555 Score : 1.0\n",
      "- Val Loss : 0.00012942681496497244 Score : 1.0\n",
      "[466/1000]\n",
      "- Train Loss : 0.00016162947981178554 Score : 1.0\n",
      "- Val Loss : 0.00012856506509706378 Score : 1.0\n",
      "[467/1000]\n",
      "- Train Loss : 0.00016057450355017662 Score : 1.0\n",
      "- Val Loss : 0.00012771211913786829 Score : 1.0\n",
      "[468/1000]\n",
      "- Train Loss : 0.00015953737642525489 Score : 1.0\n",
      "- Val Loss : 0.00012684996181633323 Score : 1.0\n",
      "[469/1000]\n",
      "- Train Loss : 0.00015848961387140056 Score : 1.0\n",
      "- Val Loss : 0.0001260102289961651 Score : 1.0\n",
      "[470/1000]\n",
      "- Train Loss : 0.00015747182826291665 Score : 1.0\n",
      "- Val Loss : 0.00012517096183728427 Score : 1.0\n",
      "[471/1000]\n",
      "- Train Loss : 0.00015643210038736774 Score : 1.0\n",
      "- Val Loss : 0.00012433411029633135 Score : 1.0\n",
      "[472/1000]\n",
      "- Train Loss : 0.00015541227791497173 Score : 1.0\n",
      "- Val Loss : 0.00012350387987680733 Score : 1.0\n",
      "[473/1000]\n",
      "- Train Loss : 0.00015441786470344395 Score : 1.0\n",
      "- Val Loss : 0.00012269157741684467 Score : 1.0\n",
      "[474/1000]\n",
      "- Train Loss : 0.00015342896585934795 Score : 1.0\n",
      "- Val Loss : 0.0001218688121298328 Score : 1.0\n",
      "[475/1000]\n",
      "- Train Loss : 0.00015241477376548573 Score : 1.0\n",
      "- Val Loss : 0.00012106371286790818 Score : 1.0\n",
      "[476/1000]\n",
      "- Train Loss : 0.00015142225957889523 Score : 1.0\n",
      "- Val Loss : 0.0001202663333970122 Score : 1.0\n",
      "[477/1000]\n",
      "- Train Loss : 0.00015044159550193904 Score : 1.0\n",
      "- Val Loss : 0.00011948194878641516 Score : 1.0\n",
      "[478/1000]\n",
      "- Train Loss : 0.0001494851623849374 Score : 1.0\n",
      "- Val Loss : 0.00011868490400956944 Score : 1.0\n",
      "[479/1000]\n",
      "- Train Loss : 0.000148533760592626 Score : 1.0\n",
      "- Val Loss : 0.00011789709969889373 Score : 1.0\n",
      "[480/1000]\n",
      "- Train Loss : 0.00014755535489206927 Score : 1.0\n",
      "- Val Loss : 0.00011712564446497709 Score : 1.0\n",
      "[481/1000]\n",
      "- Train Loss : 0.00014658964977974797 Score : 1.0\n",
      "- Val Loss : 0.00011637414718279615 Score : 1.0\n",
      "[482/1000]\n",
      "- Train Loss : 0.000145651476082599 Score : 1.0\n",
      "- Val Loss : 0.0001156223370344378 Score : 1.0\n",
      "[483/1000]\n",
      "- Train Loss : 0.00014473030185197585 Score : 1.0\n",
      "- Val Loss : 0.0001148697774624452 Score : 1.0\n",
      "[484/1000]\n",
      "- Train Loss : 0.00014381393298713697 Score : 1.0\n",
      "- Val Loss : 0.00011411181185394526 Score : 1.0\n",
      "[485/1000]\n",
      "- Train Loss : 0.00014287537002625564 Score : 1.0\n",
      "- Val Loss : 0.00011336996249156073 Score : 1.0\n",
      "[486/1000]\n",
      "- Train Loss : 0.00014195450098163242 Score : 1.0\n",
      "- Val Loss : 0.00011264299973845482 Score : 1.0\n",
      "[487/1000]\n",
      "- Train Loss : 0.00014106712438256686 Score : 1.0\n",
      "- Val Loss : 0.00011190857912879437 Score : 1.0\n",
      "[488/1000]\n",
      "- Train Loss : 0.00014015476699569263 Score : 1.0\n",
      "- Val Loss : 0.00011118903785245493 Score : 1.0\n",
      "[489/1000]\n",
      "- Train Loss : 0.00013927987028081488 Score : 1.0\n",
      "- Val Loss : 0.00011046127474401146 Score : 1.0\n",
      "[490/1000]\n",
      "- Train Loss : 0.0001383791499797048 Score : 1.0\n",
      "- Val Loss : 0.00010975015902658924 Score : 1.0\n",
      "[491/1000]\n",
      "- Train Loss : 0.00013749774855871996 Score : 1.0\n",
      "- Val Loss : 0.00010904883674811572 Score : 1.0\n",
      "[492/1000]\n",
      "- Train Loss : 0.0001366397878478488 Score : 1.0\n",
      "- Val Loss : 0.00010833501437446102 Score : 1.0\n",
      "[493/1000]\n",
      "- Train Loss : 0.00013576208705975054 Score : 1.0\n",
      "- Val Loss : 0.00010763602767838165 Score : 1.0\n",
      "[494/1000]\n",
      "- Train Loss : 0.00013491491376448216 Score : 1.0\n",
      "- Val Loss : 0.0001069343852577731 Score : 1.0\n",
      "[495/1000]\n",
      "- Train Loss : 0.00013404717942143584 Score : 1.0\n",
      "- Val Loss : 0.0001062484152498655 Score : 1.0\n",
      "[496/1000]\n",
      "- Train Loss : 0.00013320072280799245 Score : 1.0\n",
      "- Val Loss : 0.00010555460903560743 Score : 1.0\n",
      "[497/1000]\n",
      "- Train Loss : 0.00013236758604762144 Score : 1.0\n",
      "- Val Loss : 0.00010488009138498455 Score : 1.0\n",
      "[498/1000]\n",
      "- Train Loss : 0.0001315225797750625 Score : 1.0\n",
      "- Val Loss : 0.00010420578473713249 Score : 1.0\n",
      "[499/1000]\n",
      "- Train Loss : 0.00013070417500987079 Score : 1.0\n",
      "- Val Loss : 0.00010352752724429592 Score : 1.0\n",
      "[500/1000]\n",
      "- Train Loss : 0.00012987480537655452 Score : 1.0\n",
      "- Val Loss : 0.00010286417818861082 Score : 1.0\n",
      "[501/1000]\n",
      "- Train Loss : 0.000129052868538161 Score : 1.0\n",
      "- Val Loss : 0.00010220591502729803 Score : 1.0\n",
      "[502/1000]\n",
      "- Train Loss : 0.000128253314768598 Score : 1.0\n",
      "- Val Loss : 0.00010154669871553779 Score : 1.0\n",
      "[503/1000]\n",
      "- Train Loss : 0.0001274384311626717 Score : 1.0\n",
      "- Val Loss : 0.00010089812712976709 Score : 1.0\n",
      "[504/1000]\n",
      "- Train Loss : 0.0001266462734040235 Score : 1.0\n",
      "- Val Loss : 0.00010023963113781065 Score : 1.0\n",
      "[505/1000]\n",
      "- Train Loss : 0.00012584181523480866 Score : 1.0\n",
      "- Val Loss : 9.960291208699346e-05 Score : 1.0\n",
      "[506/1000]\n",
      "- Train Loss : 0.00012505414653989847 Score : 1.0\n",
      "- Val Loss : 9.897448762785643e-05 Score : 1.0\n",
      "[507/1000]\n",
      "- Train Loss : 0.00012428368710162531 Score : 1.0\n",
      "- Val Loss : 9.832852811086923e-05 Score : 1.0\n",
      "[508/1000]\n",
      "- Train Loss : 0.00012349405551503878 Score : 1.0\n",
      "- Val Loss : 9.770124597707763e-05 Score : 1.0\n",
      "[509/1000]\n",
      "- Train Loss : 0.00012273307462843755 Score : 1.0\n",
      "- Val Loss : 9.706899436423555e-05 Score : 1.0\n",
      "[510/1000]\n",
      "- Train Loss : 0.00012195476503822849 Score : 1.0\n",
      "- Val Loss : 9.645855607232079e-05 Score : 1.0\n",
      "[511/1000]\n",
      "- Train Loss : 0.0001211955227012772 Score : 1.0\n",
      "- Val Loss : 9.584493091097102e-05 Score : 1.0\n",
      "[512/1000]\n",
      "- Train Loss : 0.0001204443366683942 Score : 1.0\n",
      "- Val Loss : 9.523157496005297e-05 Score : 1.0\n",
      "[513/1000]\n",
      "- Train Loss : 0.000119686277280885 Score : 1.0\n",
      "- Val Loss : 9.46243162616156e-05 Score : 1.0\n",
      "[514/1000]\n",
      "- Train Loss : 0.00011895589118972162 Score : 1.0\n",
      "- Val Loss : 9.401218994753435e-05 Score : 1.0\n",
      "[515/1000]\n",
      "- Train Loss : 0.00011820456266933534 Score : 1.0\n",
      "- Val Loss : 9.341260010842234e-05 Score : 1.0\n",
      "[516/1000]\n",
      "- Train Loss : 0.00011746700329240412 Score : 1.0\n",
      "- Val Loss : 9.28311565076001e-05 Score : 1.0\n",
      "[517/1000]\n",
      "- Train Loss : 0.00011675109554845322 Score : 1.0\n",
      "- Val Loss : 9.22335748327896e-05 Score : 1.0\n",
      "[518/1000]\n",
      "- Train Loss : 0.00011601590591049494 Score : 1.0\n",
      "- Val Loss : 9.16458448045887e-05 Score : 1.0\n",
      "[519/1000]\n",
      "- Train Loss : 0.00011530960748334312 Score : 1.0\n",
      "- Val Loss : 9.105361095862463e-05 Score : 1.0\n",
      "[520/1000]\n",
      "- Train Loss : 0.00011457967007623261 Score : 1.0\n",
      "- Val Loss : 9.048554784385487e-05 Score : 1.0\n",
      "[521/1000]\n",
      "- Train Loss : 0.00011387173435650766 Score : 1.0\n",
      "- Val Loss : 8.991880167741328e-05 Score : 1.0\n",
      "[522/1000]\n",
      "- Train Loss : 0.0001131792754070678 Score : 1.0\n",
      "- Val Loss : 8.934079960454255e-05 Score : 1.0\n",
      "[523/1000]\n",
      "- Train Loss : 0.00011246838726543097 Score : 1.0\n",
      "- Val Loss : 8.878358494257554e-05 Score : 1.0\n",
      "[524/1000]\n",
      "- Train Loss : 0.00011177786756080523 Score : 1.0\n",
      "- Val Loss : 8.822237577987835e-05 Score : 1.0\n",
      "[525/1000]\n",
      "- Train Loss : 0.00011109690543283553 Score : 1.0\n",
      "- Val Loss : 8.765592792769894e-05 Score : 1.0\n",
      "[526/1000]\n",
      "- Train Loss : 0.00011040546814911067 Score : 1.0\n",
      "- Val Loss : 8.710497786523774e-05 Score : 1.0\n",
      "[527/1000]\n",
      "- Train Loss : 0.00010973411291262083 Score : 1.0\n",
      "- Val Loss : 8.656700083520263e-05 Score : 1.0\n",
      "[528/1000]\n",
      "- Train Loss : 0.00010904630127899711 Score : 1.0\n",
      "- Val Loss : 8.601375157013535e-05 Score : 1.0\n",
      "[529/1000]\n",
      "- Train Loss : 0.00010837209871776092 Score : 1.0\n",
      "- Val Loss : 8.547449397156015e-05 Score : 1.0\n",
      "[530/1000]\n",
      "- Train Loss : 0.0001077178743192538 Score : 1.0\n",
      "- Val Loss : 8.493749919580296e-05 Score : 1.0\n",
      "[531/1000]\n",
      "- Train Loss : 0.00010705174322033094 Score : 1.0\n",
      "- Val Loss : 8.439661905867979e-05 Score : 1.0\n",
      "[532/1000]\n",
      "- Train Loss : 0.00010640917207638267 Score : 1.0\n",
      "- Val Loss : 8.386489207623526e-05 Score : 1.0\n",
      "[533/1000]\n",
      "- Train Loss : 0.00010574085253109742 Score : 1.0\n",
      "- Val Loss : 8.334255107911304e-05 Score : 1.0\n",
      "[534/1000]\n",
      "- Train Loss : 0.00010508939218804396 Score : 1.0\n",
      "- Val Loss : 8.282151975436136e-05 Score : 1.0\n",
      "[535/1000]\n",
      "- Train Loss : 0.00010446147118475185 Score : 1.0\n",
      "- Val Loss : 8.229885861510411e-05 Score : 1.0\n",
      "[536/1000]\n",
      "- Train Loss : 0.00010380985663181895 Score : 1.0\n",
      "- Val Loss : 8.178239659173414e-05 Score : 1.0\n",
      "[537/1000]\n",
      "- Train Loss : 0.00010317762543208018 Score : 1.0\n",
      "- Val Loss : 8.128349873004481e-05 Score : 1.0\n",
      "[538/1000]\n",
      "- Train Loss : 0.0001025565222031825 Score : 1.0\n",
      "- Val Loss : 8.076585800154135e-05 Score : 1.0\n",
      "[539/1000]\n",
      "- Train Loss : 0.00010192211043936873 Score : 1.0\n",
      "- Val Loss : 8.025204442674294e-05 Score : 1.0\n",
      "[540/1000]\n",
      "- Train Loss : 0.00010131149873713084 Score : 1.0\n",
      "- Val Loss : 7.974763138918206e-05 Score : 1.0\n",
      "[541/1000]\n",
      "- Train Loss : 0.00010068146669558094 Score : 1.0\n",
      "- Val Loss : 7.925135287223384e-05 Score : 1.0\n",
      "[542/1000]\n",
      "- Train Loss : 0.00010006645991072421 Score : 1.0\n",
      "- Val Loss : 7.876949530327693e-05 Score : 1.0\n",
      "[543/1000]\n",
      "- Train Loss : 9.946652608050499e-05 Score : 1.0\n",
      "- Val Loss : 7.826963701518252e-05 Score : 1.0\n",
      "[544/1000]\n",
      "- Train Loss : 9.885290779089296e-05 Score : 1.0\n",
      "- Val Loss : 7.778329745633528e-05 Score : 1.0\n",
      "[545/1000]\n",
      "- Train Loss : 9.825096579637223e-05 Score : 1.0\n",
      "- Val Loss : 7.730092329438776e-05 Score : 1.0\n",
      "[546/1000]\n",
      "- Train Loss : 9.76700357568916e-05 Score : 1.0\n",
      "- Val Loss : 7.681694842176512e-05 Score : 1.0\n",
      "[547/1000]\n",
      "- Train Loss : 9.706510273746162e-05 Score : 1.0\n",
      "- Val Loss : 7.634974463144317e-05 Score : 1.0\n",
      "[548/1000]\n",
      "- Train Loss : 9.649119419918861e-05 Score : 1.0\n",
      "- Val Loss : 7.585873390780762e-05 Score : 1.0\n",
      "[549/1000]\n",
      "- Train Loss : 9.589354097746157e-05 Score : 1.0\n",
      "- Val Loss : 7.539186481153592e-05 Score : 1.0\n",
      "[550/1000]\n",
      "- Train Loss : 9.530853539116733e-05 Score : 1.0\n",
      "- Val Loss : 7.493216980947182e-05 Score : 1.0\n",
      "[551/1000]\n",
      "- Train Loss : 9.474719263380393e-05 Score : 1.0\n",
      "- Val Loss : 7.446988456649706e-05 Score : 1.0\n",
      "[552/1000]\n",
      "- Train Loss : 9.416596549272072e-05 Score : 1.0\n",
      "- Val Loss : 7.400742470053956e-05 Score : 1.0\n",
      "[553/1000]\n",
      "- Train Loss : 9.359183119765173e-05 Score : 1.0\n",
      "- Val Loss : 7.354909757850692e-05 Score : 1.0\n",
      "[554/1000]\n",
      "- Train Loss : 9.304304270093174e-05 Score : 1.0\n",
      "- Val Loss : 7.308176282094792e-05 Score : 1.0\n",
      "[555/1000]\n",
      "- Train Loss : 9.247128562290325e-05 Score : 1.0\n",
      "- Val Loss : 7.263089355546981e-05 Score : 1.0\n",
      "[556/1000]\n",
      "- Train Loss : 9.191245610256576e-05 Score : 1.0\n",
      "- Val Loss : 7.217664824565873e-05 Score : 1.0\n",
      "[557/1000]\n",
      "- Train Loss : 9.137489461055439e-05 Score : 1.0\n",
      "- Val Loss : 7.173549238359556e-05 Score : 1.0\n",
      "[558/1000]\n",
      "- Train Loss : 9.080877680389676e-05 Score : 1.0\n",
      "- Val Loss : 7.129347795853391e-05 Score : 1.0\n",
      "[559/1000]\n",
      "- Train Loss : 9.026272386512978e-05 Score : 1.0\n",
      "- Val Loss : 7.086521509336308e-05 Score : 1.0\n",
      "[560/1000]\n",
      "- Train Loss : 8.973267136348618e-05 Score : 1.0\n",
      "- Val Loss : 7.042451034067199e-05 Score : 1.0\n",
      "[561/1000]\n",
      "- Train Loss : 8.91859575252359e-05 Score : 1.0\n",
      "- Val Loss : 6.998699245741591e-05 Score : 1.0\n",
      "[562/1000]\n",
      "- Train Loss : 8.865178420415355e-05 Score : 1.0\n",
      "- Val Loss : 6.956336437724531e-05 Score : 1.0\n",
      "[563/1000]\n",
      "- Train Loss : 8.813010208541527e-05 Score : 1.0\n",
      "- Val Loss : 6.913218385307118e-05 Score : 1.0\n",
      "[564/1000]\n",
      "- Train Loss : 8.75885537728512e-05 Score : 1.0\n",
      "- Val Loss : 6.870761717436835e-05 Score : 1.0\n",
      "[565/1000]\n",
      "- Train Loss : 8.706489744428027e-05 Score : 1.0\n",
      "- Val Loss : 6.827987817814574e-05 Score : 1.0\n",
      "[566/1000]\n",
      "- Train Loss : 8.65510810399428e-05 Score : 1.0\n",
      "- Val Loss : 6.786379526602104e-05 Score : 1.0\n",
      "[567/1000]\n",
      "- Train Loss : 8.60266290045628e-05 Score : 1.0\n",
      "- Val Loss : 6.74562543281354e-05 Score : 1.0\n",
      "[568/1000]\n",
      "- Train Loss : 8.550729115894582e-05 Score : 1.0\n",
      "- Val Loss : 6.7032829974778e-05 Score : 1.0\n",
      "[569/1000]\n",
      "- Train Loss : 8.501056072418578e-05 Score : 1.0\n",
      "- Val Loss : 6.662343366770074e-05 Score : 1.0\n",
      "[570/1000]\n",
      "- Train Loss : 8.449534349589764e-05 Score : 1.0\n",
      "- Val Loss : 6.622436922043562e-05 Score : 1.0\n",
      "[571/1000]\n",
      "- Train Loss : 8.398624474163323e-05 Score : 1.0\n",
      "- Val Loss : 6.581372872460634e-05 Score : 1.0\n",
      "[572/1000]\n",
      "- Train Loss : 8.349552586474197e-05 Score : 1.0\n",
      "- Val Loss : 6.541643961099908e-05 Score : 1.0\n",
      "[573/1000]\n",
      "- Train Loss : 8.299028922920115e-05 Score : 1.0\n",
      "- Val Loss : 6.501722964458168e-05 Score : 1.0\n",
      "[574/1000]\n",
      "- Train Loss : 8.24907092464855e-05 Score : 1.0\n",
      "- Val Loss : 6.46245971438475e-05 Score : 1.0\n",
      "[575/1000]\n",
      "- Train Loss : 8.201304101223488e-05 Score : 1.0\n",
      "- Val Loss : 6.423337617889047e-05 Score : 1.0\n",
      "[576/1000]\n",
      "- Train Loss : 8.152046797881162e-05 Score : 1.0\n",
      "- Val Loss : 6.385110464179888e-05 Score : 1.0\n",
      "[577/1000]\n",
      "- Train Loss : 8.103375128282803e-05 Score : 1.0\n",
      "- Val Loss : 6.34689858998172e-05 Score : 1.0\n",
      "[578/1000]\n",
      "- Train Loss : 8.05659597568188e-05 Score : 1.0\n",
      "- Val Loss : 6.307919829851016e-05 Score : 1.0\n",
      "[579/1000]\n",
      "- Train Loss : 8.008339864318259e-05 Score : 1.0\n",
      "- Val Loss : 6.27021145191975e-05 Score : 1.0\n",
      "[580/1000]\n",
      "- Train Loss : 7.960691052883501e-05 Score : 1.0\n",
      "- Val Loss : 6.23254309175536e-05 Score : 1.0\n",
      "[581/1000]\n",
      "- Train Loss : 7.914441529363912e-05 Score : 1.0\n",
      "- Val Loss : 6.195000605657697e-05 Score : 1.0\n",
      "[582/1000]\n",
      "- Train Loss : 7.867199565225746e-05 Score : 1.0\n",
      "- Val Loss : 6.15742610534653e-05 Score : 1.0\n",
      "[583/1000]\n",
      "- Train Loss : 7.820222365100765e-05 Score : 1.0\n",
      "- Val Loss : 6.120766192907467e-05 Score : 1.0\n",
      "[584/1000]\n",
      "- Train Loss : 7.775202256097045e-05 Score : 1.0\n",
      "- Val Loss : 6.083402331569232e-05 Score : 1.0\n",
      "[585/1000]\n",
      "- Train Loss : 7.729004472720489e-05 Score : 1.0\n",
      "- Val Loss : 6.047287752153352e-05 Score : 1.0\n",
      "[586/1000]\n",
      "- Train Loss : 7.683133824482663e-05 Score : 1.0\n",
      "- Val Loss : 6.010397191857919e-05 Score : 1.0\n",
      "[587/1000]\n",
      "- Train Loss : 7.638586864939296e-05 Score : 1.0\n",
      "- Val Loss : 5.9745634644059464e-05 Score : 1.0\n",
      "[588/1000]\n",
      "- Train Loss : 7.593295817463918e-05 Score : 1.0\n",
      "- Val Loss : 5.939099355600774e-05 Score : 1.0\n",
      "[589/1000]\n",
      "- Train Loss : 7.547770332570912e-05 Score : 1.0\n",
      "- Val Loss : 5.9028618125012144e-05 Score : 1.0\n",
      "[590/1000]\n",
      "- Train Loss : 7.50445767860381e-05 Score : 1.0\n",
      "- Val Loss : 5.8672885643318295e-05 Score : 1.0\n",
      "[591/1000]\n",
      "- Train Loss : 7.459877714508265e-05 Score : 1.0\n",
      "- Val Loss : 5.8320543757872656e-05 Score : 1.0\n",
      "[592/1000]\n",
      "- Train Loss : 7.415910507309793e-05 Score : 1.0\n",
      "- Val Loss : 5.7969340559793636e-05 Score : 1.0\n",
      "[593/1000]\n",
      "- Train Loss : 7.372933456887647e-05 Score : 1.0\n",
      "- Val Loss : 5.761655847891234e-05 Score : 1.0\n",
      "[594/1000]\n",
      "- Train Loss : 7.329498435461169e-05 Score : 1.0\n",
      "- Val Loss : 5.727971438318491e-05 Score : 1.0\n",
      "[595/1000]\n",
      "- Train Loss : 7.286256156172992e-05 Score : 1.0\n",
      "- Val Loss : 5.6935583415906876e-05 Score : 1.0\n",
      "[596/1000]\n",
      "- Train Loss : 7.244222191123602e-05 Score : 1.0\n",
      "- Val Loss : 5.657735528075136e-05 Score : 1.0\n",
      "[597/1000]\n",
      "- Train Loss : 7.201473444082594e-05 Score : 1.0\n",
      "- Val Loss : 5.625173434964381e-05 Score : 1.0\n",
      "[598/1000]\n",
      "- Train Loss : 7.158693248735896e-05 Score : 1.0\n",
      "- Val Loss : 5.591035369434394e-05 Score : 1.0\n",
      "[599/1000]\n",
      "- Train Loss : 7.117730112642878e-05 Score : 1.0\n",
      "- Val Loss : 5.55765291210264e-05 Score : 1.0\n",
      "[600/1000]\n",
      "- Train Loss : 7.075734427214936e-05 Score : 1.0\n",
      "- Val Loss : 5.5236723710549995e-05 Score : 1.0\n",
      "[601/1000]\n",
      "- Train Loss : 7.033827508469888e-05 Score : 1.0\n",
      "- Val Loss : 5.491539559443481e-05 Score : 1.0\n",
      "[602/1000]\n",
      "- Train Loss : 6.99394709550284e-05 Score : 1.0\n",
      "- Val Loss : 5.4571788496105e-05 Score : 1.0\n",
      "[603/1000]\n",
      "- Train Loss : 6.952510001509736e-05 Score : 1.0\n",
      "- Val Loss : 5.425631024991162e-05 Score : 1.0\n",
      "[604/1000]\n",
      "- Train Loss : 6.911411795348006e-05 Score : 1.0\n",
      "- Val Loss : 5.393791798269376e-05 Score : 1.0\n",
      "[605/1000]\n",
      "- Train Loss : 6.87248772616537e-05 Score : 1.0\n",
      "- Val Loss : 5.360181239666417e-05 Score : 1.0\n",
      "[606/1000]\n",
      "- Train Loss : 6.831355211842392e-05 Score : 1.0\n",
      "- Val Loss : 5.328506085788831e-05 Score : 1.0\n",
      "[607/1000]\n",
      "- Train Loss : 6.791400280942778e-05 Score : 1.0\n",
      "- Val Loss : 5.2967250667279586e-05 Score : 1.0\n",
      "[608/1000]\n",
      "- Train Loss : 6.751958396004436e-05 Score : 1.0\n",
      "- Val Loss : 5.2654650062322617e-05 Score : 1.0\n",
      "[609/1000]\n",
      "- Train Loss : 6.713641879388079e-05 Score : 1.0\n",
      "- Val Loss : 5.2339873946039006e-05 Score : 1.0\n",
      "[610/1000]\n",
      "- Train Loss : 6.67323403023248e-05 Score : 1.0\n",
      "- Val Loss : 5.20243302162271e-05 Score : 1.0\n",
      "[611/1000]\n",
      "- Train Loss : 6.635592479950801e-05 Score : 1.0\n",
      "- Val Loss : 5.170864824322052e-05 Score : 1.0\n",
      "[612/1000]\n",
      "- Train Loss : 6.596387659859222e-05 Score : 1.0\n",
      "- Val Loss : 5.1400809752522036e-05 Score : 1.0\n",
      "[613/1000]\n",
      "- Train Loss : 6.557813058962993e-05 Score : 1.0\n",
      "- Val Loss : 5.1096980314468965e-05 Score : 1.0\n",
      "[614/1000]\n",
      "- Train Loss : 6.519939132785011e-05 Score : 1.0\n",
      "- Val Loss : 5.0794686103472486e-05 Score : 1.0\n",
      "[615/1000]\n",
      "- Train Loss : 6.482938240676756e-05 Score : 1.0\n",
      "- Val Loss : 5.049488026998006e-05 Score : 1.0\n",
      "[616/1000]\n",
      "- Train Loss : 6.444516778477312e-05 Score : 1.0\n",
      "- Val Loss : 5.0199832912767306e-05 Score : 1.0\n",
      "[617/1000]\n",
      "- Train Loss : 6.407396479820211e-05 Score : 1.0\n",
      "- Val Loss : 4.9889298679772764e-05 Score : 1.0\n",
      "[618/1000]\n",
      "- Train Loss : 6.370652671547659e-05 Score : 1.0\n",
      "- Val Loss : 4.9595837481319904e-05 Score : 1.0\n",
      "[619/1000]\n",
      "- Train Loss : 6.333036334480211e-05 Score : 1.0\n",
      "- Val Loss : 4.9299174861516804e-05 Score : 1.0\n",
      "[620/1000]\n",
      "- Train Loss : 6.296485268345957e-05 Score : 1.0\n",
      "- Val Loss : 4.9012767703970894e-05 Score : 1.0\n",
      "[621/1000]\n",
      "- Train Loss : 6.2605398751556e-05 Score : 1.0\n",
      "- Val Loss : 4.8709684051573277e-05 Score : 1.0\n",
      "[622/1000]\n",
      "- Train Loss : 6.22420352050944e-05 Score : 1.0\n",
      "- Val Loss : 4.84287811559625e-05 Score : 1.0\n",
      "[623/1000]\n",
      "- Train Loss : 6.187793587741908e-05 Score : 1.0\n",
      "- Val Loss : 4.813688065041788e-05 Score : 1.0\n",
      "[624/1000]\n",
      "- Train Loss : 6.153263044931616e-05 Score : 1.0\n",
      "- Val Loss : 4.784986231243238e-05 Score : 1.0\n",
      "[625/1000]\n",
      "- Train Loss : 6.117087317155286e-05 Score : 1.0\n",
      "- Val Loss : 4.756771886604838e-05 Score : 1.0\n",
      "[626/1000]\n",
      "- Train Loss : 6.0814854199027955e-05 Score : 1.0\n",
      "- Val Loss : 4.728831481770612e-05 Score : 1.0\n",
      "[627/1000]\n",
      "- Train Loss : 6.04668145266866e-05 Score : 1.0\n",
      "- Val Loss : 4.701062789536081e-05 Score : 1.0\n",
      "[628/1000]\n",
      "- Train Loss : 6.0124714865297494e-05 Score : 1.0\n",
      "- Val Loss : 4.673202056437731e-05 Score : 1.0\n",
      "[629/1000]\n",
      "- Train Loss : 5.9767442103798705e-05 Score : 1.0\n",
      "- Val Loss : 4.645290755433962e-05 Score : 1.0\n",
      "[630/1000]\n",
      "- Train Loss : 5.942745034796341e-05 Score : 1.0\n",
      "- Val Loss : 4.6179640776244923e-05 Score : 1.0\n",
      "[631/1000]\n",
      "- Train Loss : 5.909106221224647e-05 Score : 1.0\n",
      "- Val Loss : 4.59030234196689e-05 Score : 1.0\n",
      "[632/1000]\n",
      "- Train Loss : 5.8743587740334784e-05 Score : 1.0\n",
      "- Val Loss : 4.563491165754385e-05 Score : 1.0\n",
      "[633/1000]\n",
      "- Train Loss : 5.840514717824085e-05 Score : 1.0\n",
      "- Val Loss : 4.5360186049947515e-05 Score : 1.0\n",
      "[634/1000]\n",
      "- Train Loss : 5.8079279976179045e-05 Score : 1.0\n",
      "- Val Loss : 4.509478822001256e-05 Score : 1.0\n",
      "[635/1000]\n",
      "- Train Loss : 5.773639936200602e-05 Score : 1.0\n",
      "- Val Loss : 4.482715667109005e-05 Score : 1.0\n",
      "[636/1000]\n",
      "- Train Loss : 5.74060601744956e-05 Score : 1.0\n",
      "- Val Loss : 4.4556607463164255e-05 Score : 1.0\n",
      "[637/1000]\n",
      "- Train Loss : 5.708858589059673e-05 Score : 1.0\n",
      "- Val Loss : 4.429593900567852e-05 Score : 1.0\n",
      "[638/1000]\n",
      "- Train Loss : 5.675423542723163e-05 Score : 1.0\n",
      "- Val Loss : 4.403919228934683e-05 Score : 1.0\n",
      "[639/1000]\n",
      "- Train Loss : 5.642879750666907e-05 Score : 1.0\n",
      "- Val Loss : 4.3776533857453614e-05 Score : 1.0\n",
      "[640/1000]\n",
      "- Train Loss : 5.6103931254054056e-05 Score : 1.0\n",
      "- Val Loss : 4.351625102572143e-05 Score : 1.0\n",
      "[641/1000]\n",
      "- Train Loss : 5.578729714114969e-05 Score : 1.0\n",
      "- Val Loss : 4.326449197833426e-05 Score : 1.0\n",
      "[642/1000]\n",
      "- Train Loss : 5.546380574135886e-05 Score : 1.0\n",
      "- Val Loss : 4.299870488466695e-05 Score : 1.0\n",
      "[643/1000]\n",
      "- Train Loss : 5.514709230030551e-05 Score : 1.0\n",
      "- Val Loss : 4.275194805813953e-05 Score : 1.0\n",
      "[644/1000]\n",
      "- Train Loss : 5.483863383738531e-05 Score : 1.0\n",
      "- Val Loss : 4.2490020859986544e-05 Score : 1.0\n",
      "[645/1000]\n",
      "- Train Loss : 5.452148545575458e-05 Score : 1.0\n",
      "- Val Loss : 4.2244228097843006e-05 Score : 1.0\n",
      "[646/1000]\n",
      "- Train Loss : 5.420872447656519e-05 Score : 1.0\n",
      "- Val Loss : 4.1996427171397954e-05 Score : 1.0\n",
      "[647/1000]\n",
      "- Train Loss : 5.390515899408557e-05 Score : 1.0\n",
      "- Val Loss : 4.1742128814803436e-05 Score : 1.0\n",
      "[648/1000]\n",
      "- Train Loss : 5.3594950865469095e-05 Score : 1.0\n",
      "- Val Loss : 4.149527376284823e-05 Score : 1.0\n",
      "[649/1000]\n",
      "- Train Loss : 5.327966452265779e-05 Score : 1.0\n",
      "- Val Loss : 4.125906343688257e-05 Score : 1.0\n",
      "[650/1000]\n",
      "- Train Loss : 5.2985476031608414e-05 Score : 1.0\n",
      "- Val Loss : 4.1007813706528395e-05 Score : 1.0\n",
      "[651/1000]\n",
      "- Train Loss : 5.268322497108279e-05 Score : 1.0\n",
      "- Val Loss : 4.0772370994091034e-05 Score : 1.0\n",
      "[652/1000]\n",
      "- Train Loss : 5.237843985620808e-05 Score : 1.0\n",
      "- Val Loss : 4.053470183862373e-05 Score : 1.0\n",
      "[653/1000]\n",
      "- Train Loss : 5.207830266347931e-05 Score : 1.0\n",
      "- Val Loss : 4.029961201013066e-05 Score : 1.0\n",
      "[654/1000]\n",
      "- Train Loss : 5.179020879748148e-05 Score : 1.0\n",
      "- Val Loss : 4.005236041848548e-05 Score : 1.0\n",
      "[655/1000]\n",
      "- Train Loss : 5.149215384234493e-05 Score : 1.0\n",
      "- Val Loss : 3.982840644312091e-05 Score : 1.0\n",
      "[656/1000]\n",
      "- Train Loss : 5.119854409309078e-05 Score : 1.0\n",
      "- Val Loss : 3.958513116231188e-05 Score : 1.0\n",
      "[657/1000]\n",
      "- Train Loss : 5.091154313655958e-05 Score : 1.0\n",
      "- Val Loss : 3.93591180909425e-05 Score : 1.0\n",
      "[658/1000]\n",
      "- Train Loss : 5.0622489551541446e-05 Score : 1.0\n",
      "- Val Loss : 3.912051397492178e-05 Score : 1.0\n",
      "[659/1000]\n",
      "- Train Loss : 5.0330270318631344e-05 Score : 1.0\n",
      "- Val Loss : 3.890118023264222e-05 Score : 1.0\n",
      "[660/1000]\n",
      "- Train Loss : 5.005028348629518e-05 Score : 1.0\n",
      "- Val Loss : 3.866308179567568e-05 Score : 1.0\n",
      "[661/1000]\n",
      "- Train Loss : 4.9766274666277845e-05 Score : 1.0\n",
      "- Val Loss : 3.844418461085297e-05 Score : 1.0\n",
      "[662/1000]\n",
      "- Train Loss : 4.9476937419967726e-05 Score : 1.0\n",
      "- Val Loss : 3.821045174845494e-05 Score : 1.0\n",
      "[663/1000]\n",
      "- Train Loss : 4.920782228307669e-05 Score : 1.0\n",
      "- Val Loss : 3.7987938412697986e-05 Score : 1.0\n",
      "[664/1000]\n",
      "- Train Loss : 4.891950791514116e-05 Score : 1.0\n",
      "- Val Loss : 3.777141682803631e-05 Score : 1.0\n",
      "[665/1000]\n",
      "- Train Loss : 4.864152631651248e-05 Score : 1.0\n",
      "- Val Loss : 3.7548084947047755e-05 Score : 1.0\n",
      "[666/1000]\n",
      "- Train Loss : 4.8363836362518166e-05 Score : 1.0\n",
      "- Val Loss : 3.7336045352276415e-05 Score : 1.0\n",
      "[667/1000]\n",
      "- Train Loss : 4.8102061151035334e-05 Score : 1.0\n",
      "- Val Loss : 3.711685349117033e-05 Score : 1.0\n",
      "[668/1000]\n",
      "- Train Loss : 4.7827875379071986e-05 Score : 1.0\n",
      "- Val Loss : 3.6902969441143796e-05 Score : 1.0\n",
      "[669/1000]\n",
      "- Train Loss : 4.755073859996628e-05 Score : 1.0\n",
      "- Val Loss : 3.667413329822011e-05 Score : 1.0\n",
      "[670/1000]\n",
      "- Train Loss : 4.7295171195097886e-05 Score : 1.0\n",
      "- Val Loss : 3.646959157777019e-05 Score : 1.0\n",
      "[671/1000]\n",
      "- Train Loss : 4.701784524109421e-05 Score : 1.0\n",
      "- Val Loss : 3.6257806641515344e-05 Score : 1.0\n",
      "[672/1000]\n",
      "- Train Loss : 4.6747052995973114e-05 Score : 1.0\n",
      "- Val Loss : 3.604823359637521e-05 Score : 1.0\n",
      "[673/1000]\n",
      "- Train Loss : 4.6494036622688225e-05 Score : 1.0\n",
      "- Val Loss : 3.583332727430388e-05 Score : 1.0\n",
      "[674/1000]\n",
      "- Train Loss : 4.622659950352196e-05 Score : 1.0\n",
      "- Val Loss : 3.562382698873989e-05 Score : 1.0\n",
      "[675/1000]\n",
      "- Train Loss : 4.596157476852467e-05 Score : 1.0\n",
      "- Val Loss : 3.5420816857367754e-05 Score : 1.0\n",
      "[676/1000]\n",
      "- Train Loss : 4.571645109131673e-05 Score : 1.0\n",
      "- Val Loss : 3.520839163684286e-05 Score : 1.0\n",
      "[677/1000]\n",
      "- Train Loss : 4.544841178560293e-05 Score : 1.0\n",
      "- Val Loss : 3.500119782984257e-05 Score : 1.0\n",
      "[678/1000]\n",
      "- Train Loss : 4.519190972334602e-05 Score : 1.0\n",
      "- Val Loss : 3.480801387922838e-05 Score : 1.0\n",
      "[679/1000]\n",
      "- Train Loss : 4.493602743120088e-05 Score : 1.0\n",
      "- Val Loss : 3.460937296040356e-05 Score : 1.0\n",
      "[680/1000]\n",
      "- Train Loss : 4.4694608429078166e-05 Score : 1.0\n",
      "- Val Loss : 3.4404307371005416e-05 Score : 1.0\n",
      "[681/1000]\n",
      "- Train Loss : 4.443244142748881e-05 Score : 1.0\n",
      "- Val Loss : 3.419942004256882e-05 Score : 1.0\n",
      "[682/1000]\n",
      "- Train Loss : 4.4183755512171454e-05 Score : 1.0\n",
      "- Val Loss : 3.4000917366938666e-05 Score : 1.0\n",
      "[683/1000]\n",
      "- Train Loss : 4.3939086127567054e-05 Score : 1.0\n",
      "- Val Loss : 3.380159250809811e-05 Score : 1.0\n",
      "[684/1000]\n",
      "- Train Loss : 4.369260861696805e-05 Score : 1.0\n",
      "- Val Loss : 3.361130438861437e-05 Score : 1.0\n",
      "[685/1000]\n",
      "- Train Loss : 4.344069925031767e-05 Score : 1.0\n",
      "- Val Loss : 3.3419135434087366e-05 Score : 1.0\n",
      "[686/1000]\n",
      "- Train Loss : 4.320434759898085e-05 Score : 1.0\n",
      "- Val Loss : 3.3226369851036e-05 Score : 1.0\n",
      "[687/1000]\n",
      "- Train Loss : 4.295433558986083e-05 Score : 1.0\n",
      "- Val Loss : 3.302534605609253e-05 Score : 1.0\n",
      "[688/1000]\n",
      "- Train Loss : 4.271462350718341e-05 Score : 1.0\n",
      "- Val Loss : 3.284210470155813e-05 Score : 1.0\n",
      "[689/1000]\n",
      "- Train Loss : 4.247707794194058e-05 Score : 1.0\n",
      "- Val Loss : 3.265795385232195e-05 Score : 1.0\n",
      "[690/1000]\n",
      "- Train Loss : 4.2246502541982205e-05 Score : 1.0\n",
      "- Val Loss : 3.2467774872202426e-05 Score : 1.0\n",
      "[691/1000]\n",
      "- Train Loss : 4.200187969319005e-05 Score : 1.0\n",
      "- Val Loss : 3.22771848004777e-05 Score : 1.0\n",
      "[692/1000]\n",
      "- Train Loss : 4.1768916162254754e-05 Score : 1.0\n",
      "- Val Loss : 3.209310307283886e-05 Score : 1.0\n",
      "[693/1000]\n",
      "- Train Loss : 4.153702007493444e-05 Score : 1.0\n",
      "- Val Loss : 3.1895928259473294e-05 Score : 1.0\n",
      "[694/1000]\n",
      "- Train Loss : 4.129842505386073e-05 Score : 1.0\n",
      "- Val Loss : 3.172432116116397e-05 Score : 1.0\n",
      "[695/1000]\n",
      "- Train Loss : 4.106030958913228e-05 Score : 1.0\n",
      "- Val Loss : 3.154588193865493e-05 Score : 1.0\n",
      "[696/1000]\n",
      "- Train Loss : 4.084404817048279e-05 Score : 1.0\n",
      "- Val Loss : 3.136800660286099e-05 Score : 1.0\n",
      "[697/1000]\n",
      "- Train Loss : 4.061019864012552e-05 Score : 1.0\n",
      "- Val Loss : 3.1193896575132385e-05 Score : 1.0\n",
      "[698/1000]\n",
      "- Train Loss : 4.0380932558845314e-05 Score : 1.0\n",
      "- Val Loss : 3.1017269066069275e-05 Score : 1.0\n",
      "[699/1000]\n",
      "- Train Loss : 4.016632697635537e-05 Score : 1.0\n",
      "- Val Loss : 3.083975752815604e-05 Score : 1.0\n",
      "[700/1000]\n",
      "- Train Loss : 3.9936866515846406e-05 Score : 1.0\n",
      "- Val Loss : 3.067792204092257e-05 Score : 1.0\n",
      "[701/1000]\n",
      "- Train Loss : 3.971333373758373e-05 Score : 1.0\n",
      "- Val Loss : 3.0509268981404603e-05 Score : 1.0\n",
      "[702/1000]\n",
      "- Train Loss : 3.949656279979131e-05 Score : 1.0\n",
      "- Val Loss : 3.0327035346999764e-05 Score : 1.0\n",
      "[703/1000]\n",
      "- Train Loss : 3.9274244399065436e-05 Score : 1.0\n",
      "- Val Loss : 3.0160425012581982e-05 Score : 1.0\n",
      "[704/1000]\n",
      "- Train Loss : 3.9055186536845416e-05 Score : 1.0\n",
      "- Val Loss : 2.9985198125359602e-05 Score : 1.0\n",
      "[705/1000]\n",
      "- Train Loss : 3.884214781161669e-05 Score : 1.0\n",
      "- Val Loss : 2.982597470690962e-05 Score : 1.0\n",
      "[706/1000]\n",
      "- Train Loss : 3.863107081593221e-05 Score : 1.0\n",
      "- Val Loss : 2.9660912332474254e-05 Score : 1.0\n",
      "[707/1000]\n",
      "- Train Loss : 3.841111321081472e-05 Score : 1.0\n",
      "- Val Loss : 2.9483193429769017e-05 Score : 1.0\n",
      "[708/1000]\n",
      "- Train Loss : 3.820305912288152e-05 Score : 1.0\n",
      "- Val Loss : 2.9316552172531374e-05 Score : 1.0\n",
      "[709/1000]\n",
      "- Train Loss : 3.799077795621189e-05 Score : 1.0\n",
      "- Val Loss : 2.915814911830239e-05 Score : 1.0\n",
      "[710/1000]\n",
      "- Train Loss : 3.777342372915074e-05 Score : 1.0\n",
      "- Val Loss : 2.899168430303689e-05 Score : 1.0\n",
      "[711/1000]\n",
      "- Train Loss : 3.7567171451276714e-05 Score : 1.0\n",
      "- Val Loss : 2.8833163014496677e-05 Score : 1.0\n",
      "[712/1000]\n",
      "- Train Loss : 3.7364099929012205e-05 Score : 1.0\n",
      "- Val Loss : 2.8669308449025266e-05 Score : 1.0\n",
      "[713/1000]\n",
      "- Train Loss : 3.715380570510105e-05 Score : 1.0\n",
      "- Val Loss : 2.8499262043624185e-05 Score : 1.0\n",
      "[714/1000]\n",
      "- Train Loss : 3.694697574650895e-05 Score : 1.0\n",
      "- Val Loss : 2.834407496266067e-05 Score : 1.0\n",
      "[715/1000]\n",
      "- Train Loss : 3.6748855917620756e-05 Score : 1.0\n",
      "- Val Loss : 2.8189042495796457e-05 Score : 1.0\n",
      "[716/1000]\n",
      "- Train Loss : 3.6539177876370057e-05 Score : 1.0\n",
      "- Val Loss : 2.802814924507402e-05 Score : 1.0\n",
      "[717/1000]\n",
      "- Train Loss : 3.633922632515249e-05 Score : 1.0\n",
      "- Val Loss : 2.786265940812882e-05 Score : 1.0\n",
      "[718/1000]\n",
      "- Train Loss : 3.613639657689621e-05 Score : 1.0\n",
      "- Val Loss : 2.770489481918048e-05 Score : 1.0\n",
      "[719/1000]\n",
      "- Train Loss : 3.5939325041706776e-05 Score : 1.0\n",
      "- Val Loss : 2.7554840926313773e-05 Score : 1.0\n",
      "[720/1000]\n",
      "- Train Loss : 3.5740416125109834e-05 Score : 1.0\n",
      "- Val Loss : 2.73940840997966e-05 Score : 1.0\n",
      "[721/1000]\n",
      "- Train Loss : 3.5537563285793615e-05 Score : 1.0\n",
      "- Val Loss : 2.7239973860559985e-05 Score : 1.0\n",
      "[722/1000]\n",
      "- Train Loss : 3.535173012399658e-05 Score : 1.0\n",
      "- Val Loss : 2.7086482077720575e-05 Score : 1.0\n",
      "[723/1000]\n",
      "- Train Loss : 3.5155268657642104e-05 Score : 1.0\n",
      "- Val Loss : 2.693556234589778e-05 Score : 1.0\n",
      "[724/1000]\n",
      "- Train Loss : 3.4962096985206394e-05 Score : 1.0\n",
      "- Val Loss : 2.6772429919219576e-05 Score : 1.0\n",
      "[725/1000]\n",
      "- Train Loss : 3.4772560714676125e-05 Score : 1.0\n",
      "- Val Loss : 2.662229599081911e-05 Score : 1.0\n",
      "[726/1000]\n",
      "- Train Loss : 3.457661791293908e-05 Score : 1.0\n",
      "- Val Loss : 2.647013207024429e-05 Score : 1.0\n",
      "[727/1000]\n",
      "- Train Loss : 3.438744629925673e-05 Score : 1.0\n",
      "- Val Loss : 2.632657196954824e-05 Score : 1.0\n",
      "[728/1000]\n",
      "- Train Loss : 3.419260924096711e-05 Score : 1.0\n",
      "- Val Loss : 2.6174597223871388e-05 Score : 1.0\n",
      "[729/1000]\n",
      "- Train Loss : 3.4015716209978564e-05 Score : 1.0\n",
      "- Val Loss : 2.603120810817927e-05 Score : 1.0\n",
      "[730/1000]\n",
      "- Train Loss : 3.3824343720172896e-05 Score : 1.0\n",
      "- Val Loss : 2.5877001462504268e-05 Score : 1.0\n",
      "[731/1000]\n",
      "- Train Loss : 3.3634230779474215e-05 Score : 1.0\n",
      "- Val Loss : 2.57312258327147e-05 Score : 1.0\n",
      "[732/1000]\n",
      "- Train Loss : 3.3453513778012064e-05 Score : 1.0\n",
      "- Val Loss : 2.5577131964382716e-05 Score : 1.0\n",
      "[733/1000]\n",
      "- Train Loss : 3.327503276927423e-05 Score : 1.0\n",
      "- Val Loss : 2.543606206018012e-05 Score : 1.0\n",
      "[734/1000]\n",
      "- Train Loss : 3.308479254984478e-05 Score : 1.0\n",
      "- Val Loss : 2.5292449208791368e-05 Score : 1.0\n",
      "[735/1000]\n",
      "- Train Loss : 3.290745654440899e-05 Score : 1.0\n",
      "- Val Loss : 2.514992047508713e-05 Score : 1.0\n",
      "[736/1000]\n",
      "- Train Loss : 3.272863295933348e-05 Score : 1.0\n",
      "- Val Loss : 2.5006975192809477e-05 Score : 1.0\n",
      "[737/1000]\n",
      "- Train Loss : 3.254738996676881e-05 Score : 1.0\n",
      "- Val Loss : 2.4850023692124523e-05 Score : 1.0\n",
      "[738/1000]\n",
      "- Train Loss : 3.237079130889874e-05 Score : 1.0\n",
      "- Val Loss : 2.4723089154576883e-05 Score : 1.0\n",
      "[739/1000]\n",
      "- Train Loss : 3.21888044001955e-05 Score : 1.0\n",
      "- Val Loss : 2.458420021866914e-05 Score : 1.0\n",
      "[740/1000]\n",
      "- Train Loss : 3.201675983493462e-05 Score : 1.0\n",
      "- Val Loss : 2.4445680537610315e-05 Score : 1.0\n",
      "[741/1000]\n",
      "- Train Loss : 3.184035687000788e-05 Score : 1.0\n",
      "- Val Loss : 2.4298822609125637e-05 Score : 1.0\n",
      "[742/1000]\n",
      "- Train Loss : 3.166377599781198e-05 Score : 1.0\n",
      "- Val Loss : 2.4160177417797968e-05 Score : 1.0\n",
      "[743/1000]\n",
      "- Train Loss : 3.149709679443428e-05 Score : 1.0\n",
      "- Val Loss : 2.4019012926146388e-05 Score : 1.0\n",
      "[744/1000]\n",
      "- Train Loss : 3.132185687112118e-05 Score : 1.0\n",
      "- Val Loss : 2.389124892943073e-05 Score : 1.0\n",
      "[745/1000]\n",
      "- Train Loss : 3.114942233474317e-05 Score : 1.0\n",
      "- Val Loss : 2.3758720999467187e-05 Score : 1.0\n",
      "[746/1000]\n",
      "- Train Loss : 3.097681908709799e-05 Score : 1.0\n",
      "- Val Loss : 2.3619626517756842e-05 Score : 1.0\n",
      "[747/1000]\n",
      "- Train Loss : 3.0813824903614455e-05 Score : 1.0\n",
      "- Val Loss : 2.3490021703764796e-05 Score : 1.0\n",
      "[748/1000]\n",
      "- Train Loss : 3.0640227072985406e-05 Score : 1.0\n",
      "- Val Loss : 2.3351864001597278e-05 Score : 1.0\n",
      "[749/1000]\n",
      "- Train Loss : 3.0471619589257494e-05 Score : 1.0\n",
      "- Val Loss : 2.321448118891567e-05 Score : 1.0\n",
      "[750/1000]\n",
      "- Train Loss : 3.0306909846128998e-05 Score : 1.0\n",
      "- Val Loss : 2.307767681486439e-05 Score : 1.0\n",
      "[751/1000]\n",
      "- Train Loss : 3.0144547761463196e-05 Score : 1.0\n",
      "- Val Loss : 2.2953377992962487e-05 Score : 1.0\n",
      "[752/1000]\n",
      "- Train Loss : 2.9974274386606543e-05 Score : 1.0\n",
      "- Val Loss : 2.2829572117188945e-05 Score : 1.0\n",
      "[753/1000]\n",
      "- Train Loss : 2.981669068604889e-05 Score : 1.0\n",
      "- Val Loss : 2.2696995074511506e-05 Score : 1.0\n",
      "[754/1000]\n",
      "- Train Loss : 2.96604574840684e-05 Score : 1.0\n",
      "- Val Loss : 2.257058622490149e-05 Score : 1.0\n",
      "[755/1000]\n",
      "- Train Loss : 2.9494292498889587e-05 Score : 1.0\n",
      "- Val Loss : 2.2439908207161352e-05 Score : 1.0\n",
      "[756/1000]\n",
      "- Train Loss : 2.9329097590865178e-05 Score : 1.0\n",
      "- Val Loss : 2.231274265795946e-05 Score : 1.0\n",
      "[757/1000]\n",
      "- Train Loss : 2.9168765649956185e-05 Score : 1.0\n",
      "- Val Loss : 2.21832324314164e-05 Score : 1.0\n",
      "[758/1000]\n",
      "- Train Loss : 2.9015527110863735e-05 Score : 1.0\n",
      "- Val Loss : 2.2063542928663082e-05 Score : 1.0\n",
      "[759/1000]\n",
      "- Train Loss : 2.8855777499201293e-05 Score : 1.0\n",
      "- Val Loss : 2.1934913092991337e-05 Score : 1.0\n",
      "[760/1000]\n",
      "- Train Loss : 2.869796132169237e-05 Score : 1.0\n",
      "- Val Loss : 2.1813011699123308e-05 Score : 1.0\n",
      "[761/1000]\n",
      "- Train Loss : 2.8547959573188564e-05 Score : 1.0\n",
      "- Val Loss : 2.1688878405257128e-05 Score : 1.0\n",
      "[762/1000]\n",
      "- Train Loss : 2.838980284044131e-05 Score : 1.0\n",
      "- Val Loss : 2.1565621864283457e-05 Score : 1.0\n",
      "[763/1000]\n",
      "- Train Loss : 2.82333620210314e-05 Score : 1.0\n",
      "- Val Loss : 2.1447567633003928e-05 Score : 1.0\n",
      "[764/1000]\n",
      "- Train Loss : 2.8076930700788376e-05 Score : 1.0\n",
      "- Val Loss : 2.133465932274703e-05 Score : 1.0\n",
      "[765/1000]\n",
      "- Train Loss : 2.7929952112673265e-05 Score : 1.0\n",
      "- Val Loss : 2.120926001225598e-05 Score : 1.0\n",
      "[766/1000]\n",
      "- Train Loss : 2.7774646039486266e-05 Score : 1.0\n",
      "- Val Loss : 2.10880498343613e-05 Score : 1.0\n",
      "[767/1000]\n",
      "- Train Loss : 2.762548688729617e-05 Score : 1.0\n",
      "- Val Loss : 2.0965313524357043e-05 Score : 1.0\n",
      "[768/1000]\n",
      "- Train Loss : 2.748049246292794e-05 Score : 1.0\n",
      "- Val Loss : 2.0849054635618813e-05 Score : 1.0\n",
      "[769/1000]\n",
      "- Train Loss : 2.7324778380312233e-05 Score : 1.0\n",
      "- Val Loss : 2.0733397832373157e-05 Score : 1.0\n",
      "[770/1000]\n",
      "- Train Loss : 2.7174437845032015e-05 Score : 1.0\n",
      "- Val Loss : 2.061842315015383e-05 Score : 1.0\n",
      "[771/1000]\n",
      "- Train Loss : 2.7025891591215946e-05 Score : 1.0\n",
      "- Val Loss : 2.050014336418826e-05 Score : 1.0\n",
      "[772/1000]\n",
      "- Train Loss : 2.688895801838953e-05 Score : 1.0\n",
      "- Val Loss : 2.0382434740895405e-05 Score : 1.0\n",
      "[773/1000]\n",
      "- Train Loss : 2.6737841178853865e-05 Score : 1.0\n",
      "- Val Loss : 2.026852234848775e-05 Score : 1.0\n",
      "[774/1000]\n",
      "- Train Loss : 2.659262342300887e-05 Score : 1.0\n",
      "- Val Loss : 2.0161740394541994e-05 Score : 1.0\n",
      "[775/1000]\n",
      "- Train Loss : 2.6457627023369747e-05 Score : 1.0\n",
      "- Val Loss : 2.0040301023982465e-05 Score : 1.0\n",
      "[776/1000]\n",
      "- Train Loss : 2.6305322080588667e-05 Score : 1.0\n",
      "- Val Loss : 1.9936136595788412e-05 Score : 1.0\n",
      "[777/1000]\n",
      "- Train Loss : 2.6164753257439264e-05 Score : 1.0\n",
      "- Val Loss : 1.9822686226689257e-05 Score : 1.0\n",
      "[778/1000]\n",
      "- Train Loss : 2.6024371613352236e-05 Score : 1.0\n",
      "- Val Loss : 1.970594712474849e-05 Score : 1.0\n",
      "[779/1000]\n",
      "- Train Loss : 2.588749662714286e-05 Score : 1.0\n",
      "- Val Loss : 1.9595210687839426e-05 Score : 1.0\n",
      "[780/1000]\n",
      "- Train Loss : 2.573931002795386e-05 Score : 1.0\n",
      "- Val Loss : 1.9488381440169178e-05 Score : 1.0\n",
      "[781/1000]\n",
      "- Train Loss : 2.559837462791216e-05 Score : 1.0\n",
      "- Val Loss : 1.9380220692255534e-05 Score : 1.0\n",
      "[782/1000]\n",
      "- Train Loss : 2.546654524040706e-05 Score : 1.0\n",
      "- Val Loss : 1.926608274516184e-05 Score : 1.0\n",
      "[783/1000]\n",
      "- Train Loss : 2.5327323682075883e-05 Score : 1.0\n",
      "- Val Loss : 1.9164750483469106e-05 Score : 1.0\n",
      "[784/1000]\n",
      "- Train Loss : 2.5186416173205038e-05 Score : 1.0\n",
      "- Val Loss : 1.9058303223573603e-05 Score : 1.0\n",
      "[785/1000]\n",
      "- Train Loss : 2.5050238845223146e-05 Score : 1.0\n",
      "- Val Loss : 1.895279456221033e-05 Score : 1.0\n",
      "[786/1000]\n",
      "- Train Loss : 2.492230110975571e-05 Score : 1.0\n",
      "- Val Loss : 1.884071753011085e-05 Score : 1.0\n",
      "[787/1000]\n",
      "- Train Loss : 2.4785739292888644e-05 Score : 1.0\n",
      "- Val Loss : 1.8740905943559483e-05 Score : 1.0\n",
      "[788/1000]\n",
      "- Train Loss : 2.464971991381996e-05 Score : 1.0\n",
      "- Val Loss : 1.8635560991242528e-05 Score : 1.0\n",
      "[789/1000]\n",
      "- Train Loss : 2.451960856180651e-05 Score : 1.0\n",
      "- Val Loss : 1.8528719010646455e-05 Score : 1.0\n",
      "[790/1000]\n",
      "- Train Loss : 2.4384129447854422e-05 Score : 1.0\n",
      "- Val Loss : 1.84172185981879e-05 Score : 1.0\n",
      "[791/1000]\n",
      "- Train Loss : 2.4249272807234472e-05 Score : 1.0\n",
      "- Val Loss : 1.8321703464607708e-05 Score : 1.0\n",
      "[792/1000]\n",
      "- Train Loss : 2.411901388970566e-05 Score : 1.0\n",
      "- Val Loss : 1.8223028746433556e-05 Score : 1.0\n",
      "[793/1000]\n",
      "- Train Loss : 2.3995834302089254e-05 Score : 1.0\n",
      "- Val Loss : 1.812221307773143e-05 Score : 1.0\n",
      "[794/1000]\n",
      "- Train Loss : 2.3865435979233654e-05 Score : 1.0\n",
      "- Val Loss : 1.8024880773737095e-05 Score : 1.0\n",
      "[795/1000]\n",
      "- Train Loss : 2.373200762425161e-05 Score : 1.0\n",
      "- Val Loss : 1.7922004190040752e-05 Score : 1.0\n",
      "[796/1000]\n",
      "- Train Loss : 2.3607210752945524e-05 Score : 1.0\n",
      "- Val Loss : 1.7804975868784823e-05 Score : 1.0\n",
      "[797/1000]\n",
      "- Train Loss : 2.3479068254851478e-05 Score : 1.0\n",
      "- Val Loss : 1.7717118680593558e-05 Score : 1.0\n",
      "[798/1000]\n",
      "- Train Loss : 2.3348708787630636e-05 Score : 1.0\n",
      "- Val Loss : 1.761966268531978e-05 Score : 1.0\n",
      "[799/1000]\n",
      "- Train Loss : 2.3225554741657106e-05 Score : 1.0\n",
      "- Val Loss : 1.751489980961196e-05 Score : 1.0\n",
      "[800/1000]\n",
      "- Train Loss : 2.3108308040617783e-05 Score : 1.0\n",
      "- Val Loss : 1.7420938092982396e-05 Score : 1.0\n",
      "[801/1000]\n",
      "- Train Loss : 2.2979287096960536e-05 Score : 1.0\n",
      "- Val Loss : 1.7325943190371618e-05 Score : 1.0\n",
      "[802/1000]\n",
      "- Train Loss : 2.285561443689706e-05 Score : 1.0\n",
      "- Val Loss : 1.7225456758751534e-05 Score : 1.0\n",
      "[803/1000]\n",
      "- Train Loss : 2.274134991845737e-05 Score : 1.0\n",
      "- Val Loss : 1.71280735230539e-05 Score : 1.0\n",
      "[804/1000]\n",
      "- Train Loss : 2.2610619251079494e-05 Score : 1.0\n",
      "- Val Loss : 1.704261921986472e-05 Score : 1.0\n",
      "[805/1000]\n",
      "- Train Loss : 2.2485409822871185e-05 Score : 1.0\n",
      "- Val Loss : 1.6947386029642075e-05 Score : 1.0\n",
      "[806/1000]\n",
      "- Train Loss : 2.2363263421235995e-05 Score : 1.0\n",
      "- Val Loss : 1.684903872956056e-05 Score : 1.0\n",
      "[807/1000]\n",
      "- Train Loss : 2.2251047084864695e-05 Score : 1.0\n",
      "- Val Loss : 1.674891791481059e-05 Score : 1.0\n",
      "[808/1000]\n",
      "- Train Loss : 2.2127764042225965e-05 Score : 1.0\n",
      "- Val Loss : 1.6660336768836714e-05 Score : 1.0\n",
      "[809/1000]\n",
      "- Train Loss : 2.2004378378268382e-05 Score : 1.0\n",
      "- Val Loss : 1.65662477229489e-05 Score : 1.0\n",
      "[810/1000]\n",
      "- Train Loss : 2.1895103903564934e-05 Score : 1.0\n",
      "- Val Loss : 1.6471141861984506e-05 Score : 1.0\n",
      "[811/1000]\n",
      "- Train Loss : 2.1771359267101314e-05 Score : 1.0\n",
      "- Val Loss : 1.6383461115765385e-05 Score : 1.0\n",
      "[812/1000]\n",
      "- Train Loss : 2.165689713567392e-05 Score : 1.0\n",
      "- Val Loss : 1.6302561562042683e-05 Score : 1.0\n",
      "[813/1000]\n",
      "- Train Loss : 2.1539022352751796e-05 Score : 1.0\n",
      "- Val Loss : 1.6205374777200632e-05 Score : 1.0\n",
      "[814/1000]\n",
      "- Train Loss : 2.1428160304518922e-05 Score : 1.0\n",
      "- Val Loss : 1.6114905520225875e-05 Score : 1.0\n",
      "[815/1000]\n",
      "- Train Loss : 2.1308745671275472e-05 Score : 1.0\n",
      "- Val Loss : 1.6031455743359402e-05 Score : 1.0\n",
      "[816/1000]\n",
      "- Train Loss : 2.1194351372994584e-05 Score : 1.0\n",
      "- Val Loss : 1.5938030628603883e-05 Score : 1.0\n",
      "[817/1000]\n",
      "- Train Loss : 2.1087531927656124e-05 Score : 1.0\n",
      "- Val Loss : 1.584608071425464e-05 Score : 1.0\n",
      "[818/1000]\n",
      "- Train Loss : 2.0973180148252544e-05 Score : 1.0\n",
      "- Val Loss : 1.575704118295107e-05 Score : 1.0\n",
      "[819/1000]\n",
      "- Train Loss : 2.085722480337103e-05 Score : 1.0\n",
      "- Val Loss : 1.5662113582948223e-05 Score : 1.0\n",
      "[820/1000]\n",
      "- Train Loss : 2.0746142227936718e-05 Score : 1.0\n",
      "- Val Loss : 1.557457107992377e-05 Score : 1.0\n",
      "[821/1000]\n",
      "- Train Loss : 2.0640788812266288e-05 Score : 1.0\n",
      "- Val Loss : 1.5486430129385553e-05 Score : 1.0\n",
      "[822/1000]\n",
      "- Train Loss : 2.052518071751466e-05 Score : 1.0\n",
      "- Val Loss : 1.5405455997097306e-05 Score : 1.0\n",
      "[823/1000]\n",
      "- Train Loss : 2.041599739439132e-05 Score : 1.0\n",
      "- Val Loss : 1.533154136268422e-05 Score : 1.0\n",
      "[824/1000]\n",
      "- Train Loss : 2.0307765478517264e-05 Score : 1.0\n",
      "- Val Loss : 1.5246398106683046e-05 Score : 1.0\n",
      "[825/1000]\n",
      "- Train Loss : 2.020269817472177e-05 Score : 1.0\n",
      "- Val Loss : 1.5160071598074865e-05 Score : 1.0\n",
      "[826/1000]\n",
      "- Train Loss : 2.008680225016077e-05 Score : 1.0\n",
      "- Val Loss : 1.5071826965140644e-05 Score : 1.0\n",
      "[827/1000]\n",
      "- Train Loss : 1.9982868833620967e-05 Score : 1.0\n",
      "- Val Loss : 1.4991058378654998e-05 Score : 1.0\n",
      "[828/1000]\n",
      "- Train Loss : 1.988015133570621e-05 Score : 1.0\n",
      "- Val Loss : 1.4901256690791342e-05 Score : 1.0\n",
      "[829/1000]\n",
      "- Train Loss : 1.976845060703959e-05 Score : 1.0\n",
      "- Val Loss : 1.4818302588537335e-05 Score : 1.0\n",
      "[830/1000]\n",
      "- Train Loss : 1.9664262759356967e-05 Score : 1.0\n",
      "- Val Loss : 1.4742277926416136e-05 Score : 1.0\n",
      "[831/1000]\n",
      "- Train Loss : 1.9560929609320334e-05 Score : 1.0\n",
      "- Val Loss : 1.4663665751868393e-05 Score : 1.0\n",
      "[832/1000]\n",
      "- Train Loss : 1.9460263476705426e-05 Score : 1.0\n",
      "- Val Loss : 1.4583974007109646e-05 Score : 1.0\n",
      "[833/1000]\n",
      "- Train Loss : 1.9352712949209188e-05 Score : 1.0\n",
      "- Val Loss : 1.450220042897854e-05 Score : 1.0\n",
      "[834/1000]\n",
      "- Train Loss : 1.9244423280421568e-05 Score : 1.0\n",
      "- Val Loss : 1.4419273611565586e-05 Score : 1.0\n",
      "[835/1000]\n",
      "- Train Loss : 1.914527236597981e-05 Score : 1.0\n",
      "- Val Loss : 1.4335926607600413e-05 Score : 1.0\n",
      "[836/1000]\n",
      "- Train Loss : 1.9045722587583845e-05 Score : 1.0\n",
      "- Val Loss : 1.4259194358601235e-05 Score : 1.0\n",
      "[837/1000]\n",
      "- Train Loss : 1.8941614598588785e-05 Score : 1.0\n",
      "- Val Loss : 1.4180923244566657e-05 Score : 1.0\n",
      "[838/1000]\n",
      "- Train Loss : 1.883899656402516e-05 Score : 1.0\n",
      "- Val Loss : 1.410426375514362e-05 Score : 1.0\n",
      "[839/1000]\n",
      "- Train Loss : 1.8740390916920864e-05 Score : 1.0\n",
      "- Val Loss : 1.4026598364580423e-05 Score : 1.0\n",
      "[840/1000]\n",
      "- Train Loss : 1.8639599248773367e-05 Score : 1.0\n",
      "- Val Loss : 1.394664741383167e-05 Score : 1.0\n",
      "[841/1000]\n",
      "- Train Loss : 1.853784861951782e-05 Score : 1.0\n",
      "- Val Loss : 1.386992971674772e-05 Score : 1.0\n",
      "[842/1000]\n",
      "- Train Loss : 1.8443404466476448e-05 Score : 1.0\n",
      "- Val Loss : 1.3797131941828411e-05 Score : 1.0\n",
      "[843/1000]\n",
      "- Train Loss : 1.8345943721619227e-05 Score : 1.0\n",
      "- Val Loss : 1.3718032278120518e-05 Score : 1.0\n",
      "[844/1000]\n",
      "- Train Loss : 1.824444444133001e-05 Score : 1.0\n",
      "- Val Loss : 1.3641494660987519e-05 Score : 1.0\n",
      "[845/1000]\n",
      "- Train Loss : 1.814565065514115e-05 Score : 1.0\n",
      "- Val Loss : 1.3575127923104446e-05 Score : 1.0\n",
      "[846/1000]\n",
      "- Train Loss : 1.8052947578123873e-05 Score : 1.0\n",
      "- Val Loss : 1.3490935998561326e-05 Score : 1.0\n",
      "[847/1000]\n",
      "- Train Loss : 1.795415161925323e-05 Score : 1.0\n",
      "- Val Loss : 1.3425499673758168e-05 Score : 1.0\n",
      "[848/1000]\n",
      "- Train Loss : 1.786149919603809e-05 Score : 1.0\n",
      "- Val Loss : 1.3346304513106588e-05 Score : 1.0\n",
      "[849/1000]\n",
      "- Train Loss : 1.7766989306740772e-05 Score : 1.0\n",
      "- Val Loss : 1.3266963833302725e-05 Score : 1.0\n",
      "[850/1000]\n",
      "- Train Loss : 1.7671536599866362e-05 Score : 1.0\n",
      "- Val Loss : 1.3193848644732498e-05 Score : 1.0\n",
      "[851/1000]\n",
      "- Train Loss : 1.7575776029035398e-05 Score : 1.0\n",
      "- Val Loss : 1.3127420061209705e-05 Score : 1.0\n",
      "[852/1000]\n",
      "- Train Loss : 1.7483309041684453e-05 Score : 1.0\n",
      "- Val Loss : 1.305001296714181e-05 Score : 1.0\n",
      "[853/1000]\n",
      "- Train Loss : 1.739513413667737e-05 Score : 1.0\n",
      "- Val Loss : 1.2980522114958148e-05 Score : 1.0\n",
      "[854/1000]\n",
      "- Train Loss : 1.729630290962329e-05 Score : 1.0\n",
      "- Val Loss : 1.2912583770230412e-05 Score : 1.0\n",
      "[855/1000]\n",
      "- Train Loss : 1.7205464599909545e-05 Score : 1.0\n",
      "- Val Loss : 1.2838444490625989e-05 Score : 1.0\n",
      "[856/1000]\n",
      "- Train Loss : 1.7112503100078255e-05 Score : 1.0\n",
      "- Val Loss : 1.2770343346346635e-05 Score : 1.0\n",
      "[857/1000]\n",
      "- Train Loss : 1.702982098726756e-05 Score : 1.0\n",
      "- Val Loss : 1.2697423699137289e-05 Score : 1.0\n",
      "[858/1000]\n",
      "- Train Loss : 1.693386490033946e-05 Score : 1.0\n",
      "- Val Loss : 1.2626004718185868e-05 Score : 1.0\n",
      "[859/1000]\n",
      "- Train Loss : 1.6843182594230813e-05 Score : 1.0\n",
      "- Val Loss : 1.2557889021991286e-05 Score : 1.0\n",
      "[860/1000]\n",
      "- Train Loss : 1.6757374977613734e-05 Score : 1.0\n",
      "- Val Loss : 1.2489768778323196e-05 Score : 1.0\n",
      "[861/1000]\n",
      "- Train Loss : 1.666334686787094e-05 Score : 1.0\n",
      "- Val Loss : 1.242328198713949e-05 Score : 1.0\n",
      "[862/1000]\n",
      "- Train Loss : 1.6580681782822165e-05 Score : 1.0\n",
      "- Val Loss : 1.2359203537926078e-05 Score : 1.0\n",
      "[863/1000]\n",
      "- Train Loss : 1.6487362422089467e-05 Score : 1.0\n",
      "- Val Loss : 1.2284252989047673e-05 Score : 1.0\n",
      "[864/1000]\n",
      "- Train Loss : 1.6404478199345045e-05 Score : 1.0\n",
      "- Val Loss : 1.2217370567668695e-05 Score : 1.0\n",
      "[865/1000]\n",
      "- Train Loss : 1.63169295824951e-05 Score : 1.0\n",
      "- Val Loss : 1.2151803275628481e-05 Score : 1.0\n",
      "[866/1000]\n",
      "- Train Loss : 1.6231202430895388e-05 Score : 1.0\n",
      "- Val Loss : 1.2089580195606686e-05 Score : 1.0\n",
      "[867/1000]\n",
      "- Train Loss : 1.614581292920371e-05 Score : 1.0\n",
      "- Val Loss : 1.2019006135233212e-05 Score : 1.0\n",
      "[868/1000]\n",
      "- Train Loss : 1.605423972250719e-05 Score : 1.0\n",
      "- Val Loss : 1.1949829058721662e-05 Score : 1.0\n",
      "[869/1000]\n",
      "- Train Loss : 1.5967903815787093e-05 Score : 1.0\n",
      "- Val Loss : 1.1891411304532085e-05 Score : 1.0\n",
      "[870/1000]\n",
      "- Train Loss : 1.5888630084898774e-05 Score : 1.0\n",
      "- Val Loss : 1.182648975373013e-05 Score : 1.0\n",
      "[871/1000]\n",
      "- Train Loss : 1.5803377588478423e-05 Score : 1.0\n",
      "- Val Loss : 1.1765527233364992e-05 Score : 1.0\n",
      "[872/1000]\n",
      "- Train Loss : 1.5718832628408563e-05 Score : 1.0\n",
      "- Val Loss : 1.1705693395924754e-05 Score : 1.0\n",
      "[873/1000]\n",
      "- Train Loss : 1.5635317671088462e-05 Score : 1.0\n",
      "- Val Loss : 1.1639546755759511e-05 Score : 1.0\n",
      "[874/1000]\n",
      "- Train Loss : 1.5554219418037166e-05 Score : 1.0\n",
      "- Val Loss : 1.1562608960957732e-05 Score : 1.0\n",
      "[875/1000]\n",
      "- Train Loss : 1.547509838120378e-05 Score : 1.0\n",
      "- Val Loss : 1.1493975762277842e-05 Score : 1.0\n",
      "[876/1000]\n",
      "- Train Loss : 1.538876585982507e-05 Score : 1.0\n",
      "- Val Loss : 1.143488861998776e-05 Score : 1.0\n",
      "[877/1000]\n",
      "- Train Loss : 1.5305337784512732e-05 Score : 1.0\n",
      "- Val Loss : 1.1370552783773746e-05 Score : 1.0\n",
      "[878/1000]\n",
      "- Train Loss : 1.5231554875248952e-05 Score : 1.0\n",
      "- Val Loss : 1.1315152733004652e-05 Score : 1.0\n",
      "[879/1000]\n",
      "- Train Loss : 1.5146404671718807e-05 Score : 1.0\n",
      "- Val Loss : 1.1248233022342902e-05 Score : 1.0\n",
      "[880/1000]\n",
      "- Train Loss : 1.5067677812415544e-05 Score : 1.0\n",
      "- Val Loss : 1.1196228115295526e-05 Score : 1.0\n",
      "[881/1000]\n",
      "- Train Loss : 1.4988006657606295e-05 Score : 1.0\n",
      "- Val Loss : 1.1133547559438739e-05 Score : 1.0\n",
      "[882/1000]\n",
      "- Train Loss : 1.4910456052853584e-05 Score : 1.0\n",
      "- Val Loss : 1.1070583241234999e-05 Score : 1.0\n",
      "[883/1000]\n",
      "- Train Loss : 1.4824158925977019e-05 Score : 1.0\n",
      "- Val Loss : 1.101662746805232e-05 Score : 1.0\n",
      "[884/1000]\n",
      "- Train Loss : 1.4746293484475043e-05 Score : 1.0\n",
      "- Val Loss : 1.0956054211419541e-05 Score : 1.0\n",
      "[885/1000]\n",
      "- Train Loss : 1.4670618535698546e-05 Score : 1.0\n",
      "- Val Loss : 1.0893348189711105e-05 Score : 1.0\n",
      "[886/1000]\n",
      "- Train Loss : 1.4593123195380838e-05 Score : 1.0\n",
      "- Val Loss : 1.082640574168181e-05 Score : 1.0\n",
      "[887/1000]\n",
      "- Train Loss : 1.4520462236254793e-05 Score : 1.0\n",
      "- Val Loss : 1.0764500075310934e-05 Score : 1.0\n",
      "[888/1000]\n",
      "- Train Loss : 1.4442477903584302e-05 Score : 1.0\n",
      "- Val Loss : 1.0714099516917486e-05 Score : 1.0\n",
      "[889/1000]\n",
      "- Train Loss : 1.4368663111478478e-05 Score : 1.0\n",
      "- Val Loss : 1.0647125236573629e-05 Score : 1.0\n",
      "[890/1000]\n",
      "- Train Loss : 1.4297365813339033e-05 Score : 1.0\n",
      "- Val Loss : 1.0589682460704353e-05 Score : 1.0\n",
      "[891/1000]\n",
      "- Train Loss : 1.4215071726337354e-05 Score : 1.0\n",
      "- Val Loss : 1.0534376997384243e-05 Score : 1.0\n",
      "[892/1000]\n",
      "- Train Loss : 1.4137788651977542e-05 Score : 1.0\n",
      "- Val Loss : 1.047277601173846e-05 Score : 1.0\n",
      "[893/1000]\n",
      "- Train Loss : 1.4068109256489455e-05 Score : 1.0\n",
      "- Val Loss : 1.0415632459626067e-05 Score : 1.0\n",
      "[894/1000]\n",
      "- Train Loss : 1.3995391605526998e-05 Score : 1.0\n",
      "- Val Loss : 1.0367730283178389e-05 Score : 1.0\n",
      "[895/1000]\n",
      "- Train Loss : 1.3921381006791004e-05 Score : 1.0\n",
      "- Val Loss : 1.0304845091013703e-05 Score : 1.0\n",
      "[896/1000]\n",
      "- Train Loss : 1.3845054834544297e-05 Score : 1.0\n",
      "- Val Loss : 1.0244252734992187e-05 Score : 1.0\n",
      "[897/1000]\n",
      "- Train Loss : 1.3771769823102255e-05 Score : 1.0\n",
      "- Val Loss : 1.0192427907895762e-05 Score : 1.0\n",
      "[898/1000]\n",
      "- Train Loss : 1.3698767866622398e-05 Score : 1.0\n",
      "- Val Loss : 1.0124153959623072e-05 Score : 1.0\n",
      "[899/1000]\n",
      "- Train Loss : 1.3625662922499436e-05 Score : 1.0\n",
      "- Val Loss : 1.007910213957075e-05 Score : 1.0\n",
      "[900/1000]\n",
      "- Train Loss : 1.3551491242146261e-05 Score : 1.0\n",
      "- Val Loss : 1.0019265573646408e-05 Score : 1.0\n",
      "[901/1000]\n",
      "- Train Loss : 1.34809684545366e-05 Score : 1.0\n",
      "- Val Loss : 9.96822473098291e-06 Score : 1.0\n",
      "[902/1000]\n",
      "- Train Loss : 1.3415387861136373e-05 Score : 1.0\n",
      "- Val Loss : 9.917757779476233e-06 Score : 1.0\n",
      "[903/1000]\n",
      "- Train Loss : 1.3344409985115311e-05 Score : 1.0\n",
      "- Val Loss : 9.864978892437648e-06 Score : 1.0\n",
      "[904/1000]\n",
      "- Train Loss : 1.3271688986707079e-05 Score : 1.0\n",
      "- Val Loss : 9.810268238652498e-06 Score : 1.0\n",
      "[905/1000]\n",
      "- Train Loss : 1.3202083967674602e-05 Score : 1.0\n",
      "- Val Loss : 9.760175089468248e-06 Score : 1.0\n",
      "[906/1000]\n",
      "- Train Loss : 1.3130806286830597e-05 Score : 1.0\n",
      "- Val Loss : 9.701953786134254e-06 Score : 1.0\n",
      "[907/1000]\n",
      "- Train Loss : 1.3061931844681061e-05 Score : 1.0\n",
      "- Val Loss : 9.64988339546835e-06 Score : 1.0\n",
      "[908/1000]\n",
      "- Train Loss : 1.2995896756567365e-05 Score : 1.0\n",
      "- Val Loss : 9.595896699465811e-06 Score : 1.0\n",
      "[909/1000]\n",
      "- Train Loss : 1.2929723791583254e-05 Score : 1.0\n",
      "- Val Loss : 9.546698493068106e-06 Score : 1.0\n",
      "[910/1000]\n",
      "- Train Loss : 1.2859325427901544e-05 Score : 1.0\n",
      "- Val Loss : 9.489297553955112e-06 Score : 1.0\n",
      "[911/1000]\n",
      "- Train Loss : 1.2789926990485077e-05 Score : 1.0\n",
      "- Val Loss : 9.439637324248906e-06 Score : 1.0\n",
      "[912/1000]\n",
      "- Train Loss : 1.2724601017705734e-05 Score : 1.0\n",
      "- Val Loss : 9.378128197568003e-06 Score : 1.0\n",
      "[913/1000]\n",
      "- Train Loss : 1.265803431730698e-05 Score : 1.0\n",
      "- Val Loss : 9.338347808807157e-06 Score : 1.0\n",
      "[914/1000]\n",
      "- Train Loss : 1.2593238226246387e-05 Score : 1.0\n",
      "- Val Loss : 9.283472536480986e-06 Score : 1.0\n",
      "[915/1000]\n",
      "- Train Loss : 1.252337983714824e-05 Score : 1.0\n",
      "- Val Loss : 9.237833182851318e-06 Score : 1.0\n",
      "[916/1000]\n",
      "- Train Loss : 1.2458066470976013e-05 Score : 1.0\n",
      "- Val Loss : 9.185641829390079e-06 Score : 1.0\n",
      "[917/1000]\n",
      "- Train Loss : 1.239897032216201e-05 Score : 1.0\n",
      "- Val Loss : 9.138097993854899e-06 Score : 1.0\n",
      "[918/1000]\n",
      "- Train Loss : 1.2328873557028905e-05 Score : 1.0\n",
      "- Val Loss : 9.08635320229223e-06 Score : 1.0\n",
      "[919/1000]\n",
      "- Train Loss : 1.226486546733617e-05 Score : 1.0\n",
      "- Val Loss : 9.03810632735258e-06 Score : 1.0\n",
      "[920/1000]\n",
      "- Train Loss : 1.2197059731988702e-05 Score : 1.0\n",
      "- Val Loss : 8.982343388197478e-06 Score : 1.0\n",
      "[921/1000]\n",
      "- Train Loss : 1.2136591774631395e-05 Score : 1.0\n",
      "- Val Loss : 8.935382538766135e-06 Score : 1.0\n",
      "[922/1000]\n",
      "- Train Loss : 1.2070447079774263e-05 Score : 1.0\n",
      "- Val Loss : 8.889674973033834e-06 Score : 1.0\n",
      "[923/1000]\n",
      "- Train Loss : 1.200920266657906e-05 Score : 1.0\n",
      "- Val Loss : 8.85040299181128e-06 Score : 1.0\n",
      "[924/1000]\n",
      "- Train Loss : 1.1947781672461133e-05 Score : 1.0\n",
      "- Val Loss : 8.79472372616874e-06 Score : 1.0\n",
      "[925/1000]\n",
      "- Train Loss : 1.1884767521526859e-05 Score : 1.0\n",
      "- Val Loss : 8.739081749808975e-06 Score : 1.0\n",
      "[926/1000]\n",
      "- Train Loss : 1.1819904115226462e-05 Score : 1.0\n",
      "- Val Loss : 8.697960765857715e-06 Score : 1.0\n",
      "[927/1000]\n",
      "- Train Loss : 1.1759091282530992e-05 Score : 1.0\n",
      "- Val Loss : 8.655148121761158e-06 Score : 1.0\n",
      "[928/1000]\n",
      "- Train Loss : 1.1697768930692595e-05 Score : 1.0\n",
      "- Val Loss : 8.604647518950514e-06 Score : 1.0\n",
      "[929/1000]\n",
      "- Train Loss : 1.1637037358822352e-05 Score : 1.0\n",
      "- Val Loss : 8.558928129787091e-06 Score : 1.0\n",
      "[930/1000]\n",
      "- Train Loss : 1.1576160520437244e-05 Score : 1.0\n",
      "- Val Loss : 8.51894492370775e-06 Score : 1.0\n",
      "[931/1000]\n",
      "- Train Loss : 1.151243759522913e-05 Score : 1.0\n",
      "- Val Loss : 8.459968739771284e-06 Score : 1.0\n",
      "[932/1000]\n",
      "- Train Loss : 1.1455284607109206e-05 Score : 1.0\n",
      "- Val Loss : 8.41453129396541e-06 Score : 1.0\n",
      "[933/1000]\n",
      "- Train Loss : 1.1397923155224513e-05 Score : 1.0\n",
      "- Val Loss : 8.3732811617665e-06 Score : 1.0\n",
      "[934/1000]\n",
      "- Train Loss : 1.1335989332413495e-05 Score : 1.0\n",
      "- Val Loss : 8.325208000314888e-06 Score : 1.0\n",
      "[935/1000]\n",
      "- Train Loss : 1.1276729259811367e-05 Score : 1.0\n",
      "- Val Loss : 8.27970688987989e-06 Score : 1.0\n",
      "[936/1000]\n",
      "- Train Loss : 1.1218313602512353e-05 Score : 1.0\n",
      "- Val Loss : 8.226582394854631e-06 Score : 1.0\n",
      "[937/1000]\n",
      "- Train Loss : 1.115646467850537e-05 Score : 1.0\n",
      "- Val Loss : 8.19028264231747e-06 Score : 1.0\n",
      "[938/1000]\n",
      "- Train Loss : 1.1097855248307395e-05 Score : 1.0\n",
      "- Val Loss : 8.151321708282921e-06 Score : 1.0\n",
      "[939/1000]\n",
      "- Train Loss : 1.104317868794169e-05 Score : 1.0\n",
      "- Val Loss : 8.102233550744131e-06 Score : 1.0\n",
      "[940/1000]\n",
      "- Train Loss : 1.0984812534944569e-05 Score : 1.0\n",
      "- Val Loss : 8.054114914557431e-06 Score : 1.0\n",
      "[941/1000]\n",
      "- Train Loss : 1.0927668123864956e-05 Score : 1.0\n",
      "- Val Loss : 8.014258128241636e-06 Score : 1.0\n",
      "[942/1000]\n",
      "- Train Loss : 1.0868433605537575e-05 Score : 1.0\n",
      "- Val Loss : 7.975987500685733e-06 Score : 1.0\n",
      "[943/1000]\n",
      "- Train Loss : 1.0810463815384233e-05 Score : 1.0\n",
      "- Val Loss : 7.923293196654413e-06 Score : 1.0\n",
      "[944/1000]\n",
      "- Train Loss : 1.0756734519014168e-05 Score : 1.0\n",
      "- Val Loss : 7.884380465839058e-06 Score : 1.0\n",
      "[945/1000]\n",
      "- Train Loss : 1.0702910849936921e-05 Score : 1.0\n",
      "- Val Loss : 7.84938038123073e-06 Score : 1.0\n",
      "[946/1000]\n",
      "- Train Loss : 1.0642289920623523e-05 Score : 1.0\n",
      "- Val Loss : 7.809522685420234e-06 Score : 1.0\n",
      "[947/1000]\n",
      "- Train Loss : 1.058609277353955e-05 Score : 1.0\n",
      "- Val Loss : 7.754010766802821e-06 Score : 1.0\n",
      "[948/1000]\n",
      "- Train Loss : 1.05300760222033e-05 Score : 1.0\n",
      "- Val Loss : 7.723776434431784e-06 Score : 1.0\n",
      "[949/1000]\n",
      "- Train Loss : 1.0478576983688174e-05 Score : 1.0\n",
      "- Val Loss : 7.665372322662733e-06 Score : 1.0\n",
      "[950/1000]\n",
      "- Train Loss : 1.0420445530851592e-05 Score : 1.0\n",
      "- Val Loss : 7.630946129211225e-06 Score : 1.0\n",
      "[951/1000]\n",
      "- Train Loss : 1.0370619520472246e-05 Score : 1.0\n",
      "- Val Loss : 7.5763223321700934e-06 Score : 1.0\n",
      "[952/1000]\n",
      "- Train Loss : 1.0314110884084786e-05 Score : 1.0\n",
      "- Val Loss : 7.546756478404859e-06 Score : 1.0\n",
      "[953/1000]\n",
      "- Train Loss : 1.0261940423737946e-05 Score : 1.0\n",
      "- Val Loss : 7.510220257245237e-06 Score : 1.0\n",
      "[954/1000]\n",
      "- Train Loss : 1.0208354347519667e-05 Score : 1.0\n",
      "- Val Loss : 7.4678619057522155e-06 Score : 1.0\n",
      "[955/1000]\n",
      "- Train Loss : 1.0153177186111861e-05 Score : 1.0\n",
      "- Val Loss : 7.435167844960233e-06 Score : 1.0\n",
      "[956/1000]\n",
      "- Train Loss : 1.0101264946469099e-05 Score : 1.0\n",
      "- Val Loss : 7.38491371521377e-06 Score : 1.0\n",
      "[957/1000]\n",
      "- Train Loss : 1.0044378718197953e-05 Score : 1.0\n",
      "- Val Loss : 7.344568530243123e-06 Score : 1.0\n",
      "[958/1000]\n",
      "- Train Loss : 9.992574746320315e-06 Score : 1.0\n",
      "- Val Loss : 7.306909537874162e-06 Score : 1.0\n",
      "[959/1000]\n",
      "- Train Loss : 9.942494721649888e-06 Score : 1.0\n",
      "- Val Loss : 7.270425157912541e-06 Score : 1.0\n",
      "[960/1000]\n",
      "- Train Loss : 9.892385315247844e-06 Score : 1.0\n",
      "- Val Loss : 7.2290927164431196e-06 Score : 1.0\n",
      "[961/1000]\n",
      "- Train Loss : 9.837253413833321e-06 Score : 1.0\n",
      "- Val Loss : 7.193472811195534e-06 Score : 1.0\n",
      "[962/1000]\n",
      "- Train Loss : 9.787254877361798e-06 Score : 1.0\n",
      "- Val Loss : 7.1606023084314074e-06 Score : 1.0\n",
      "[963/1000]\n",
      "- Train Loss : 9.739058011998875e-06 Score : 1.0\n",
      "- Val Loss : 7.120484042388853e-06 Score : 1.0\n",
      "[964/1000]\n",
      "- Train Loss : 9.686906701568256e-06 Score : 1.0\n",
      "- Val Loss : 7.079683655319968e-06 Score : 1.0\n",
      "[965/1000]\n",
      "- Train Loss : 9.637610358165855e-06 Score : 1.0\n",
      "- Val Loss : 7.036068382149097e-06 Score : 1.0\n",
      "[966/1000]\n",
      "- Train Loss : 9.587473439549083e-06 Score : 1.0\n",
      "- Val Loss : 6.995236617513001e-06 Score : 1.0\n",
      "[967/1000]\n",
      "- Train Loss : 9.539854115953656e-06 Score : 1.0\n",
      "- Val Loss : 6.9599796006514225e-06 Score : 1.0\n",
      "[968/1000]\n",
      "- Train Loss : 9.482977424745008e-06 Score : 1.0\n",
      "- Val Loss : 6.915413450769847e-06 Score : 1.0\n",
      "[969/1000]\n",
      "- Train Loss : 9.434148801624866e-06 Score : 1.0\n",
      "- Val Loss : 6.87872079652152e-06 Score : 1.0\n",
      "[970/1000]\n",
      "- Train Loss : 9.38918689522931e-06 Score : 1.0\n",
      "- Val Loss : 6.839457000751281e-06 Score : 1.0\n",
      "[971/1000]\n",
      "- Train Loss : 9.340555038761522e-06 Score : 1.0\n",
      "- Val Loss : 6.803843461966608e-06 Score : 1.0\n",
      "[972/1000]\n",
      "- Train Loss : 9.288215336608926e-06 Score : 1.0\n",
      "- Val Loss : 6.773922450520331e-06 Score : 1.0\n",
      "[973/1000]\n",
      "- Train Loss : 9.24363370005368e-06 Score : 1.0\n",
      "- Val Loss : 6.7340597524889745e-06 Score : 1.0\n",
      "[974/1000]\n",
      "- Train Loss : 9.191699758856783e-06 Score : 1.0\n",
      "- Val Loss : 6.695546744595049e-06 Score : 1.0\n",
      "[975/1000]\n",
      "- Train Loss : 9.146404194224386e-06 Score : 1.0\n",
      "- Val Loss : 6.660442977590719e-06 Score : 1.0\n",
      "[976/1000]\n",
      "- Train Loss : 9.098530439486138e-06 Score : 1.0\n",
      "- Val Loss : 6.618156930926489e-06 Score : 1.0\n",
      "[977/1000]\n",
      "- Train Loss : 9.048412493939395e-06 Score : 1.0\n",
      "- Val Loss : 6.587234111066209e-06 Score : 1.0\n",
      "[978/1000]\n",
      "- Train Loss : 9.00486504128316e-06 Score : 1.0\n",
      "- Val Loss : 6.553465482284082e-06 Score : 1.0\n",
      "[979/1000]\n",
      "- Train Loss : 8.954752919028882e-06 Score : 1.0\n",
      "- Val Loss : 6.514527740364429e-06 Score : 1.0\n",
      "[980/1000]\n",
      "- Train Loss : 8.905982238300365e-06 Score : 1.0\n",
      "- Val Loss : 6.481154741777573e-06 Score : 1.0\n",
      "[981/1000]\n",
      "- Train Loss : 8.862218464855687e-06 Score : 1.0\n",
      "- Val Loss : 6.4548812588327564e-06 Score : 1.0\n",
      "[982/1000]\n",
      "- Train Loss : 8.819097047307878e-06 Score : 1.0\n",
      "- Val Loss : 6.421586476790253e-06 Score : 1.0\n",
      "[983/1000]\n",
      "- Train Loss : 8.773436295288977e-06 Score : 1.0\n",
      "- Val Loss : 6.383098934747977e-06 Score : 1.0\n",
      "[984/1000]\n",
      "- Train Loss : 8.725116092338996e-06 Score : 1.0\n",
      "- Val Loss : 6.3501620388706215e-06 Score : 1.0\n",
      "[985/1000]\n",
      "- Train Loss : 8.680465334186414e-06 Score : 1.0\n",
      "- Val Loss : 6.31584725852008e-06 Score : 1.0\n",
      "[986/1000]\n",
      "- Train Loss : 8.63651036474443e-06 Score : 1.0\n",
      "- Val Loss : 6.278801265580114e-06 Score : 1.0\n",
      "[987/1000]\n",
      "- Train Loss : 8.588681648941727e-06 Score : 1.0\n",
      "- Val Loss : 6.2407207224168815e-06 Score : 1.0\n",
      "[988/1000]\n",
      "- Train Loss : 8.548720239155551e-06 Score : 1.0\n",
      "- Val Loss : 6.2124672695063055e-06 Score : 1.0\n",
      "[989/1000]\n",
      "- Train Loss : 8.498547382866187e-06 Score : 1.0\n",
      "- Val Loss : 6.1743085097987205e-06 Score : 1.0\n",
      "[990/1000]\n",
      "- Train Loss : 8.453692695790474e-06 Score : 1.0\n",
      "- Val Loss : 6.146194664324867e-06 Score : 1.0\n",
      "[991/1000]\n",
      "- Train Loss : 8.411571419249716e-06 Score : 1.0\n",
      "- Val Loss : 6.1127188928367104e-06 Score : 1.0\n",
      "[992/1000]\n",
      "- Train Loss : 8.366677939698598e-06 Score : 1.0\n",
      "- Val Loss : 6.089012458687648e-06 Score : 1.0\n",
      "[993/1000]\n",
      "- Train Loss : 8.32409567566275e-06 Score : 1.0\n",
      "- Val Loss : 6.051240688975668e-06 Score : 1.0\n",
      "[994/1000]\n",
      "- Train Loss : 8.282302221434495e-06 Score : 1.0\n",
      "- Val Loss : 6.015100552758668e-06 Score : 1.0\n",
      "[995/1000]\n",
      "- Train Loss : 8.243307977156494e-06 Score : 1.0\n",
      "- Val Loss : 5.982102265988942e-06 Score : 1.0\n",
      "[996/1000]\n",
      "- Train Loss : 8.200204332650981e-06 Score : 1.0\n",
      "- Val Loss : 5.950392278464278e-06 Score : 1.0\n",
      "[997/1000]\n",
      "- Train Loss : 8.158124381275735e-06 Score : 1.0\n",
      "- Val Loss : 5.92166588830878e-06 Score : 1.0\n",
      "[998/1000]\n",
      "- Train Loss : 8.112891925470143e-06 Score : 1.0\n",
      "- Val Loss : 5.881833658349933e-06 Score : 1.0\n",
      "[999/1000]\n",
      "- Train Loss : 8.071776171113419e-06 Score : 1.0\n",
      "- Val Loss : 5.853518814546987e-06 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 - 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]], [[],[]]\n",
    "CNT = len(trainDL)\n",
    "print(f'CNT =>{CNT}')\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 학습 모드로 모델 성정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "    for featureTS, targetTS in trainDL :\n",
    "        # 학습 진행\n",
    "        pre_y = model(featureTS)\n",
    "\n",
    "        # 손실계산\n",
    "        loss = reqLoss(pre_y, targetTS)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        score = BinaryF1Score()(pre_y, targetTS)\n",
    "        # 방법2 : score = F1Score(task='binary')(pre_y, targetTS)\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증용 데이터셋 생성\n",
    "        val_feaure_TS = torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_target_TS = torch.FloatTensor(valDS.targetDF.values)\n",
    "        # 평가\n",
    "        pre_val = model(val_feaure_TS)\n",
    "        # 손실 계산\n",
    "        loss_val = reqLoss(pre_val, val_target_TS)\n",
    "        # 성능 평가\n",
    "        score_val = BinaryF1Score()(pre_val, val_target_TS)\n",
    "\n",
    "    # 에포크 당 손실과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 결과 체크 (시각화) => 학습과 검증의 Loss 변화, 성능 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (2, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m TH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(SCORE_HISTORY[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m fg, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m)) \u001b[38;5;66;03m#, sharex=)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTH\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOSS_HISTORY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, TH\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), SCORE_HISTORY )\n\u001b[0;32m      9\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (2, 0)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "TH = len(SCORE_HISTORY[1])\n",
    "\n",
    "fg, axes = plt.subplots(1,2,figsize = (10,5)) #, sharex=)\n",
    "\n",
    "axes[0].plot(range(1, TH+1), LOSS_HISTORY)\n",
    "\n",
    "axes[1].plot(range(1, TH+1), SCORE_HISTORY )\n",
    "axes[1]\n",
    "\n",
    "\n",
    "# plt.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

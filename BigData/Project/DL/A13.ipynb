{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score,BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder  # 라벨 인코더\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# from ProBinModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDF = pd.read_csv(r'C:\\Git\\KDT\\BigData\\Project\\DL\\Second_try\\Data.csv', index_col=0)\n",
    "\n",
    "### 전처리 ------------------------------------------------------------------------------------------------\n",
    "# VIN drop\n",
    "DataDF.drop('VIN', inplace=True, axis=1)\n",
    "\n",
    "# 컬럼 이름 간략하게 + Location 1000 이하 삭제 // Street도 세분화해서 Location 총 9개로 축약\n",
    "DataDF['Location'] = DataDF['Location Description']\n",
    "DataDF.drop('Location Description', inplace=True, axis=1)\n",
    "go_location = ['Commercial_Roads','Residential_Roads','Public_Spaces','PARKINGZONE','HOUSE','Alcohol','STORE','CAR','AIRPORT']\n",
    "VehicleDF = DataDF[DataDF['Location'].isin(go_location)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjy19\\AppData\\Local\\Temp\\ipykernel_1468\\302297085.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VehicleDF['IUCR'] = Labelencoder.fit_transform(VehicleDF['IUCR'])\n",
      "C:\\Users\\kjy19\\AppData\\Local\\Temp\\ipykernel_1468\\302297085.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VehicleDF['Location'] = Labelencoder.fit_transform(VehicleDF['Location'])\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 ---------------------------------------------------------------------------------------------------\n",
    "Labelencoder = LabelEncoder()\n",
    "\n",
    "# IUCR : 4개 라벨\n",
    "# Location : 9개 라벨\n",
    "VehicleDF['IUCR'] = Labelencoder.fit_transform(VehicleDF['IUCR'])\n",
    "VehicleDF['Location'] = Labelencoder.fit_transform(VehicleDF['Location'])\n",
    "\n",
    "# OHtemp = pd.get_dummies(VehicleDF['IUCR'])\n",
    "# VehicleDF.drop(['IUCR'], axis=1, inplace=True)\n",
    "# VehicleDF = pd.concat([VehicleDF, OHtemp], axis=1)\n",
    "# OHtemp = pd.get_dummies(VehicleDF['Location'])\n",
    "# VehicleDF.drop(['Location'], axis=1, inplace=True)\n",
    "# VehicleDF = pd.concat([VehicleDF, OHtemp], axis=1)\n",
    "\n",
    "\n",
    "# print(\"VehicleDF\")\n",
    "# print(VehicleDF)\n",
    "VehicleDF.to_csv('확인용.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjy19\\AppData\\Local\\Temp\\ipykernel_1468\\970895404.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TTtestDF.drop(TrainFalseDF.index, inplace=True)\n",
      "C:\\Users\\kjy19\\AppData\\Local\\Temp\\ipykernel_1468\\970895404.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TTtestDF.drop(TrainTrueDF.index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 트레인 / 테스트 Arrest 비율 맞추기 용----------------------------------------------------------------------\n",
    "# 체포 확률 20프로 데이터셋으로 ... 맞추기 \n",
    "TTtestDF = VehicleDF  # 기존 데이터 안 건들이기 목적 복사\n",
    "\n",
    "TrainFalseDF = VehicleDF[VehicleDF['Arrest'] == False].sample(n=75000, random_state=42) # sample(n=데이터양)\n",
    "TrainTrueDF = VehicleDF[VehicleDF['Arrest'] == True].sample(n=25000, random_state=42)\n",
    "TrainDF = pd.concat([TrainFalseDF, TrainTrueDF], axis=0)\n",
    "\n",
    "# 이미 트레인에 들어간 것 제외하고 테스트 데이터 구성\n",
    "TTtestDF.drop(TrainFalseDF.index, inplace=True)\n",
    "TTtestDF.drop(TrainTrueDF.index, inplace=True)\n",
    "\n",
    "TestFalseDF = TTtestDF[TTtestDF['Arrest'] == False].sample(n=7000, random_state=42)\n",
    "TestTrueDF = TTtestDF[TTtestDF['Arrest'] == True].sample(n=2094, random_state=42)\n",
    "TestDF = pd.concat([TestFalseDF, TestTrueDF], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 피쳐 타겟 스플릿----\n",
    "X_train = TrainDF.loc[:, TrainDF.columns != 'Arrest']\n",
    "y_train = TrainDF.loc[:, TrainDF.columns =='Arrest']\n",
    "\n",
    "X_test = TestDF.iloc[:, TestDF.columns != 'Arrest']\n",
    "y_test = TestDF.loc[:, TrainDF.columns =='Arrest']\n",
    "# print(\"\\n\\nX_train\")\n",
    "# print(X_train)\n",
    "\n",
    "# print(\"\\n\\ny_train\")\n",
    "# print(y_train)\n",
    "# 확인용\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# print(X_train.columns, X_test.columns)\n",
    "# print(VehicleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 본격 딥러닝...---------------------------------------------------------------------------------------\n",
    "class BinClaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(8,10)\n",
    "        self.hid_layer1 = nn.Linear(10,15)\n",
    "        self.hid_layer2 = nn.Linear(15,20)\n",
    "        self.hid_layer3 = nn.Linear(20,10)\n",
    "        self.out_layer = nn.Linear(10,1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.in_layer(x)\n",
    "        y = F.relu(y)\n",
    "        y = self.dropout(y) # 과적합 방지 dropout\n",
    "\n",
    "        y = self.hid_layer1(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.dropout(y)# 과적합 방지 dropout\n",
    "\n",
    "        y = self.hid_layer2(y)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.hid_layer3(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return torch.sigmoid(self.out_layer(y))\n",
    "\n",
    "\n",
    "class BinDataset(Dataset):\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF = featureDF\n",
    "        self.targetDF = targetDF\n",
    "        self.n_rows = featureDF.shape[0]\n",
    "        self.n_features = featureDF.shape[1]\n",
    "\n",
    "    def __len__(self) : \n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        # print(\"피쳐\", self.featureDF.dtypes)\n",
    "        # print(\"타겟\",self.targetDF.dtypes)\n",
    "        # self.featureDF.iloc[index].values.astype(float)\n",
    "        # self.targetDF.iloc[index].values.astype(float)\n",
    "        feaureTS = torch.FloatTensor(self.featureDF.iloc[index].values.astype(float)).to(DEVICE)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values.astype(float)).to(DEVICE)\n",
    "\n",
    "        \n",
    "        # 피처와 타겟 반환\n",
    "        return feaureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't cuda\n",
      "0.74908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kjy19\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### 본격 딥러닝...---------------------------------------------------------------------------------------\n",
    "VehicleDS = BinDataset(X_train, y_train)\n",
    "VehicleDL = DataLoader(VehicleDS)\n",
    "# print(VehDL)\n",
    "# for feature, label in VehicleDL:\n",
    "#     print(\"\\n\\nVehicleDL출력\")\n",
    "#     # print('feature.shape, label.shape, feature, label')\n",
    "#     print(\"feature.shape : \",feature.shape)\n",
    "#     print(\"label.shape : \",label.shape)\n",
    "#     print(\"feature\",feature)\n",
    "#     print(\"label : \",label)\n",
    "#     break\n",
    "\n",
    "\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 10\n",
    "BATCH_CNT = X_train.shape[0]//BATCH_SIZE # 선택사항 - 코드에 넣을 수도 있음\n",
    "\n",
    "# DEVICE = 'cuda'\n",
    "LR = 0.01\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    print(\"\\n\\n use cuda! \\n\\n\")\n",
    "else:\n",
    "    print(\"can't cuda\")\n",
    "\n",
    "model = BinClaModel().to(DEVICE)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "\n",
    "trainDS = BinDataset(X_train, y_train)\n",
    "valDS = BinDataset(X_val, y_val)\n",
    "testDS = BinDataset(X_test, y_test)\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "numSample_list = [y_train.value_counts().values[1], y_train.value_counts().values[0]]\n",
    "weights = [1 - (x / sum(numSample_list)) for x in numSample_list][0]\n",
    "print(weights)\n",
    "\n",
    "# 클래스 가중치 부여\n",
    "weightsTS = torch.FloatTensor([weights]) \n",
    "crossLoss=nn.BCELoss(weight=weightsTS)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR,weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', patience = 5, verbose = True)\n",
    "reqLoss = nn.BCELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport pandas as pd\\n\\n\\n# 모델 훈련\\nRFCmodel = RandomForestClassifier()\\nRFCmodel.fit(X_train, y_train)\\n\\n# 피쳐 중요도 추출\\nimportances = RFCmodel.feature_importances_\\n\\n# 데이터프레임으로 변환\\nfeature_importance_df = pd.DataFrame({\\'Feature\\': X_train.columns, \\'Importance\\': importances})\\nfeature_importance_df = feature_importance_df.sort_values(by=\\'Importance\\', ascending=False)\\n\\n# print(feature_importance_df)\\n# type(feature_importance_df)\\nprint(feature_importance_df)\\n\\nplt.barh(feature_importance_df.loc[:,\\'Feature\\'], feature_importance_df.loc[:,\\'Importance\\'])\\nplt.title(\"IUCR(4) LB // Location(9) OH // Time(H), col = 17\")\\n# plt.show()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 피쳐 중요도 ------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "RFCmodel = RandomForestClassifier()\n",
    "RFCmodel.fit(X_train, y_train)\n",
    "\n",
    "# 피쳐 중요도 추출\n",
    "importances = RFCmodel.feature_importances_\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# print(feature_importance_df)\n",
    "# type(feature_importance_df)\n",
    "print(feature_importance_df)\n",
    "\n",
    "plt.barh(feature_importance_df.loc[:,'Feature'], feature_importance_df.loc[:,'Importance'])\n",
    "plt.title(\"IUCR(4) LB // Location(9) OH // Time(H), col = 17\")\n",
    "# plt.show()\n",
    "\"\"\"\n",
    "## 피쳐 중요도 ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT =>7500\n",
      "scheduler.num_bad_epochs 0 scheduler.patience 5\n",
      "[0/100]\n",
      "- Train Loss : 0.5649083309113979 Score : 0.7488799993713697\n",
      "- Val Loss : 0.5599049925804138 Score : 0.7527599930763245\n",
      "scheduler.num_bad_epochs 1 scheduler.patience 5\n",
      "[1/100]\n",
      "- Train Loss : 0.5635548051834106 Score : 0.7490799993753433\n",
      "- Val Loss : 0.5597885847091675 Score : 0.7527599930763245\n",
      "scheduler.num_bad_epochs 2 scheduler.patience 5\n",
      "[2/100]\n",
      "- Train Loss : 0.5635545228481292 Score : 0.7490799993753433\n",
      "- Val Loss : 0.5597891211509705 Score : 0.7527599930763245\n",
      "scheduler.num_bad_epochs 3 scheduler.patience 5\n",
      "[3/100]\n",
      "- Train Loss : 0.5635814311126868 Score : 0.7490799993753433\n",
      "- Val Loss : 0.5597881078720093 Score : 0.7527599930763245\n",
      "scheduler.num_bad_epochs 4 scheduler.patience 5\n",
      "[4/100]\n",
      "- Train Loss : 0.5635632911682129 Score : 0.7490799993753433\n",
      "- Val Loss : 0.5597885251045227 Score : 0.7527599930763245\n",
      "scheduler.num_bad_epochs 5 scheduler.patience 5\n",
      "5EPOCH 성능 개선이 없어 조기종료함\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 - 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]], [[],[]]\n",
    "CNT = len(trainDL)\n",
    "print(f'CNT =>{CNT}')\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 학습 모드로 모델 성정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "    for featureTS, targetTS in trainDL :\n",
    "        # 학습 진행\n",
    "        pre_y = model(featureTS)\n",
    "\n",
    "        # 손실계산\n",
    "        loss = reqLoss(pre_y, targetTS)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        accuracy = BinaryAccuracy(threshold=0.5).to(DEVICE)(pre_y, targetTS)\n",
    "        # 방법2 : score = F1Score(task='binary')(pre_y, targetTS)\n",
    "        score_total += accuracy.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증용 데이터셋 생성\n",
    "        val_feaure_TS = torch.FloatTensor(valDS.featureDF.values.astype(float)).to(DEVICE)\n",
    "        val_target_TS = torch.FloatTensor(valDS.targetDF.values.astype(float)).to(DEVICE)\n",
    "        # 평가\n",
    "        pre_val = model(val_feaure_TS)\n",
    "        # 손실 계산\n",
    "        loss_val = reqLoss(pre_val, val_target_TS)\n",
    "        # 성능 평가\n",
    "        score_val = BinaryAccuracy().to(DEVICE)(pre_val, val_target_TS)\n",
    "\n",
    "        scheduler.step(score_val)   # 최댓값 해야함 scheduler에 모드 max 추가.\n",
    "    \n",
    "        print(f'scheduler.num_bad_epochs {scheduler.num_bad_epochs}', end=\" \")\n",
    "        print(f'scheduler.patience {scheduler.patience}')\n",
    "        # 손실 감소 (또는 성능 개선)이 안되는 경우 조기종료\n",
    "        if scheduler.num_bad_epochs >= scheduler.patience:\n",
    "            print(f'{scheduler.patience}EPOCH 성능 개선이 없어 조기종료함')\n",
    "            break\n",
    "\n",
    "    # 에포크 당 손실과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Loss / Score 그래프 ------------------------------------------------------------------------------------------------------------\n",
    "# \"\"\"\n",
    "# fg, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "# axes[0].plot(range(1, len(LOSS_HISTORY[0]) + 1), list(map(float,(LOSS_HISTORY[0]))), label='Train')\n",
    "# axes[0].plot(range(1, len(LOSS_HISTORY[1]) + 1), list(map(float,(LOSS_HISTORY[1]))), label='Val')\n",
    "# axes[0].grid()\n",
    "# axes[0].legend()\n",
    "# axes[0].set_xlabel(\"Epoch\")\n",
    "# axes[0].set_ylabel(\"Loss\")\n",
    "# axes[0].set_title(\"LOSS\")\n",
    "\n",
    "# axes[1].plot(range(1, len(SCORE_HISTORY[0]) + 1), list(map(float,(SCORE_HISTORY[0]))), label='Train')\n",
    "# axes[1].plot(range(1, len(SCORE_HISTORY[0]) + 1), list(map(float,(SCORE_HISTORY[1]))), label='Val')  # Move to CPU\n",
    "# axes[1].grid()\n",
    "# axes[1].legend()\n",
    "# axes[1].set_xlabel(\"Epoch\")\n",
    "# axes[1].set_ylabel(\"R2\")\n",
    "# axes[1].set_title(\"Accuracy\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# # plt.title(\"A13 \")\n",
    "# plt.show()\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models 폴더 아래 프로젝트 폴더 아래 모델 파일 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/Arrest'\n",
    "\n",
    "# 저장 파일명\n",
    "SAVE_FILE = 'Arrest_train_wbs.pth'\n",
    "\n",
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)  # 폴더 / 폴더 / ... 하위 폴더까지 생성\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), SAVE_PATH + SAVE_FILE)\n",
    "\n",
    "wbTS = torch.load(SAVE_PATH+SAVE_FILE, weights_only=True)\n",
    "print(type(wbTS))\n",
    "\n",
    "model.load_state_dict(wbTS)\n",
    "\n",
    "# model2 = model() # W,b 초기화\n",
    "model2 = BinClaModel()\n",
    "model.load_state_dict(wbTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  IUCR  District  Ward  Community Area  ATTEMPT  RECOVERY  Location\n",
      "0    15  17.0         0   2.0             3.0     38.0         0         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# import torch\n",
    "\n",
    "data = [10.0,0,4.0,10.0,49.0,0,0,9]\n",
    "data = [15,17.0,0,2.0,3.0,38.0,0,0,3]\n",
    "\n",
    "def preprocessing(text):\n",
    "    datadict = {'Date':text[0], 'IUCR':text[1], 'District': text[2], 'Ward':text[3], 'Community Area':text[4], 'ATTEMPT':text[5], 'RECOVERY':text[6], 'Location':text[7]}\n",
    "    return datadict\n",
    "\n",
    "data=preprocessing(data)\n",
    "dataDF=pd.DataFrame([data])\n",
    "print(dataDF)\n",
    "model.eval()\n",
    "input_data = torch.FloatTensor(dataDF.values).to(DEVICE)  # 입력 데이터와 장치 설정\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_data)\n",
    "predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "predicted_classes[0]\n",
    "\n",
    "int(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRIPT_MODE:\n",
    "    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach()) #웹에서만 필요 : 표준출력을 utf-8로\n",
    "\n",
    "\n",
    "if SCRIPT_MODE:\n",
    "    pklfile = os.path.dirname(__file__)+ '/lang.pkl' # 웹상에서는 절대경로만\n",
    "else:\n",
    "    pklfile = './lang.pkl'\n",
    "langModel = joblib.load(pklfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
